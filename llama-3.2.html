<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Meta Llama 3.2: The Ultimate Guide</title>
    <script src="/app-redirect.js"></script>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700;800&display=swap" rel="stylesheet">
    <meta name="description" content="A complete guide to Meta's next-generation Llama 3.2 models. Understand its open-source nature, multimodal capabilities, applications, and future potential.">
    <meta name="keywords" content="Meta, Llama 3, Llama 3.2, Open Source AI, Large Language Models, AI, Artificial Intelligence, Multimodal AI, Generative AI, RLAIF">
    <meta name="author" content="AI Tools Guide">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #111827; /* Gray 900 */
            color: #e5e7eb; /* Light Gray */
        }
        .section-heading {
            border-image: linear-gradient(to right, #ec4899, #0ea5e9) 1;
            border-bottom: 4px solid;
            padding-bottom: 0.5rem;
            margin-bottom: 1.5rem;
            display: inline-block;
            color: #ffffff; /* White */
        }
        .card {
            background: rgba(17, 24, 39, 0.8); /* Gray 900 Transparent */
            backdrop-filter: blur(16px);
            -webkit-backdrop-filter: blur(16px);
            border-radius: 1.25rem;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.6);
            padding: 2rem;
            transition: all 0.4s cubic-bezier(0.175, 0.885, 0.32, 1.275);
            border: 1px solid #374151; /* Gray 700 */
        }
        .card:hover {
            box-shadow: 0 0 40px rgba(236, 72, 153, 0.25);
            transform: translateY(-12px) scale(1.02);
            border-color: #ec4899; /* Pink 500 */
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 550px;
            margin: auto;
            height: 320px;
            max-height: 450px;
        }
        @media (min-width: 768px) {
            .chart-container {
                height: 380px;
            }
        }
        .flowchart-step {
            border: 3px solid #ec4899; /* Pink 500 */
            color: #ec4899;
            background: #1f2937; /* Gray 800 */
            transition: all 0.3s ease-in-out;
            cursor: pointer;
            box-shadow: 0 0 15px rgba(236, 72, 153, 0.2);
            text-shadow: 0 0 5px rgba(236, 72, 153, 0.5);
        }
        .flowchart-step:hover {
            background-color: #ec4899;
            color: #111827;
            transform: scale(1.08);
            box-shadow: 0 0 25px rgba(236, 72, 153, 0.5);
        }
        .flowchart-arrow {
            color: #0ea5e9; /* Sky 500 */
            text-shadow: 0 0 10px rgba(14, 165, 233, 0.5);
        }
        .subject-category-heading {
            color: #ffffff;
            font-weight: 700;
            margin-bottom: 1rem;
            font-size: 1.3rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid #0ea5e9;
        }
        .subject-item {
            display: flex;
            align-items: center;
            margin-bottom: 0.75rem;
            padding: 0.75rem;
            border-radius: 0.75rem;
            transition: background-color 0.2s;
        }
        .subject-item:hover {
            background-color: rgba(14, 165, 233, 0.1);
        }
        .subject-item .icon {
            color: #0ea5e9;
            margin-right: 1rem;
            font-size: 1.5rem;
        }
        .timeline-step {
            position: relative;
            padding-left: 2.5rem;
            padding-bottom: 2.5rem;
            border-left: 4px solid #ec4899;
        }
        .timeline-step:last-child {
            padding-bottom: 0;
        }
        .timeline-dot {
            position: absolute;
            left: -0.9375rem;
            top: 0;
            width: 1.75rem;
            height: 1.75rem;
            border-radius: 50%;
            background-color: #111827;
            border: 4px solid #ec4899;
            box-shadow: 0 0 15px rgba(236, 72, 153, 0.7);
        }
        .modal-overlay {
            position: fixed;
            top: 0; left: 0; width: 100%; height: 100%;
            background-color: rgba(17, 24, 39, 0.85);
            backdrop-filter: blur(8px);
            -webkit-backdrop-filter: blur(8px);
            display: flex;
            align-items: center; justify-content: center;
            z-index: 1000;
            opacity: 0; visibility: hidden;
            transition: opacity 0.3s ease, visibility 0.3s ease;
            padding: 1rem;
        }
        .modal-overlay.active {
            opacity: 1; visibility: visible;
        }
        .modal-content {
            background-color: #1f2937;
            color: #e5e7eb;
            border-radius: 1rem;
            padding: 2.5rem;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.5);
            max-width: 800px;
            width: 100%;
            position: relative;
            transform: translateY(20px) scale(0.98);
            opacity: 0;
            transition: transform 0.4s ease, opacity 0.4s ease;
            max-height: 90vh;
            overflow-y: auto;
            border: 1px solid #4b5563;
        }
        .modal-overlay.active .modal-content {
            transform: translateY(0) scale(1);
            opacity: 1;
        }
        .modal-close-button {
            position: absolute; top: 1rem; right: 1rem;
            background: none; border: none; font-size: 2.5rem;
            cursor: pointer; color: #9ca3af; line-height: 1;
            transition: color 0.2s, transform 0.2s;
        }
        .modal-close-button:hover { color: #f87171; transform: rotate(90deg); }
        .modal-action-button {
            background: linear-gradient(45deg, #ec4899, #d946ef);
            color: #ffffff;
            padding: 0.85rem 1.75rem;
            border-radius: 0.75rem;
            font-weight: 700;
            cursor: pointer;
            transition: all 0.3s ease;
            border: none;
            box-shadow: 0 5px 15px rgba(236, 72, 153, 0.3);
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        .modal-action-button:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 25px rgba(236, 72, 153, 0.4);
        }
        .lang-btn {
            padding: 0.6rem 2rem;
            font-weight: 700;
            cursor: pointer;
            transition: all 0.3s;
            border: 2px solid #ec4899;
            color: #ec4899;
            background-color: transparent;
        }
        .lang-btn:first-child { border-top-left-radius: 0.75rem; border-bottom-left-radius: 0.75rem; }
        .lang-btn:last-child { border-top-right-radius: 0.75rem; border-bottom-right-radius: 0.75rem; }
        .lang-btn.active, .lang-btn:hover {
            background-color: #ec4899;
            color: #111827;
            box-shadow: 0 0 15px #ec4899;
        }
        .notes-btn, .lectures-btn {
            padding: 0.35rem 1rem;
            border-radius: 0.5rem;
            font-size: 0.875rem;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.2s ease-in-out;
            border: 1px solid transparent;
            text-transform: uppercase;
        }
        .notes-btn {
            background: linear-gradient(45deg, #0ea5e9, #0284c7);
            color: #ffffff;
        }
        .notes-btn:hover {
            filter: brightness(1.2);
        }
        .lectures-btn {
            background-color: #374151; 
            color: #d1d5db; 
            border-color: #4b5563; 
        }
        .lectures-btn:hover {
            background-color: #4b5563;
            color: #ffffff;
        }
    </style>
</head>
<body class="leading-relaxed">

    <div class="container mx-auto p-4 sm:p-6 md:p-12">

        <header class="text-center mb-10">
            <h1 id="header-title" class="text-4xl md:text-5xl font-extrabold text-[#ffffff] mb-4 tracking-tight">ü¶ô Meta's Llama 3.2: The Ultimate Guide ü¶ô</h1>
            <p id="header-subtitle" class="text-xl md:text-2xl font-semibold bg-clip-text text-transparent bg-gradient-to-r from-pink-400 to-sky-400">The Next Evolution in Open Source AI</p>
        </header>

        <div class="text-center mb-12">
            <div class="inline-flex rounded-md shadow-sm" role="group">
                <button type="button" class="lang-btn active" data-lang="en">English</button>
                <button type="button" class="lang-btn" data-lang="hi">‡§π‡§ø‡§Ç‡§¶‡•Ä</button>
            </div>
        </div>

        <main class="grid grid-cols-1 gap-12">

            <!-- Section 1: What is Llama 3.2? -->
            <section class="card !p-0 overflow-hidden bg-gradient-to-br from-[#1f2937] via-[#111827] to-[#111827]">
                <div class="p-8">
                    <h2 id="sec1-title" class="text-3xl font-bold section-heading">1. What is Meta's Llama 3.2? üí°</h2>
                    <p id="sec1-p1" class="text-lg mb-6">Llama 3.2 is the next-generation family of open-source large language models from Meta. Building on the massive success of Llama 2, this new series represents a significant leap in performance, efficiency, and capability. Llama 3.2 introduces **full multimodality**, allowing it to understand and process not just text, but also images, audio, and video. It is designed to be the most powerful and accessible open-source model, empowering developers to create more sophisticated and helpful AI applications than ever before.</p>
                </div>
                <div id="postCardsContainer" class="px-8 pb-8 pt-2 grid grid-cols-1 md:grid-cols-3 gap-8 bg-transparent">
                    <!-- Cards will be dynamically inserted here by JavaScript -->
                </div>
            </section>

            <!-- Section 2: How It Works -->
            <section class="card">
                <h2 id="sec2-title" class="text-3xl font-bold section-heading text-center w-full">2. The Technology Behind Llama 3.2 üéØ</h2>
                <p id="sec2-p1" class="text-lg text-center mb-10 max-w-3xl mx-auto">Llama 3.2 utilizes an advanced Transformer architecture and is refined with state-of-the-art alignment techniques for superior performance and safety.</p>
                <div class="flex flex-col md:flex-row items-center justify-center space-y-8 md:space-y-0 md:space-x-8 lg:space-x-16">
                    <div class="text-center flex flex-col items-center">
                        <div class="w-40 h-40 flex items-center justify-center rounded-full flowchart-step font-bold text-lg p-3 text-center leading-tight" data-modal-target="prelimsMainModal"><span id="sec2-step1-title">Stage 1:</span><br><span id="sec2-step1-name">Pre-training</span></div>
                        <p id="sec2-step1-desc" class="mt-4 font-semibold text-gray-400">Learning from a massive multimodal dataset</p>
                    </div>
                    <div class="text-5xl font-light flowchart-arrow transform md:rotate-0 rotate-90">‚Üí</div>
                    <div class="text-center flex flex-col items-center">
                        <div class="w-40 h-40 flex items-center justify-center rounded-full flowchart-step font-bold text-lg p-3 text-center leading-tight" data-modal-target="mainsMainModal"><span id="sec2-step2-title">Stage 2:</span><br><span id="sec2-step2-name">Post-training</span></div>
                        <p id="sec2-step2-desc" class="mt-4 font-semibold text-gray-400">Fine-tuning with SFT & RLAIF</p>
                    </div>
                    <div class="text-5xl font-light flowchart-arrow transform md:rotate-0 rotate-90">‚Üí</div>
                    <div class="text-center flex flex-col items-center">
                        <div class="w-40 h-40 flex items-center justify-center rounded-full flowchart-step font-bold text-lg p-3 text-center leading-tight" data-modal-target="interviewModal"><span id="sec2-step3-title">Stage 3:</span><br><span id="sec2-step3-name">Red Teaming</span></div>
                        <p id="sec2-step3-desc" class="mt-4 font-semibold text-gray-400">Extensive safety and adversarial testing</p>
                    </div>
                </div>
            </section>

            <!-- Section 3: Access & Availability -->
            <section class="card">
                <h2 id="sec3-title" class="text-3xl font-bold section-heading">3. Who Can Use It? ‚úÖ</h2>
                <div class="grid grid-cols-1 lg:grid-cols-2 gap-8 items-start">
                    <div>
                        <p id="sec3-p1" class="text-lg mb-6">Following its open-source philosophy, Meta aims to make Llama 3.2 widely available across numerous platforms.</p>
                        <ul class="space-y-5 text-base">
                            <li class="flex items-start"><span class="text-3xl text-[#ec4899] mr-4">üåê</span><div><strong id="sec3-age-title" class="text-white text-lg">Open Source Community:</strong> <span id="sec3-age-desc">The model weights are freely available for download for research and most commercial applications.</span></div></li>
                            <li class="flex items-start"><span class="text-3xl text-[#ec4899] mr-4">‚òÅÔ∏è</span><div><strong id="sec3-edu-title" class="text-white text-lg">Cloud Platforms:</strong> <span id="sec3-edu-desc">Available on all major cloud providers like AWS, Google Cloud, and Microsoft Azure for easy, scalable deployment.</span></div></li>
                        </ul>
                    </div>
                    <div>
                        <h3 id="sec3-attempts-title" class="text-2xl font-bold text-white mb-4 border-b border-[#0ea5e9] pb-2">Hardware & Devices üìä</h3>
                         <ul id="sec3-agerelax-list" class="list-disc list-inside space-y-3 text-base">
                             <li><strong>Optimized Versions:</strong> Llama 3.2 will be available in various sizes, optimized to run on a wide range of hardware, from large GPU clusters to personal computers and mobile devices.</li>
                             <li><strong>Hardware Partners:</strong> Meta is collaborating with partners like NVIDIA, Intel, and Qualcomm to optimize performance on their latest chips.</li>
                        </ul>
                    </div>
                </div>
            </section>
            
            <!-- Section 4: Core Capabilities -->
            <section class="card bg-black/20">
                <h2 id="sec4-title" class="text-3xl font-bold section-heading text-center w-full">4. Next-Generation Capabilities üìö</h2>
                <p id="sec4-p1" class="text-lg text-center mb-10 max-w-4xl mx-auto">Llama 3.2 introduces full multimodality and significantly enhanced reasoning, setting a new standard for open-source models.</p>
                 <div id="syllabus-grid" class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-8">
                    <div class="bg-[#1f2937]/80 backdrop-blur-sm p-6 rounded-xl shadow-lg border border-slate-700 flex flex-col transition duration-300 hover:shadow-2xl hover:border-pink-500 hover:-translate-y-1">
                        <h3 id="gs1-title" class="subject-category-heading">Full Multimodality</h3>
                        <p class="text-sm">Understands and generates content from text, images, audio, and video.</p>
                    </div>
                     <div class="bg-[#1f2937]/80 backdrop-blur-sm p-6 rounded-xl shadow-lg border border-slate-700 flex flex-col transition duration-300 hover:shadow-2xl hover:border-pink-500 hover:-translate-y-1">
                        <h3 id="gs2-title" class="subject-category-heading">Advanced Reasoning</h3>
                         <p class="text-sm">Improved performance on complex, multi-step reasoning, coding, and math problems.</p>
                    </div>
                    <div class="bg-[#1f2937]/80 backdrop-blur-sm p-6 rounded-xl shadow-lg border border-slate-700 flex flex-col transition duration-300 hover:shadow-2xl hover:border-pink-500 hover:-translate-y-1">
                        <h3 id="gs3-title" class="subject-category-heading">Massive Context</h3>
                         <p class="text-sm">A significantly larger context window (over 1M tokens), allowing it to process and analyze entire books or codebases.</p>
                    </div>
                    <div class="bg-[#1f2937]/80 backdrop-blur-sm p-6 rounded-xl shadow-lg border border-slate-700 flex flex-col transition duration-300 hover:shadow-2xl hover:border-pink-500 hover:-translate-y-1">
                        <h3 id="gs4-title" class="subject-category-heading">Improved Safety</h3>
                         <p class="text-sm">Enhanced safety features powered by new versions of Llama Guard and other advanced alignment techniques.</p>
                    </div>
                </div>
            </section>

            <!-- Section 5: Training & Data -->
            <section class="card">
                <h2 id="sec5-title" class="text-3xl font-bold section-heading">5. How is Llama 3.2 Trained? üèõÔ∏è</h2>
                <p id="sec5-p1" class="text-lg text-center mb-8 max-w-3xl mx-auto">Llama 3.2 is trained on an unprecedented scale, using a massive custom-built GPU infrastructure and a highly curated dataset.</p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8 text-base">
                    <div class="p-6 bg-[#1f2937] rounded-xl border-2 border-[#ec4899]">
                        <h3 id="sec5-hindi-title" class="font-bold text-xl text-white mb-2">1. The Dataset</h3>
                        <p id="sec5-hindi-p1" class="text-sm mb-2">A vastly expanded and more recent dataset.</p>
                        <ul id="sec5-hindi-list" class="list-disc list-inside space-y-1 mt-2 text-sm">
                            <li>Pre-trained on over 15 trillion tokens of publicly available data, including text, code, images, and video.</li>
                            <li>The data undergoes extensive filtering and curation to improve quality and reduce bias.</li>
                        </ul>
                    </div>
                    <div class="p-6 bg-[#1f2937] rounded-xl border-2 border-[#0ea5e9]">
                        <h3 id="sec5-essay-title" class="font-bold text-xl text-white mb-2">2. The Alignment Process</h3>
                        <p id="sec5-essay-p1" class="text-sm mb-2">A combination of advanced techniques.</p>
                        <ul id="sec5-essay-list" class="list-disc list-inside space-y-1 mt-2 text-sm">
                           <li>Uses a combination of Supervised Fine-Tuning (SFT), Reinforcement Learning with Human Feedback (RLHF), and a new technique, Reinforcement Learning with AI Feedback (RLAIF).</li>
                           <li>RLAIF allows for faster and more scalable safety alignment by using a powerful AI model to help label data.</li>
                        </ul>
                    </div>
                </div>
            </section>
            
            <!-- Section 6: Key Applications -->
            <section class="card">
                <h2 id="sec6-title" class="text-3xl font-bold section-heading">6. Key Applications & Use Cases üåü</h2>
                <p id="sec6-p1" class="text-lg text-center mb-8 max-w-4xl mx-auto">Llama 3.2's multimodal and enhanced reasoning capabilities open up new frontiers for AI applications.</p>
                <div class="grid grid-cols-1 md:grid-cols-3 gap-8 text-base">
                    <div>
                        <h3 id="sec6-eng-title" class="subject-category-heading">AI Agents</h3>
                        <ul id="sec6-eng-list" class="list-disc list-inside space-y-2 text-base">
                           <li>Building sophisticated autonomous agents that can reason and take multi-step actions.</li>
                           <li>Creating more capable and helpful personal and professional assistants.</li>
                        </ul>
                    </div>
                    <div>
                        <h3 id="sec6-elec-title" class="subject-category-heading">Multimodal Content</h3>
                        <ul id="sec6-elec-list" class="list-disc list-inside space-y-2 text-base">
                           <li>Generating video from text or images.</li>
                           <li>Creating presentations from documents.</li>
                           <li>Analyzing and describing the content of videos.</li>
                        </ul>
                    </div>
                     <div>
                        <h3 id="sec6-naic-title" class="subject-category-heading">Advanced Coding</h3>
                        <ul id="sec6-naic-list" class="list-disc list-inside space-y-2 text-base">
                           <li>Generating entire codebases from high-level specifications.</li>
                           <li>Advanced debugging and code optimization.</li>
                           <li>Serving as an expert pair programmer for complex projects.</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Section 7: Performance Benchmarks -->
            <section class="card grid grid-cols-1 lg:grid-cols-2 gap-8 items-center">
                <div>
                     <h2 id="sec7-title" class="text-3xl font-bold section-heading">7. Performance & Benchmarks üéôÔ∏è</h2>
                    <p id="sec7-p1" class="text-lg mb-6">Llama 3.2 is expected to set new state-of-the-art performance records for open-source models, rivaling and even surpassing the best proprietary models available.</p>
                    <div class="chart-container h-80">
                        <canvas id="interviewQualitiesChart"></canvas>
                    </div>
                </div>
                <div>
                    <h3 id="sec7-qualities-title" class="text-2xl font-bold text-white mb-6 border-b border-[#ec4899] pb-2">Key Benchmarks:</h3>
                    <ul id="sec7-qualities-list" class="space-y-4 text-base">
                        <li class="flex items-start"><span class="text-2xl text-[#0ea5e9] mr-3">üß†</span><div><strong class="text-white">Reasoning & Knowledge (MMLU):</strong> Expected to achieve near-human-level scores on general knowledge and reasoning tasks.</div></li>
                        <li class="flex items-start"><span class="text-2xl text-[#0ea5e9] mr-3">üíª</span><div><strong class="text-white">Coding (HumanEval):</strong> A significant leap in performance for complex code generation, debugging, and planning.</div></li>
                        <li class="flex items-start"><span class="text-2xl text-[#0ea5e9] mr-3">üñºÔ∏è</span><div><strong class="text-white">Multimodal Benchmarks:</strong> Strong performance on benchmarks that test the ability to reason across text, images, and video.</div></li>
                    </ul>
                </div>
            </section>
            
            <!-- Section 8: Prompt Engineering -->
            <section class="card">
                <h2 id="sec8-title" class="text-3xl font-bold section-heading">8. Prompting Llama 3.2: Best Practices üìà</h2>
                <p id="sec8-p1" class="text-lg mb-8 max-w-4xl mx-auto">To effectively use Llama 3.2's multimodal and reasoning capabilities, prompts should be structured to provide rich context across different data types.</p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                    <div>
                        <h3 id="sec8-steps-title" class="text-2xl font-bold text-white mb-4">üìö Key Principles:</h3>
                        <ol class="relative border-l-4 border-[#ec4899] pl-6 space-y-8">
                            <li class="ml-4">
                                <div class="absolute w-4 h-4 bg-[#ec4899] rounded-full -left-2.5 border-4 border-[#1f2937]"></div>
                                <h4 id="sec8-step1-title" class="font-semibold text-xl text-white mb-1">Interleave Modalities</h4>
                                <p id="sec8-step1-desc" class="text-base">Provide a sequence of text and images in your prompt to ask complex visual reasoning questions.</p>
                            </li>
                             <li class="ml-4">
                               <div class="absolute w-4 h-4 bg-[#ec4899] rounded-full -left-2.5 border-4 border-[#1f2937]"></div>
                                <h4 id="sec8-step2-title" class="font-semibold text-xl text-white mb-1">Leverage the Context Window</h4>
                                <p id="sec8-step2-desc" class="text-base">Provide entire documents, codebases, or transcripts as context for in-depth analysis and question-answering.</p>
                            </li>
                        </ol>
                    </div>
                    <div>
                        <h3 id="sec8-revision-title" class="text-2xl font-bold text-white mb-4">üîÑ Advanced Techniques:</h3>
                         <ul id="sec8-revision-list" class="list-disc list-inside space-y-3 text-base">
                            <li><strong>Chain of Thought for Multimodality:</strong> Ask the model to "think step by step" when analyzing complex images or charts to improve accuracy.</li>
                            <li><strong>System Prompts for Safety:</strong> Use robust system prompts and tools like Llama Guard 2 to ensure the model's outputs remain safe and on-topic.</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Section 9: Model Evolution -->
            <section class="card">
                <h2 id="sec9-title" class="text-3xl font-bold section-heading">9. Model Evolution üóìÔ∏è</h2>
                <p id="sec9-p1" class="text-lg mb-8">Llama 3.2 represents a major milestone in Meta's mission to build and openly share advanced AI technology.</p>
                <div class="career-path-timeline">
                    <div class="timeline-step">
                        <div class="timeline-dot"></div>
                        <h4 id="sec9-step2-title" class="font-semibold text-xl text-white mb-1">Llama 2 Release</h4>
                        <p id="sec9-step2-desc" class="text-base"><strong class="text-gray-400">Timeline:</strong> July 2023. Established a new standard for open-source LLMs and enabled commercial use.</p>
                    </div>
                     <div class="timeline-step">
                        <div class="timeline-dot"></div>
                        <h4 id="sec9-step3-title" class="font-semibold text-xl text-white mb-1">Llama 3 Release</h4>
                        <p id="sec9-step3-desc" class="text-base"><strong class="text-gray-400">Timeline:</strong> April 2024. Delivered a major performance leap with models up to 70B parameters, outperforming many proprietary models.</p>
                    </div>
                     <div class="timeline-step">
                        <div class="timeline-dot"></div>
                        <h4 id="sec9-step4-title" class="font-semibold text-xl text-white mb-1">Llama 3.2 Release</h4>
                        <p id="sec9-step4-desc" class="text-base"><strong class="text-gray-400">Timeline:</strong> Q4 2025 (Hypothetical). The introduction of a fully multimodal, 400B+ parameter model that redefines state-of-the-art for open-source AI.</p>
                    </div>
                </div>
            </section>

            <!-- Section 10: Mistakes -->
            <section class="card">
                <h2 id="sec10-title" class="text-3xl font-bold section-heading text-center w-full">10. Limitations & Responsible Use üö´</h2>
                <p id="sec10-p1" class="text-lg text-center mb-10 max-w-4xl mx-auto">As AI models become more powerful, a commitment to responsible development and deployment is more important than ever.</p>
                <div id="sec10-donts-grid" class="grid grid-cols-1 md:grid-cols-2 gap-8 text-base">
                    <div class="group relative p-6 bg-slate-900/50 rounded-lg border-2 border-red-700 shadow-md transition-all duration-300 hover:shadow-xl hover:border-red-500 hover:-translate-y-1">
                        <div class="absolute -top-4 -left-4 bg-red-600 text-white rounded-full w-12 h-12 flex items-center justify-center text-2xl font-bold shadow-lg group-hover:scale-110 transition-transform">‚ùå</div>
                        <div class="ml-10">
                            <strong id="sec10-mistake1-title" class="text-lg text-red-300">Misinformation Potential:</strong>
                            <p id="sec10-mistake1-desc" class="text-red-400 mt-1">The ability to generate realistic multimodal content increases the risk of creating sophisticated misinformation. Fact-checking is essential.</p>
                        </div>
                    </div>
                    <div class="group relative p-6 bg-slate-900/50 rounded-lg border-2 border-red-700 shadow-md transition-all duration-300 hover:shadow-xl hover:border-red-500 hover:-translate-y-1">
                         <div class="absolute -top-4 -left-4 bg-red-600 text-white rounded-full w-12 h-12 flex items-center justify-center text-2xl font-bold shadow-lg group-hover:scale-110 transition-transform">‚ùå</div>
                        <div class="ml-10">
                            <strong id="sec10-mistake2-title" class="text-lg text-red-300">Bias Amplification:</strong>
                            <p id="sec10-mistake2-desc" class="text-red-400 mt-1">Models can still reflect and amplify biases present in their training data, requiring careful implementation of safety guardrails.</p>
                        </div>
                    </div>
                     <div class="group relative p-6 bg-slate-900/50 rounded-lg border-2 border-red-700 shadow-md transition-all duration-300 hover:shadow-xl hover:border-red-500 hover:-translate-y-1">
                         <div class="absolute -top-4 -left-4 bg-red-600 text-white rounded-full w-12 h-12 flex items-center justify-center text-2xl font-bold shadow-lg group-hover:scale-110 transition-transform">‚ùå</div>
                        <div class="ml-10">
                            <strong id="sec10-mistake3-title" class="text-lg text-red-300">Resource Intensive:</strong>
                            <p id="sec10-mistake3-desc" class="text-red-400 mt-1">Running the largest versions of Llama 3.2 requires significant computational resources, which can be a barrier for individual developers.</p>
                        </div>
                    </div>
                     <div class="group relative p-6 bg-slate-900/50 rounded-lg border-2 border-red-700 shadow-md transition-all duration-300 hover:shadow-xl hover:border-red-500 hover:-translate-y-1">
                         <div class="absolute -top-4 -left-4 bg-red-600 text-white rounded-full w-12 h-12 flex items-center justify-center text-2xl font-bold shadow-lg group-hover:scale-110 transition-transform">‚ùå</div>
                        <div class="ml-10">
                            <strong id="sec10-mistake4-title" class="text-lg text-red-300">Responsible Use:</strong>
                            <p id="sec10-mistake4-desc" class="text-red-400 mt-1">All users must adhere to Meta's Acceptable Use Policy and implement safety tools like Llama Guard 2 to prevent misuse.</p>
                        </div>
                    </div>
                </div>
            </section>
            
            <!-- Section 11: How to Access -->
            <section class="card">
                <h2 id="sec11-title" class="text-3xl font-bold section-heading">11. How to Access & Use Llama 3.2 üè´</h2>
                <p id="sec11-p1" class="text-lg text-center mb-8 max-w-4xl mx-auto">Meta will continue its strategy of broad and open distribution for Llama 3.2.</p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                    <div>
                        <h3 id="sec11-gov-title" class="subject-category-heading">üìö For Developers & Hobbyists</h3>
                        <ul id="sec11-gov-list" class="list-disc list-inside space-y-2 text-base">
                           <li>**Hugging Face:** The easiest way to get started with the models using the `transformers` library.</li>
                           <li>**Direct Download:** Apply for access on the Meta AI website to download the model weights.</li>
                        </ul>
                    </div>
                    <div>
                        <h3 id="sec11-pvt-title" class="subject-category-heading">üíª For Enterprise Use</h3>
                        <p id="sec11-pvt-p1" class="text-base mb-2">Use managed cloud services for scalable deployments.</p>
                        <ul id="sec11-pvt-list" class="list-disc list-inside space-y-2 text-base">
                            <li>Available on all major cloud platforms, including Microsoft Azure, AWS, and Google Cloud.</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Section 12: The Future -->
            <section class="card">
                <h2 id="sec12-title" class="text-3xl font-bold section-heading">12. The Future: Open-Source AGI üöÄ</h2>
                <p id="sec12-p1" class="text-lg mb-6">Llama 3.2 continues Meta's push to make state-of-the-art AI openly available, accelerating innovation and collaboration across the globe. The rapid progress in open-source models is a key driver on the path to developing safe and beneficial Artificial General Intelligence (AGI).</p>
                <button id="sec12-career-btn" class="modal-action-button mt-4 inline-block" data-modal-target="careerGrowthModal">View Key Differentiators</button>
            </section>

            <!-- Section 13: Core Concepts -->
            <section class="card bg-black/20">
                <h2 id="sec13-title" class="text-3xl font-bold section-heading">13. Core AI Concepts üìö</h2>
                <p id="sec13-p1" class="text-lg text-center mb-10 max-w-4xl mx-auto">Understanding these core concepts is fundamental to appreciating the technology behind Llama 3.2.</p>
                <div id="syllabus-material-grid" class="grid grid-cols-1 md:grid-cols-2 gap-8">
                    <div class="bg-[#1a2a24] p-6 rounded-lg shadow-lg border border-stone-700">
                        <h3 id="syllabus-oir-title" class="subject-category-heading">Open Source AI</h3>
                        <div id="syllabus-oir-list" class="space-y-4">
                            <!-- Content via JS -->
                        </div>
                    </div>
                    <div class="bg-[#1a2a24] p-6 rounded-lg shadow-lg border border-stone-700">
                        <h3 id="syllabus-ppdt-title" class="subject-category-heading">Advanced Alignment</h3>
                        <div id="syllabus-ppdt-list" class="space-y-4">
                            <!-- Content via JS -->
                        </div>
                    </div>
                </div>
            </section>

        </main>

        <footer class="text-center mt-16 border-t-2 pt-8 border-pink-500/20">
            <p id="footer-text" class="text-slate-400 text-lg font-medium">&copy; 2025 | Building the future of AI, openly.</p>
        </footer>

    </div>

    <!-- Modals -->
    <div id="prelimsMainModal" class="modal-overlay">
        <div class="modal-content">
            <button class="modal-close-button">&times;</button>
            <h3 id="prelims-modal-title" class="text-2xl font-bold text-white mb-4">Stage 1: Pre-training</h3>
            <p id="prelims-modal-p1">The model is trained on a colossal dataset of over 15 trillion tokens, including a vast amount of text, code, images, and video data. This unsupervised phase builds the model's foundational knowledge and its ability to understand patterns across different modalities.</p>
        </div>
    </div>

    <div id="mainsMainModal" class="modal-overlay">
        <div class="modal-content">
            <button class="modal-close-button">&times;</button>
            <h3 id="mains-modal-title" class="text-2xl font-bold text-white mb-4">Stage 2: Post-training Alignment</h3>
            <p id="mains-modal-p1">After pre-training, the model is extensively fine-tuned for safety and helpfulness. This involves a combination of Supervised Fine-Tuning (SFT) and advanced techniques like Reinforcement Learning with both Human and AI Feedback (RLHF & RLAIF) to align the model's behavior with desired principles.</p>
        </div>
    </div>

    <div id="interviewModal" class="modal-overlay">
        <div class="modal-content">
            <button class="modal-close-button">&times;</button>
            <h3 id="interview-modal-title" class="text-2xl font-bold text-white mb-4">Stage 3: Red Teaming & Safety</h3>
            <p id="interview-modal-p1" class="text-base mb-3">Before release, the model undergoes rigorous adversarial testing, or "Red Teaming," where internal and external experts try to make the model produce harmful or biased content. This helps identify and mitigate potential risks. Tools like Llama Guard 2 are also developed to help filter unsafe outputs.</p>
        </div>
    </div>

    <!-- Post Detail Modal -->
    <div id="postDetailModal" class="modal-overlay">
        <div class="modal-content">
            <button class="modal-close-button">&times;</button>
            <div class="flex items-center mb-4">
                <div id="postDetailIcon" class="text-5xl mr-4"></div>
                <h3 id="postDetailTitle" class="text-3xl font-bold text-white"></h3>
            </div>
            <p id="postDetailDescription" class="text-lg text-indigo-200 leading-relaxed"></p>
        </div>
    </div>
    
    <div id="careerGrowthModal" class="modal-overlay">
        <div class="modal-content">
            <button class="modal-close-button">&times;</button>
            <h3 id="career-modal-title" class="text-2xl font-bold text-white mb-6">Key Differentiators of Llama 3.2</h3>
            <div id="careerPathContainer" class="career-path-timeline"></div>
        </div>
    </div>


    <script>
        document.addEventListener('DOMContentLoaded', () => {
            let interviewChartInstance;
            let currentLang = 'en';

            const serviceDetailsData = {
                'en': {
                    's': { 
                        name: 'Llama 3.2 8B', 
                        icon: '‚ö°',
                        description: 'A highly efficient and fast model, ideal for on-device applications, simple chatbots, and tasks requiring low latency.',
                        cta: 'Learn More'
                    },
                    'm': { 
                        name: 'Llama 3.2 70B', 
                        icon: 'üß†',
                        description: 'The balanced model, offering a great combination of performance and efficiency for a wide range of enterprise and developer applications.',
                        cta: 'Learn More'
                    },
                    'l': { 
                        name: 'Llama 3.2 405B+', 
                        icon: 'üèÜ',
                        description: 'The flagship, state-of-the-art model designed for the most complex reasoning and multimodal tasks, rivaling the best proprietary models.',
                        cta: 'Learn More'
                    }
                },
                'hi': {
                    's': { 
                        name: '‡§≤‡§æ‡§Æ‡§æ 3.2 8B', 
                        icon: '‚ö°',
                        description: '‡§è‡§ï ‡§Ö‡§§‡•ç‡§Ø‡§ß‡§ø‡§ï ‡§ï‡•Å‡§∂‡§≤ ‡§î‡§∞ ‡§§‡•á‡§ú‡§º ‡§Æ‡•â‡§°‡§≤, ‡§ú‡•ã ‡§ë‡§®-‡§°‡§ø‡§µ‡§æ‡§á‡§∏ ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡•ã‡§Ç, ‡§∏‡§∞‡§≤ ‡§ö‡•à‡§ü‡§¨‡•â‡§ü‡•ç‡§∏ ‡§î‡§∞ ‡§ï‡§Æ ‡§µ‡§ø‡§≤‡§Ç‡§¨‡§§‡§æ ‡§ï‡•Ä ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï‡§§‡§æ ‡§µ‡§æ‡§≤‡•á ‡§ï‡§æ‡§∞‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§¶‡§∞‡•ç‡§∂ ‡§π‡•à‡•§',
                        cta: '‡§î‡§∞ ‡§ú‡§æ‡§®‡•á‡§Ç'
                    },
                    'm': { 
                        name: '‡§≤‡§æ‡§Æ‡§æ 3.2 70B', 
                        icon: 'üß†',
                        description: '‡§∏‡§Ç‡§§‡•Å‡§≤‡§ø‡§§ ‡§Æ‡•â‡§°‡§≤, ‡§ú‡•ã ‡§â‡§¶‡•ç‡§Ø‡§Æ ‡§î‡§∞ ‡§°‡•á‡§µ‡§≤‡§™‡§∞ ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•Ä ‡§è‡§ï ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§∂‡•ç‡§∞‡•É‡§Ç‡§ñ‡§≤‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§î‡§∞ ‡§¶‡§ï‡•ç‡§∑‡§§‡§æ ‡§ï‡§æ ‡§è‡§ï ‡§¨‡§°‡§º‡§æ ‡§∏‡§Ç‡§Ø‡•ã‡§ú‡§® ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§',
                        cta: '‡§î‡§∞ ‡§ú‡§æ‡§®‡•á‡§Ç'
                    },
                    'l': { 
                        name: '‡§≤‡§æ‡§Æ‡§æ 3.2 405B+', 
                        icon: 'üèÜ',
                        description: '‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ, ‡§Ö‡§§‡•ç‡§Ø‡§æ‡§ß‡•Å‡§®‡§ø‡§ï ‡§Æ‡•â‡§°‡§≤ ‡§ú‡•ã ‡§∏‡§¨‡§∏‡•á ‡§ú‡§ü‡§ø‡§≤ ‡§§‡§∞‡•ç‡§ï ‡§î‡§∞ ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§ú‡§º‡§æ‡§á‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§∏‡§∞‡•ç‡§µ‡•ã‡§§‡•ç‡§§‡§Æ ‡§Æ‡§æ‡§≤‡§ø‡§ï‡§æ‡§®‡§æ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§ü‡§ï‡•ç‡§ï‡§∞ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§',
                        cta: '‡§î‡§∞ ‡§ú‡§æ‡§®‡•á‡§Ç'
                    }
                }
            };
            
            const careerPathData = {
                'en': {
                    path: [
                        { rank: "Openness", desc: "Committed to being open source, allowing for widespread access, innovation, and scrutiny by the global research community." },
                        { rank: "State-of-the-Art Performance", desc: "Aims to be the best open model, directly competing with and even surpassing top closed-source models on key benchmarks." },
                        { rank: "Full Multimodality", desc: "Natively built to handle text, images, audio, and video, enabling more seamless and powerful applications." },
                        { rank: "Responsible Development", desc: "A strong emphasis on safety through advanced alignment techniques and the release of tools like Llama Guard to help developers build responsibly." }
                    ]
                },
                'hi': {
                    path: [
                        { rank: "‡§ñ‡•Å‡§≤‡§æ‡§™‡§®", desc: "‡§ì‡§™‡§® ‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡•ç‡§∞‡§§‡§ø‡§¨‡§¶‡•ç‡§ß, ‡§µ‡•à‡§∂‡•ç‡§µ‡§ø‡§ï ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§µ‡•ç‡§Ø‡§æ‡§™‡§ï ‡§™‡§π‡•Å‡§Ç‡§ö, ‡§®‡§µ‡§æ‡§ö‡§æ‡§∞ ‡§î‡§∞ ‡§ú‡§æ‡§Ç‡§ö ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§" },
                        { rank: "‡§Ö‡§§‡•ç‡§Ø‡§æ‡§ß‡•Å‡§®‡§ø‡§ï ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®", desc: "‡§∏‡§∞‡•ç‡§µ‡§∂‡•ç‡§∞‡•á‡§∑‡•ç‡§† ‡§ñ‡•Å‡§≤‡§æ ‡§Æ‡•â‡§°‡§≤ ‡§¨‡§®‡§®‡•á ‡§ï‡§æ ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø, ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§¨‡•á‡§Ç‡§ö‡§Æ‡§æ‡§∞‡•ç‡§ï ‡§™‡§∞ ‡§∂‡•Ä‡§∞‡•ç‡§∑ ‡§¨‡§Ç‡§¶-‡§∏‡•ç‡§∞‡•ã‡§§ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡•Ä‡§ß‡•á ‡§™‡•ç‡§∞‡§§‡§ø‡§∏‡•ç‡§™‡§∞‡•ç‡§ß‡§æ ‡§ï‡§∞‡§®‡§æ ‡§î‡§∞ ‡§Ø‡§π‡§æ‡§Ç ‡§§‡§ï ‡§ï‡§ø ‡§â‡§®‡§∏‡•á ‡§Ü‡§ó‡•á ‡§®‡§ø‡§ï‡§≤‡§®‡§æ‡•§" },
                        { rank: "‡§™‡•Ç‡§∞‡•ç‡§£ ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤", desc: "‡§Æ‡•Ç‡§≤ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§™‡§æ‡§†, ‡§ö‡§ø‡§§‡•ç‡§∞, ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§î‡§∞ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§ï‡•ã ‡§∏‡§Ç‡§≠‡§æ‡§≤‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§Ö‡§ß‡§ø‡§ï ‡§∏‡§π‡§ú ‡§î‡§∞ ‡§∂‡§ï‡•ç‡§§‡§ø‡§∂‡§æ‡§≤‡•Ä ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§" },
                        { rank: "‡§ú‡§ø‡§Æ‡•ç‡§Æ‡•á‡§¶‡§æ‡§∞ ‡§µ‡§ø‡§ï‡§æ‡§∏", desc: "‡§â‡§®‡•ç‡§®‡§§ ‡§∏‡§Ç‡§∞‡•á‡§ñ‡§£ ‡§§‡§ï‡§®‡•Ä‡§ï‡•ã‡§Ç ‡§ï‡•á ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ ‡§∏‡•á ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§™‡§∞ ‡§è‡§ï ‡§Æ‡§ú‡§¨‡•Ç‡§§ ‡§ú‡•ã‡§∞ ‡§î‡§∞ ‡§°‡•á‡§µ‡§≤‡§™‡§∞‡•ç‡§∏ ‡§ï‡•ã ‡§ú‡§ø‡§Æ‡•ç‡§Æ‡•á‡§¶‡§æ‡§∞‡•Ä ‡§∏‡•á ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§Æ‡•á‡§Ç ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§≤‡§æ‡§Æ‡§æ ‡§ó‡§æ‡§∞‡•ç‡§° ‡§ú‡•à‡§∏‡•á ‡§â‡§™‡§ï‡§∞‡§£‡•ã‡§Ç ‡§ï‡•Ä ‡§∞‡§ø‡§π‡§æ‡§à‡•§" }
                    ]
                }
            };

            const translations = {
                'en': {
                    'header-title': 'ü¶ô Meta\'s Llama 3.2: The Ultimate Guide ü¶ô',
                    'header-subtitle': 'The Next Evolution in Open Source AI',
                    'sec1-title': '1. What is Meta\'s Llama 3.2? üí°',
                    'sec1-p1': 'Llama 3.2 is the next-generation family of open-source large language models from Meta. Building on the massive success of Llama 2, this new series represents a significant leap in performance, efficiency, and capability. Llama 3.2 introduces **full multimodality**, allowing it to understand and process not just text, but also images, audio, and video. It is designed to be the most powerful and accessible open-source model, empowering developers to create more sophisticated and helpful AI applications than ever before.',
                    'sec2-title': '2. The Technology Behind Llama 3.2 üéØ',
                    'sec2-p1': 'Llama 3.2 utilizes an advanced Transformer architecture and is refined with state-of-the-art alignment techniques for superior performance and safety.',
                    'sec2-step1-title': 'Stage 1:', 'sec2-step1-name': 'Pre-training', 'sec2-step1-desc': 'Learning from a massive multimodal dataset',
                    'sec2-step2-title': 'Stage 2:', 'sec2-step2-name': 'Post-training', 'sec2-step2-desc': 'Fine-tuning with SFT & RLAIF',
                    'sec2-step3-title': 'Stage 3:', 'sec2-step3-name': 'Red Teaming', 'sec2-step3-desc': 'Extensive safety and adversarial testing',
                    'sec3-title': '3. Who Can Use It? ‚úÖ',
                    'sec3-p1': 'Following its open-source philosophy, Meta aims to make Llama 3.2 widely available across numerous platforms.',
                    'sec3-age-title': 'Open Source Community:', 'sec3-age-desc': 'The model weights are freely available for download for research and most commercial applications.',
                    'sec3-edu-title': 'Cloud Platforms:', 'sec3-edu-desc': 'Available on all major cloud providers like AWS, Google Cloud, and Microsoft Azure for easy, scalable deployment.',
                    'sec3-attempts-title': 'Hardware & Devices üìä',
                    'sec3-agerelax-list': '<li><strong>Optimized Versions:</strong> Llama 3.2 will be available in various sizes, optimized to run on a wide range of hardware, from large GPU clusters to personal computers and mobile devices.</li><li><strong>Hardware Partners:</strong> Meta is collaborating with partners like NVIDIA, Intel, and Qualcomm to optimize performance on their latest chips.</li>',
                    'sec4-title': '4. Next-Generation Capabilities üìö',
                    'sec4-p1': 'Llama 3.2 introduces full multimodality and significantly enhanced reasoning, setting a new standard for open-source models.',
                    'gs1-title': 'Full Multimodality',
                    'gs2-title': 'Advanced Reasoning',
                    'gs3-title': 'Massive Context',
                    'gs4-title': 'Improved Safety',
                    'sec5-title': '5. How is Llama 3.2 Trained? üèõÔ∏è',
                    'sec5-p1': 'Llama 3.2 is trained on an unprecedented scale, using a massive custom-built GPU infrastructure and a highly curated dataset.',
                    'sec5-hindi-title': '1. The Dataset', 'sec5-hindi-p1': 'A vastly expanded and more recent dataset.', 'sec5-hindi-list': '<li>Pre-trained on over 15 trillion tokens of publicly available data, including text, code, images, and video.</li><li>The data undergoes extensive filtering and curation to improve quality and reduce bias.</li>',
                    'sec5-essay-title': '2. The Alignment Process', 'sec5-essay-p1': 'A combination of advanced techniques.', 'sec5-essay-list': '<li>Uses a combination of Supervised Fine-Tuning (SFT), Reinforcement Learning with Human Feedback (RLHF), and a new technique, Reinforcement Learning with AI Feedback (RLAIF).</li><li>RLAIF allows for faster and more scalable safety alignment by using a powerful AI model to help label data.</li>',
                    'sec6-title': '6. Key Applications & Use Cases üåü',
                    'sec6-p1': 'Llama 3.2\'s multimodal and enhanced reasoning capabilities open up new frontiers for AI applications.',
                    'sec6-eng-title': 'AI Agents', 'sec6-eng-list': '<li>Building sophisticated autonomous agents that can reason and take multi-step actions.</li><li>Creating more capable and helpful personal and professional assistants.</li>',
                    'sec6-elec-title': 'Multimodal Content', 'sec6-elec-list': '<li>Generating video from text or images.</li><li>Creating presentations from documents.</li><li>Analyzing and describing the content of videos.</li>',
                    'sec6-naic-title': 'Advanced Coding', 'sec6-naic-list': '<li>Generating entire codebases from high-level specifications.</li><li>Advanced debugging and code optimization.</li><li>Serving as an expert pair programmer for complex projects.</li>',
                    'sec7-title': '7. Performance & Benchmarks üéôÔ∏è',
                    'sec7-p1': 'Llama 3.2 is expected to set new state-of-the-art performance records for open-source models, rivaling and even surpassing the best proprietary models available.',
                    'sec7-qualities-title': 'Key Benchmarks:',
                    'sec7-qualities-list': '<li class="flex items-start"><span class="text-2xl text-[#0ea5e9] mr-3">üß†</span><div><strong class="text-white">Reasoning & Knowledge (MMLU):</strong> Expected to achieve near-human-level scores on general knowledge and reasoning tasks.</div></li><li class="flex items-start"><span class="text-2xl text-[#0ea5e9] mr-3">üíª</span><div><strong class="text-white">Coding (HumanEval):</strong> A significant leap in performance for complex code generation, debugging, and planning.</div></li><li class="flex items-start"><span class="text-2xl text-[#0ea5e9] mr-3">üñºÔ∏è</span><div><strong class="text-white">Multimodal Benchmarks:</strong> Strong performance on benchmarks that test the ability to reason across text, images, and video.</div></li>',
                    'sec8-title': '8. Prompting Llama 3.2: Best Practices üìà',
                    'sec8-p1': 'To effectively use Llama 3.2\'s multimodal and reasoning capabilities, prompts should be structured to provide rich context across different data types.',
                    'sec8-steps-title': 'üìö Key Principles:',
                    'sec8-step1-title': 'Interleave Modalities', 'sec8-step1-desc': 'Provide a sequence of text and images in your prompt to ask complex visual reasoning questions.',
                    'sec8-step2-title': 'Leverage the Context Window', 'sec8-step2-desc': 'Provide entire documents, codebases, or transcripts as context for in-depth analysis and question-answering.',
                    'sec8-revision-title': 'üîÑ Advanced Techniques:',
                    'sec8-revision-list': '<li><strong>Chain of Thought for Multimodality:</strong> Ask the model to "think step by step" when analyzing complex images or charts to improve accuracy.</li><li><strong>System Prompts for Safety:</strong> Use robust system prompts and tools like Llama Guard 2 to ensure the model\'s outputs remain safe and on-topic.</li>',
                    'sec9-title': '9. Model Evolution üóìÔ∏è',
                    'sec9-p1': 'Llama 3.2 represents a major milestone in Meta\'s mission to build and openly share advanced AI technology.',
                    'sec9-step2-title': 'Llama 2 Release', 'sec9-step2-desc': '<strong>Timeline:</strong> July 2023. Established a new standard for open-source LLMs and enabled commercial use.',
                    'sec9-step3-title': 'Llama 3 Release', 'sec9-step3-desc': '<strong>Timeline:</strong> April 2024. Delivered a major performance leap with models up to 70B parameters, outperforming many proprietary models.',
                    'sec9-step4-title': 'Llama 3.2 Release', 'sec9-step4-desc': '<strong>Timeline:</strong> Q4 2025 (Hypothetical). The introduction of a fully multimodal, 400B+ parameter model that redefines state-of-the-art for open-source AI.',
                    'sec10-title': '10. Limitations & Responsible Use üö´',
                    'sec10-p1': 'As AI models become more powerful, a commitment to responsible development and deployment is more important than ever.',
                    'sec10-mistake1-title': 'Misinformation Potential:', 'sec10-mistake1-desc': 'The ability to generate realistic multimodal content increases the risk of creating sophisticated misinformation. Fact-checking is essential.',
                    'sec10-mistake2-title': 'Bias Amplification:', 'sec10-mistake2-desc': 'Models can still reflect and amplify biases present in their training data, requiring careful implementation of safety guardrails.',
                    'sec10-mistake3-title': 'Resource Intensive:', 'sec10-mistake3-desc': 'Running the largest versions of Llama 3.2 requires significant computational resources, which can be a barrier for individual developers.',
                    'sec10-mistake4-title': 'Responsible Use:', 'sec10-mistake4-desc': 'All users must adhere to Meta\'s Acceptable Use Policy and implement safety tools like Llama Guard 2 to prevent misuse.',
                    'sec11-title': '11. How to Access & Use Llama 3.2 üè´',
                    'sec11-p1': 'Meta will continue its strategy of broad and open distribution for Llama 3.2.',
                    'sec11-gov-title': 'üìö For Developers & Hobbyists', 'sec11-gov-list': '<li>**Hugging Face:** The easiest way to get started with the models using the `transformers` library.</li><li>**Direct Download:** Apply for access on the Meta AI website to download the model weights.</li>',
                    'sec11-pvt-title': 'üíª For Enterprise Use', 'sec11-pvt-p1': 'Use managed cloud services for scalable deployments.', 'sec11-pvt-list': '<li>Available on all major cloud platforms, including Microsoft Azure, AWS, and Google Cloud.</li>',
                    'sec12-title': '12. The Future: Open-Source AGI üöÄ',
                    'sec12-p1': 'The release of Llama 2 has catalyzed a global wave of innovation in open-source AI. The upcoming Llama 3 is highly anticipated and is expected to feature full multimodality and even more powerful reasoning capabilities, further narrowing the gap between open and closed-source models and empowering a new generation of AI-powered products and services.',
                    'sec12-career-btn': 'View Key Differentiators',
                    'sec13-title': '13. Core AI Concepts üìö',
                    'sec13-p1': 'Understanding these core concepts is fundamental to appreciating the technology behind Llama 3.2.',
                    'syllabus-oir-title': 'Open Source AI',
                    'syllabus-oir-list': '<div class="subject-item flex-col sm:flex-row"><div class="flex items-center w-full"><span class="icon">üåê</span><div class="flex-grow">Democratizing AI</div></div><div class="w-full sm:w-auto mt-2 sm:mt-0 flex justify-end flex-shrink-0 space-x-2"><button class="notes-btn">Notes</button><button class="lectures-btn">Lectures</button></div></div>',
                    'syllabus-ppdt-title': 'Advanced Alignment',
                    'syllabus-ppdt-list': '<div class="subject-item flex-col sm:flex-row"><div class="flex items-center w-full"><span class="icon">ü§ù</span><div class="flex-grow">RLAIF & SFT</div></div><div class="w-full sm:w-auto mt-2 sm:mt-0 flex justify-end flex-shrink-0 space-x-2"><button class="notes-btn">Notes</button><button class="lectures-btn">Lectures</button></div></div>',
                    'footer-text': '&copy; 2025 | Building the future of AI, openly.',
                    'interviewChartData': {
                        labels: ['MMLU', 'GSM8K', 'HumanEval', 'MATH'],
                        data: [85.5, 96.0, 85.1, 75.3] // Hypothetical Llama 3.2 scores
                    }
                },
                'hi': {
                    'header-title': 'ü¶ô ‡§Æ‡•á‡§ü‡§æ ‡§ï‡§æ ‡§≤‡§æ‡§Æ‡§æ 3.2: ‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§ó‡§æ‡§á‡§° ü¶ô',
                    'header-subtitle': '‡§ì‡§™‡§® ‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§è‡§Ü‡§à ‡§Æ‡•á‡§Ç ‡§Ö‡§ó‡§≤‡§æ ‡§µ‡§ø‡§ï‡§æ‡§∏',
                    'sec1-title': '1. ‡§Æ‡•á‡§ü‡§æ ‡§ï‡§æ ‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à? üí°',
                    'sec1-p1': '‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§Æ‡•á‡§ü‡§æ ‡§∏‡•á ‡§ì‡§™‡§®-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§¨‡§°‡§º‡•á ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•Ä ‡§Ö‡§ó‡§≤‡•Ä ‡§™‡•Ä‡§¢‡§º‡•Ä ‡§ï‡§æ ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§π‡•à‡•§ ‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡•Ä ‡§≠‡§æ‡§∞‡•Ä ‡§∏‡§´‡§≤‡§§‡§æ ‡§™‡§∞ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§, ‡§Ø‡§π ‡§®‡§à ‡§∂‡•ç‡§∞‡•É‡§Ç‡§ñ‡§≤‡§æ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®, ‡§¶‡§ï‡•ç‡§∑‡§§‡§æ ‡§î‡§∞ ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§õ‡§≤‡§æ‡§Ç‡§ó ‡§ï‡§æ ‡§™‡•ç‡§∞‡§§‡§ø‡§®‡§ø‡§ß‡§ø‡§§‡•ç‡§µ ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à‡•§ ‡§≤‡§æ‡§Æ‡§æ 3.2 **‡§™‡•Ç‡§∞‡•ç‡§£ ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤** ‡§ï‡§æ ‡§™‡§∞‡§ø‡§ö‡§Ø ‡§¶‡•á‡§§‡§æ ‡§π‡•à, ‡§ú‡§ø‡§∏‡§∏‡•á ‡§Ø‡§π ‡§® ‡§ï‡•á‡§µ‡§≤ ‡§™‡§æ‡§†, ‡§¨‡§≤‡•ç‡§ï‡§ø ‡§õ‡§µ‡§ø‡§Ø‡•ã‡§Ç, ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§î‡§∞ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§ï‡•ã ‡§≠‡•Ä ‡§∏‡§Æ‡§ù ‡§î‡§∞ ‡§∏‡§Ç‡§∏‡§æ‡§ß‡§ø‡§§ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§ ‡§á‡§∏‡•á ‡§∏‡§¨‡§∏‡•á ‡§∂‡§ï‡•ç‡§§‡§ø‡§∂‡§æ‡§≤‡•Ä ‡§î‡§∞ ‡§∏‡•Å‡§≤‡§≠ ‡§ì‡§™‡§®-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§°‡§ø‡§ú‡§º‡§æ‡§á‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§°‡•á‡§µ‡§≤‡§™‡§∞‡•ç‡§∏ ‡§ï‡•ã ‡§™‡§π‡§≤‡•á ‡§∏‡•á ‡§ï‡§π‡•Ä‡§Ç ‡§Ö‡§ß‡§ø‡§ï ‡§™‡§∞‡§ø‡§∑‡•ç‡§ï‡•É‡§§ ‡§î‡§∞ ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§è‡§Ü‡§à ‡§è‡§™‡•ç‡§≤‡§ø‡§ï‡•á‡§∂‡§® ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§∂‡§ï‡•ç‡§§ ‡§¨‡§®‡§æ‡§§‡§æ ‡§π‡•à‡•§',
                    'sec2-title': '2. ‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§ï‡•á ‡§™‡•Ä‡§õ‡•á ‡§ï‡•Ä ‡§§‡§ï‡§®‡•Ä‡§ï üéØ',
                    'sec2-p1': '‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§è‡§ï ‡§â‡§®‡•ç‡§®‡§§ ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡§æ‡§∞‡•ç‡§Æ‡§∞ ‡§µ‡§æ‡§∏‡•ç‡§§‡•Å‡§ï‡§≤‡§æ ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§¨‡•á‡§π‡§§‡§∞ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§î‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§§‡•ç‡§Ø‡§æ‡§ß‡•Å‡§®‡§ø‡§ï ‡§∏‡§Ç‡§∞‡•á‡§ñ‡§£ ‡§§‡§ï‡§®‡•Ä‡§ï‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§æ‡§• ‡§™‡§∞‡§ø‡§∑‡•ç‡§ï‡•É‡§§ ‡§π‡•à‡•§',
                    'sec2-step1-title': '‡§ö‡§∞‡§£ 1:', 'sec2-step1-name': '‡§™‡•Ç‡§∞‡•ç‡§µ-‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£', 'sec2-step1-desc': '‡§è‡§ï ‡§µ‡§ø‡§∂‡§æ‡§≤ ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤ ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§∏‡•á ‡§∏‡•Ä‡§ñ‡§®‡§æ',
                    'sec2-step2-title': '‡§ö‡§∞‡§£ 2:', 'sec2-step2-name': '‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§ï‡•á ‡§¨‡§æ‡§¶', 'sec2-step2-desc': 'SFT ‡§î‡§∞ RLAIF ‡§ï‡•á ‡§∏‡§æ‡§• ‡§´‡§æ‡§á‡§®-‡§ü‡•ç‡§Ø‡•Ç‡§®‡§ø‡§Ç‡§ó',
                    'sec2-step3-title': '‡§ö‡§∞‡§£ 3:', 'sec2-step3-name': '‡§∞‡•á‡§° ‡§ü‡•Ä‡§Æ‡§ø‡§Ç‡§ó', 'sec2-step3-desc': '‡§µ‡•ç‡§Ø‡§æ‡§™‡§ï ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§î‡§∞ ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•Ç‡§≤ ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£',
                    'sec3-title': '3. ‡§á‡§∏‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡•å‡§® ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à? ‚úÖ',
                    'sec3-p1': '‡§Ö‡§™‡§®‡•á ‡§ì‡§™‡§®-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§æ ‡§™‡§æ‡§≤‡§® ‡§ï‡§∞‡§§‡•á ‡§π‡•Å‡§è, ‡§Æ‡•á‡§ü‡§æ ‡§ï‡§æ ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§ï‡•ã ‡§ï‡§à ‡§™‡•ç‡§≤‡•á‡§ü‡§´‡§æ‡§∞‡•ç‡§Æ‡•ã‡§Ç ‡§™‡§∞ ‡§µ‡•ç‡§Ø‡§æ‡§™‡§ï ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§ï‡§∞‡§æ‡§®‡§æ ‡§π‡•à‡•§',
                    'sec3-age-title': '‡§ì‡§™‡§® ‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø:', 'sec3-age-desc': '‡§Æ‡•â‡§°‡§≤ ‡§≠‡§æ‡§∞ ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§î‡§∞ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§Ç‡§∂ ‡§µ‡§æ‡§£‡§ø‡§ú‡•ç‡§Ø‡§ø‡§ï ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡•ç‡§µ‡§§‡§Ç‡§§‡•ç‡§∞ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•à‡§Ç‡•§',
                    'sec3-edu-title': '‡§ï‡•ç‡§≤‡§æ‡§â‡§° ‡§™‡•ç‡§≤‡•á‡§ü‡§´‡•â‡§∞‡•ç‡§Æ:', 'sec3-edu-desc': '‡§Ü‡§∏‡§æ‡§®, ‡§∏‡•ç‡§ï‡•á‡§≤‡•á‡§¨‡§≤ ‡§™‡§∞‡§ø‡§®‡§ø‡§Ø‡•ã‡§ú‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§°‡§¨‡•ç‡§≤‡•ç‡§Ø‡•Ç‡§è‡§∏, ‡§ó‡•Ç‡§ó‡§≤ ‡§ï‡•ç‡§≤‡§æ‡§â‡§° ‡§î‡§∞ ‡§Æ‡§æ‡§á‡§ï‡•ç‡§∞‡•ã‡§∏‡•â‡§´‡•ç‡§ü ‡§è‡§ú‡§º‡•ç‡§Ø‡•ã‡§∞ ‡§ú‡•à‡§∏‡•á ‡§∏‡§≠‡•Ä ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§ï‡•ç‡§≤‡§æ‡§â‡§° ‡§™‡•ç‡§∞‡§¶‡§æ‡§§‡§æ‡§ì‡§Ç ‡§™‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•à‡•§',
                    'sec3-attempts-title': '‡§π‡§æ‡§∞‡•ç‡§°‡§µ‡•á‡§Ø‡§∞ ‡§î‡§∞ ‡§â‡§™‡§ï‡§∞‡§£ üìä',
                    'sec3-agerelax-list': '<li><strong>‡§Ö‡§®‡•Å‡§ï‡•Ç‡§≤‡§ø‡§§ ‡§∏‡§Ç‡§∏‡•ç‡§ï‡§∞‡§£:</strong> ‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§Ü‡§ï‡§æ‡§∞‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã‡§ó‡§æ, ‡§ú‡•ã ‡§¨‡§°‡§º‡•á ‡§ú‡•Ä‡§™‡•Ä‡§Ø‡•Ç ‡§ï‡•ç‡§≤‡§∏‡•ç‡§ü‡§∞ ‡§∏‡•á ‡§≤‡•á‡§ï‡§∞ ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§ó‡§§ ‡§ï‡§Ç‡§™‡•ç‡§Ø‡•Ç‡§ü‡§∞ ‡§î‡§∞ ‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§â‡§™‡§ï‡§∞‡§£‡•ã‡§Ç ‡§§‡§ï, ‡§π‡§æ‡§∞‡•ç‡§°‡§µ‡•á‡§Ø‡§∞ ‡§ï‡•Ä ‡§è‡§ï ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§∂‡•ç‡§∞‡•É‡§Ç‡§ñ‡§≤‡§æ ‡§™‡§∞ ‡§ö‡§≤‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§®‡•Å‡§ï‡•Ç‡§≤‡§ø‡§§ ‡§π‡•à‡•§</li><li><strong>‡§π‡§æ‡§∞‡•ç‡§°‡§µ‡•á‡§Ø‡§∞ ‡§≠‡§æ‡§ó‡•Ä‡§¶‡§æ‡§∞:</strong> ‡§Æ‡•á‡§ü‡§æ ‡§Ö‡§™‡§®‡•á ‡§®‡§µ‡•Ä‡§®‡§§‡§Æ ‡§ö‡§ø‡§™‡•ç‡§∏ ‡§™‡§∞ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡•ã ‡§Ö‡§®‡•Å‡§ï‡•Ç‡§≤‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§®‡§µ‡•Ä‡§°‡§ø‡§Ø‡§æ, ‡§á‡§Ç‡§ü‡•á‡§≤ ‡§î‡§∞ ‡§ï‡•ç‡§µ‡§æ‡§≤‡§ï‡•â‡§Æ ‡§ú‡•à‡§∏‡•á ‡§≠‡§æ‡§ó‡•Ä‡§¶‡§æ‡§∞‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡§π‡§Ø‡•ã‡§ó ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à‡•§</li>',
                    'sec4-title': '4. ‡§Ö‡§ó‡§≤‡•Ä ‡§™‡•Ä‡§¢‡§º‡•Ä ‡§ï‡•Ä ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ‡§è‡§Ç üìö',
                    'sec4-p1': '‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤ ‡§î‡§∞ ‡§ï‡§æ‡§´‡•Ä ‡§¨‡§¢‡§º‡•Ä ‡§π‡•Å‡§à ‡§§‡§∞‡•ç‡§ï ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§ï‡§æ ‡§™‡§∞‡§ø‡§ö‡§Ø ‡§¶‡•á‡§§‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§ì‡§™‡§®-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§®‡§Ø‡§æ ‡§Æ‡§æ‡§®‡§ï ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§',
                    'gs1-title': '‡§™‡•Ç‡§∞‡•ç‡§£ ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤',
                    'gs2-title': '‡§â‡§®‡•ç‡§®‡§§ ‡§§‡§∞‡•ç‡§ï',
                    'gs3-title': '‡§µ‡§ø‡§∂‡§æ‡§≤ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠',
                    'gs4-title': '‡§¨‡•á‡§π‡§§‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ',
                    'sec5-title': '5. ‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§ï‡•ã ‡§ï‡•à‡§∏‡•á ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à? üèõÔ∏è',
                    'sec5-p1': '‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§ï‡•ã ‡§è‡§ï ‡§Ö‡§≠‡•Ç‡§§‡§™‡•Ç‡§∞‡•ç‡§µ ‡§™‡•à‡§Æ‡§æ‡§®‡•á ‡§™‡§∞ ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à, ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§µ‡§ø‡§∂‡§æ‡§≤ ‡§ï‡§∏‡•ç‡§ü‡§Æ-‡§®‡§ø‡§∞‡•ç‡§Æ‡§ø‡§§ ‡§ú‡•Ä‡§™‡•Ä‡§Ø‡•Ç ‡§¨‡•Å‡§®‡§ø‡§Ø‡§æ‡§¶‡•Ä ‡§¢‡§æ‡§Ç‡§ö‡•á ‡§î‡§∞ ‡§è‡§ï ‡§â‡§ö‡•ç‡§ö ‡§ï‡•ç‡§Ø‡•Ç‡§∞‡•á‡§ü‡•á‡§° ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§',
                    'sec5-hindi-title': '1. ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü', 'sec5-hindi-p1': '‡§è‡§ï ‡§¨‡§π‡•Å‡§§ ‡§µ‡§ø‡§∏‡•ç‡§§‡§æ‡§∞‡§ø‡§§ ‡§î‡§∞ ‡§Ö‡§ß‡§ø‡§ï ‡§π‡§æ‡§≤‡§ø‡§Ø‡§æ ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü‡•§', 'sec5-hindi-list': '<li>‡§™‡§æ‡§†, ‡§ï‡•ã‡§°, ‡§õ‡§µ‡§ø‡§Ø‡•ã‡§Ç ‡§î‡§∞ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§∏‡§π‡§ø‡§§ ‡§∏‡§æ‡§∞‡•ç‡§µ‡§ú‡§®‡§ø‡§ï ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§°‡•á‡§ü‡§æ ‡§ï‡•á 15 ‡§ü‡•ç‡§∞‡§ø‡§≤‡§ø‡§Ø‡§® ‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§ü‡•ã‡§ï‡§® ‡§™‡§∞ ‡§™‡•Ç‡§∞‡•ç‡§µ-‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§ø‡§§‡•§</li><li>‡§ó‡•Å‡§£‡§µ‡§§‡•ç‡§§‡§æ ‡§Æ‡•á‡§Ç ‡§∏‡•Å‡§ß‡§æ‡§∞ ‡§î‡§∞ ‡§™‡•Ç‡§∞‡•ç‡§µ‡§æ‡§ó‡•ç‡§∞‡§π ‡§ï‡•ã ‡§ï‡§Æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡•á‡§ü‡§æ ‡§µ‡•ç‡§Ø‡§æ‡§™‡§ï ‡§´‡§º‡§ø‡§≤‡•ç‡§ü‡§∞‡§ø‡§Ç‡§ó ‡§î‡§∞ ‡§ï‡•ç‡§Ø‡•Ç‡§∞‡•á‡§∂‡§® ‡§∏‡•á ‡§ó‡•Å‡§ú‡§∞‡§§‡§æ ‡§π‡•à‡•§</li>',
                    'sec5-essay-title': '2. ‡§∏‡§Ç‡§∞‡•á‡§ñ‡§£ ‡§™‡•ç‡§∞‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ', 'sec5-essay-p1': '‡§â‡§®‡•ç‡§®‡§§ ‡§§‡§ï‡§®‡•Ä‡§ï‡•ã‡§Ç ‡§ï‡§æ ‡§è‡§ï ‡§∏‡§Ç‡§Ø‡•ã‡§ú‡§®‡•§', 'sec5-essay-list': '<li>‡§™‡§∞‡•ç‡§Ø‡§µ‡•á‡§ï‡•ç‡§∑‡§ø‡§§ ‡§´‡§æ‡§á‡§®-‡§ü‡•ç‡§Ø‡•Ç‡§®‡§ø‡§Ç‡§ó (‡§è‡§∏‡§è‡§´‡§ü‡•Ä), ‡§Æ‡§æ‡§®‡§µ ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡•Å‡§¶‡•É‡§¢‡•Ä‡§ï‡§∞‡§£ ‡§∏‡•Ä‡§ñ‡§®‡§æ (‡§Ü‡§∞‡§è‡§≤‡§è‡§ö‡§è‡§´), ‡§î‡§∞ ‡§è‡§ï ‡§®‡§à ‡§§‡§ï‡§®‡•Ä‡§ï, ‡§è‡§Ü‡§à ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡•Å‡§¶‡•É‡§¢‡•Ä‡§ï‡§∞‡§£ ‡§∏‡•Ä‡§ñ‡§®‡§æ (‡§Ü‡§∞‡§è‡§≤‡§è‡§Ü‡§à‡§è‡§´) ‡§ï‡•á ‡§∏‡§Ç‡§Ø‡•ã‡§ú‡§® ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§</li><li>‡§Ü‡§∞‡§è‡§≤‡§è‡§Ü‡§à‡§è‡§´ ‡§°‡•á‡§ü‡§æ ‡§ï‡•ã ‡§≤‡•á‡§¨‡§≤ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§∂‡§ï‡•ç‡§§‡§ø‡§∂‡§æ‡§≤‡•Ä ‡§è‡§Ü‡§à ‡§Æ‡•â‡§°‡§≤ ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§ï‡•á ‡§§‡•á‡§ú‡•Ä ‡§∏‡•á ‡§î‡§∞ ‡§Ö‡§ß‡§ø‡§ï ‡§∏‡•ç‡§ï‡•á‡§≤‡•á‡§¨‡§≤ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§∏‡§Ç‡§∞‡•á‡§ñ‡§£ ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§</li>',
                    'sec6-title': '6. ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§î‡§∞ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡•á ‡§Æ‡§æ‡§Æ‡§≤‡•á üåü',
                    'sec6-p1': '‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§ï‡•Ä ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤ ‡§î‡§∞ ‡§¨‡§¢‡§º‡•Ä ‡§π‡•Å‡§à ‡§§‡§∞‡•ç‡§ï ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ‡§è‡§Ç ‡§è‡§Ü‡§à ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§®‡§à ‡§∏‡•Ä‡§Æ‡§æ‡§è‡§Ç ‡§ñ‡•ã‡§≤‡§§‡•Ä ‡§π‡•à‡§Ç‡•§',
                    'sec6-eng-title': '‡§è‡§Ü‡§à ‡§è‡§ú‡•á‡§Ç‡§ü', 'sec6-eng-list': '<li>‡§™‡§∞‡§ø‡§∑‡•ç‡§ï‡•É‡§§ ‡§∏‡•ç‡§µ‡§æ‡§Ø‡§§‡•ç‡§§ ‡§è‡§ú‡•á‡§Ç‡§ü ‡§¨‡§®‡§æ‡§®‡§æ ‡§ú‡•ã ‡§§‡§∞‡•ç‡§ï ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§¨‡§π‡•Å-‡§ö‡§∞‡§£‡•Ä‡§Ø ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§</li><li>‡§Ö‡§ß‡§ø‡§ï ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§î‡§∞ ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§ó‡§§ ‡§î‡§∞ ‡§™‡•á‡§∂‡•á‡§µ‡§∞ ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§¨‡§®‡§æ‡§®‡§æ‡•§</li>',
                    'sec6-elec-title': '‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤ ‡§∏‡§æ‡§Æ‡§ó‡•ç‡§∞‡•Ä', 'sec6-elec-list': '<li>‡§™‡§æ‡§† ‡§Ø‡§æ ‡§õ‡§µ‡§ø‡§Ø‡•ã‡§Ç ‡§∏‡•á ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§¨‡§®‡§æ‡§®‡§æ‡•§</li><li>‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡•ã‡§Ç ‡§∏‡•á ‡§™‡•ç‡§∞‡§∏‡•ç‡§§‡•Å‡§§‡§ø‡§Ø‡§æ‡§Å ‡§¨‡§®‡§æ‡§®‡§æ‡•§</li><li>‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§ï‡•Ä ‡§∏‡§æ‡§Æ‡§ó‡•ç‡§∞‡•Ä ‡§ï‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§î‡§∞ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡§®‡§æ‡•§</li>',
                    'sec6-naic-title': '‡§â‡§®‡•ç‡§®‡§§ ‡§ï‡•ã‡§°‡§ø‡§Ç‡§ó', 'sec6-naic-list': '<li>‡§â‡§ö‡•ç‡§ö-‡§∏‡•ç‡§§‡§∞‡•Ä‡§Ø ‡§µ‡§ø‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂‡•ã‡§Ç ‡§∏‡•á ‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§ï‡•ã‡§°‡§¨‡•á‡§∏ ‡§â‡§§‡•ç‡§™‡§®‡•ç‡§® ‡§ï‡§∞‡§®‡§æ‡•§</li><li>‡§â‡§®‡•ç‡§®‡§§ ‡§°‡•Ä‡§¨‡§ó‡§ø‡§Ç‡§ó ‡§î‡§∞ ‡§ï‡•ã‡§° ‡§Ö‡§®‡•Å‡§ï‡•Ç‡§≤‡§®‡•§</li><li>‡§ú‡§ü‡§ø‡§≤ ‡§™‡§∞‡§ø‡§Ø‡•ã‡§ú‡§®‡§æ‡§ì‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§ú‡•ã‡§°‡§º‡•Ä ‡§™‡•ç‡§∞‡•ã‡§ó‡•ç‡§∞‡§æ‡§Æ‡§∞ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§∏‡•á‡§µ‡§æ ‡§ï‡§∞‡§®‡§æ‡•§</li>',
                    'sec7-title': '7. ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§î‡§∞ ‡§¨‡•á‡§Ç‡§ö‡§Æ‡§æ‡§∞‡•ç‡§ï üéôÔ∏è',
                    'sec7-p1': '‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§∏‡•á ‡§ì‡§™‡§®-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§®‡§è ‡§Ö‡§§‡•ç‡§Ø‡§æ‡§ß‡•Å‡§®‡§ø‡§ï ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§° ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§â‡§Æ‡•ç‡§Æ‡•Ä‡§¶ ‡§π‡•à, ‡§ú‡•ã ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§∏‡§∞‡•ç‡§µ‡•ã‡§§‡•ç‡§§‡§Æ ‡§Æ‡§æ‡§≤‡§ø‡§ï‡§æ‡§®‡§æ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§ü‡§ï‡•ç‡§ï‡§∞ ‡§¶‡•á‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§Ø‡§π‡§æ‡§Ç ‡§§‡§ï ‡§ï‡§ø ‡§â‡§®‡§∏‡•á ‡§Ü‡§ó‡•á ‡§≠‡•Ä ‡§®‡§ø‡§ï‡§≤ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§',
                    'sec7-qualities-title': '‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§¨‡•á‡§Ç‡§ö‡§Æ‡§æ‡§∞‡•ç‡§ï:',
                    'sec7-qualities-list': '<li class="flex items-start"><span class="text-2xl text-[#0ea5e9] mr-3">üß†</span><div><strong class="text-white">‡§§‡§∞‡•ç‡§ï ‡§î‡§∞ ‡§ú‡•ç‡§û‡§æ‡§® (‡§è‡§Æ‡§è‡§Æ‡§è‡§≤‡§Ø‡•Ç):</strong> ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡•ç‡§û‡§æ‡§® ‡§î‡§∞ ‡§§‡§∞‡•ç‡§ï ‡§ï‡§æ‡§∞‡•ç‡§Ø‡•ã‡§Ç ‡§™‡§∞ ‡§Æ‡§æ‡§®‡§µ-‡§∏‡•ç‡§§‡§∞ ‡§ï‡•á ‡§®‡§ø‡§ï‡§ü ‡§∏‡•ç‡§ï‡•ã‡§∞ ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§â‡§Æ‡•ç‡§Æ‡•Ä‡§¶ ‡§π‡•à‡•§</div></li><li class="flex items-start"><span class="text-2xl text-[#0ea5e9] mr-3">üíª</span><div><strong class="text-white">‡§ï‡•ã‡§°‡§ø‡§Ç‡§ó (‡§π‡•ç‡§Ø‡•Ç‡§Æ‡§®‡§á‡§µ‡§≤):</strong> ‡§ú‡§ü‡§ø‡§≤ ‡§ï‡•ã‡§° ‡§ú‡§®‡§∞‡•á‡§∂‡§®, ‡§°‡•Ä‡§¨‡§ó‡§ø‡§Ç‡§ó ‡§î‡§∞ ‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§õ‡§≤‡§æ‡§Ç‡§ó‡•§</div></li><li class="flex items-start"><span class="text-2xl text-[#0ea5e9] mr-3">üñºÔ∏è</span><div><strong class="text-white">‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤ ‡§¨‡•á‡§Ç‡§ö‡§Æ‡§æ‡§∞‡•ç‡§ï:</strong> ‡§™‡§æ‡§†, ‡§õ‡§µ‡§ø‡§Ø‡•ã‡§Ç ‡§î‡§∞ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§Æ‡•á‡§Ç ‡§§‡§∞‡•ç‡§ï ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§ï‡§æ ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§¨‡•á‡§Ç‡§ö‡§Æ‡§æ‡§∞‡•ç‡§ï ‡§™‡§∞ ‡§Æ‡§ú‡§¨‡•Ç‡§§ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®‡•§</div></li>',
                    'sec8-title': '8. ‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§ï‡•ã ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü ‡§ï‡§∞‡§®‡§æ: ‡§∏‡§∞‡•ç‡§µ‡•ã‡§§‡•ç‡§§‡§Æ ‡§Ö‡§≠‡•ç‡§Ø‡§æ‡§∏ üìà',
                    'sec8-p1': '‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§ï‡•Ä ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤ ‡§î‡§∞ ‡§§‡§∞‡•ç‡§ï ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ‡§ì‡§Ç ‡§ï‡§æ ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡•Ä ‡§¢‡§Ç‡§ó ‡§∏‡•á ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü ‡§ï‡•ã ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§°‡•á‡§ü‡§æ ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§Ç‡§∞‡§ö‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§',
                    'sec8-steps-title': 'üìö ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§∏‡§ø‡§¶‡•ç‡§ß‡§æ‡§Ç‡§§:',
                    'sec8-step1-title': '‡§§‡•å‡§∞-‡§§‡§∞‡•Ä‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§á‡§Ç‡§ü‡§∞‡§≤‡•Ä‡§µ ‡§ï‡§∞‡•á‡§Ç', 'sec8-step1-desc': '‡§ú‡§ü‡§ø‡§≤ ‡§¶‡•É‡§∂‡•ç‡§Ø ‡§§‡§∞‡•ç‡§ï ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§™‡•Ç‡§õ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§™‡§®‡•á ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü ‡§Æ‡•á‡§Ç ‡§™‡§æ‡§† ‡§î‡§∞ ‡§õ‡§µ‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡§æ ‡§è‡§ï ‡§Ö‡§®‡•Å‡§ï‡•ç‡§∞‡§Æ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡•á‡§Ç‡•§',
                    'sec8-step2-title': '‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§µ‡§ø‡§Ç‡§°‡•ã ‡§ï‡§æ ‡§≤‡§æ‡§≠ ‡§â‡§†‡§æ‡§è‡§Ç', 'sec8-step2-desc': '‡§ó‡§π‡§® ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§î‡§∞ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®-‡§â‡§§‡•ç‡§§‡§∞ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º, ‡§ï‡•ã‡§°‡§¨‡•á‡§∏ ‡§Ø‡§æ ‡§™‡•ç‡§∞‡§§‡§ø‡§≤‡•á‡§ñ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡•á‡§Ç‡•§',
                    'sec8-revision-title': 'üîÑ ‡§â‡§®‡•ç‡§®‡§§ ‡§§‡§ï‡§®‡•Ä‡§ï‡•á‡§Ç:',
                    'sec8-revision-list': '<li><strong>‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§∂‡•ç‡§∞‡•É‡§Ç‡§ñ‡§≤‡§æ:</strong> ‡§∏‡§ü‡•Ä‡§ï‡§§‡§æ ‡§Æ‡•á‡§Ç ‡§∏‡•Å‡§ß‡§æ‡§∞ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ú‡§ü‡§ø‡§≤ ‡§õ‡§µ‡§ø‡§Ø‡•ã‡§Ç ‡§Ø‡§æ ‡§ö‡§æ‡§∞‡•ç‡§ü ‡§ï‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã "‡§ï‡§¶‡§Æ-‡§¶‡§∞-‡§ï‡§¶‡§Æ ‡§∏‡•ã‡§ö‡§®‡•á" ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§π‡•á‡§Ç‡•§</li><li><strong>‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü:</strong> ‡§Ø‡§π ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§ø ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§Ü‡§â‡§ü‡§™‡•Å‡§ü ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§î‡§∞ ‡§µ‡§ø‡§∑‡§Ø ‡§™‡§∞ ‡§¨‡§®‡•á ‡§∞‡§π‡•á‡§Ç, ‡§Æ‡§ú‡§¨‡•Ç‡§§ ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü ‡§î‡§∞ ‡§≤‡§æ‡§Æ‡§æ ‡§ó‡§æ‡§∞‡•ç‡§° 2 ‡§ú‡•à‡§∏‡•á ‡§â‡§™‡§ï‡§∞‡§£‡•ã‡§Ç ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§</li>',
                    'sec9-title': '9. ‡§Æ‡•â‡§°‡§≤ ‡§µ‡§ø‡§ï‡§æ‡§∏ üóìÔ∏è',
                    'sec9-p1': '‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§â‡§®‡•ç‡§®‡§§ ‡§è‡§Ü‡§à ‡§™‡•ç‡§∞‡•å‡§¶‡•ç‡§Ø‡•ã‡§ó‡§ø‡§ï‡•Ä ‡§ï‡•á ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§î‡§∞ ‡§ñ‡•Å‡§≤‡•á ‡§§‡•å‡§∞ ‡§™‡§∞ ‡§∏‡§æ‡§ù‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§Æ‡•á‡§ü‡§æ ‡§ï‡•á ‡§Æ‡§ø‡§∂‡§® ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§Æ‡•Ä‡§≤ ‡§ï‡§æ ‡§™‡§§‡•ç‡§•‡§∞ ‡§ï‡§æ ‡§™‡•ç‡§∞‡§§‡§ø‡§®‡§ø‡§ß‡§ø‡§§‡•ç‡§µ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§',
                    'sec9-step2-title': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§∞‡§ø‡§≤‡•Ä‡§ú', 'sec9-step2-desc': '<strong>‡§∏‡§Æ‡§Ø‡§∞‡•á‡§ñ‡§æ:</strong> ‡§ú‡•Å‡§≤‡§æ‡§à 2023‡•§ ‡§ì‡§™‡§®-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§è‡§≤‡§è‡§≤‡§è‡§Æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§®‡§Ø‡§æ ‡§Æ‡§æ‡§®‡§ï ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§î‡§∞ ‡§µ‡§æ‡§£‡§ø‡§ú‡•ç‡§Ø‡§ø‡§ï ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡•ã ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§ø‡§Ø‡§æ‡•§',
                    'sec9-step3-title': '‡§≤‡§æ‡§Æ‡§æ 3 ‡§∞‡§ø‡§≤‡•Ä‡§ú', 'sec9-step3-desc': '<strong>‡§∏‡§Æ‡§Ø‡§∞‡•á‡§ñ‡§æ:</strong> ‡§Ö‡§™‡•ç‡§∞‡•à‡§≤ 2024‡•§ 70B ‡§Æ‡§æ‡§™‡§¶‡§Ç‡§°‡•ã‡§Ç ‡§§‡§ï ‡§ï‡•á ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§è‡§ï ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§õ‡§≤‡§æ‡§Ç‡§ó ‡§¶‡•Ä, ‡§ú‡•ã ‡§ï‡§à ‡§Æ‡§æ‡§≤‡§ø‡§ï‡§æ‡§®‡§æ ‡§Æ‡•â‡§°‡§≤ ‡§∏‡•á ‡§¨‡•á‡§π‡§§‡§∞ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à‡•§',
                    'sec9-step4-title': '‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§∞‡§ø‡§≤‡•Ä‡§ú', 'sec9-step4-desc': '<strong>‡§∏‡§Æ‡§Ø‡§∞‡•á‡§ñ‡§æ:</strong> Q4 2025 (‡§ï‡§æ‡§≤‡•ç‡§™‡§®‡§ø‡§ï)‡•§ ‡§è‡§ï ‡§™‡•Ç‡§∞‡•Ä ‡§§‡§∞‡§π ‡§∏‡•á ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤, 400B+ ‡§™‡•à‡§∞‡§æ‡§Æ‡•Ä‡§ü‡§∞ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡§æ ‡§™‡§∞‡§ø‡§ö‡§Ø ‡§ú‡•ã ‡§ì‡§™‡§®-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§è‡§Ü‡§à ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§§‡•ç‡§Ø‡§æ‡§ß‡•Å‡§®‡§ø‡§ï ‡§ï‡•ã ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§™‡§∞‡§ø‡§≠‡§æ‡§∑‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§',
                    'sec10-title': '10. ‡§∏‡•Ä‡§Æ‡§æ‡§è‡§Ç ‡§î‡§∞ ‡§ú‡§ø‡§Æ‡•ç‡§Æ‡•á‡§¶‡§æ‡§∞ ‡§â‡§™‡§Ø‡•ã‡§ó üö´',
                    'sec10-p1': '‡§ú‡•à‡§∏‡•á-‡§ú‡•à‡§∏‡•á ‡§è‡§Ü‡§à ‡§Æ‡•â‡§°‡§≤ ‡§Ö‡§ß‡§ø‡§ï ‡§∂‡§ï‡•ç‡§§‡§ø‡§∂‡§æ‡§≤‡•Ä ‡§π‡•ã‡§§‡•á ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç, ‡§ú‡§ø‡§Æ‡•ç‡§Æ‡•á‡§¶‡§æ‡§∞ ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§î‡§∞ ‡§™‡§∞‡§ø‡§®‡§ø‡§Ø‡•ã‡§ú‡§® ‡§ï‡•á ‡§™‡•ç‡§∞‡§§‡§ø ‡§™‡•ç‡§∞‡§§‡§ø‡§¨‡§¶‡•ç‡§ß‡§§‡§æ ‡§™‡§π‡§≤‡•á ‡§∏‡•á ‡§ï‡§π‡•Ä‡§Ç ‡§Ö‡§ß‡§ø‡§ï ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§π‡•à‡•§',
                    'sec10-mistake1-title': '‡§ó‡§≤‡§§ ‡§∏‡•Ç‡§ö‡§®‡§æ ‡§ï‡•Ä ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§®‡§æ:', 'sec10-mistake1-desc': '‡§Ø‡§•‡§æ‡§∞‡•ç‡§•‡§µ‡§æ‡§¶‡•Ä ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤ ‡§∏‡§æ‡§Æ‡§ó‡•ç‡§∞‡•Ä ‡§â‡§§‡•ç‡§™‡§®‡•ç‡§® ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§™‡§∞‡§ø‡§∑‡•ç‡§ï‡•É‡§§ ‡§ó‡§≤‡§§ ‡§∏‡•Ç‡§ö‡§®‡§æ ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§ï‡•ã ‡§¨‡§¢‡§º‡§æ‡§§‡•Ä ‡§π‡•à‡•§ ‡§§‡§•‡•ç‡§Ø-‡§ú‡§æ‡§Ç‡§ö ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•à‡•§',
                    'sec10-mistake2-title': '‡§™‡•Ç‡§∞‡•ç‡§µ‡§æ‡§ó‡•ç‡§∞‡§π ‡§™‡•ç‡§∞‡§µ‡§∞‡•ç‡§ß‡§®:', 'sec10-mistake2-desc': '‡§Æ‡•â‡§°‡§≤ ‡§Ö‡§≠‡•Ä ‡§≠‡•Ä ‡§Ö‡§™‡§®‡•á ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§°‡•á‡§ü‡§æ ‡§Æ‡•á‡§Ç ‡§Æ‡•å‡§ú‡•Ç‡§¶ ‡§™‡•Ç‡§∞‡•ç‡§µ‡§æ‡§ó‡•ç‡§∞‡§π‡•ã‡§Ç ‡§ï‡•ã ‡§™‡•ç‡§∞‡§§‡§ø‡§¨‡§ø‡§Ç‡§¨‡§ø‡§§ ‡§î‡§∞ ‡§¨‡§¢‡§º‡§æ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç, ‡§ú‡§ø‡§∏‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§ó‡§æ‡§∞‡•ç‡§°‡§∞‡•á‡§≤ ‡§ï‡•á ‡§∏‡§æ‡§µ‡§ß‡§æ‡§®‡•Ä‡§™‡•Ç‡§∞‡•ç‡§µ‡§ï ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§®‡•ç‡§µ‡§Ø‡§® ‡§ï‡•Ä ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï‡§§‡§æ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§',
                    'sec10-mistake3-title': '‡§∏‡§Ç‡§∏‡§æ‡§ß‡§® ‡§ó‡§π‡§®:', 'sec10-mistake3-desc': '‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§ï‡•á ‡§∏‡§¨‡§∏‡•á ‡§¨‡§°‡§º‡•á ‡§∏‡§Ç‡§∏‡•ç‡§ï‡§∞‡§£‡•ã‡§Ç ‡§ï‡•ã ‡§ö‡§≤‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§ï‡§Æ‡•ç‡§™‡•ç‡§Ø‡•Ç‡§ü‡•á‡§∂‡§®‡§≤ ‡§∏‡§Ç‡§∏‡§æ‡§ß‡§®‡•ã‡§Ç ‡§ï‡•Ä ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï‡§§‡§æ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à, ‡§ú‡•ã ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§ó‡§§ ‡§°‡•á‡§µ‡§≤‡§™‡§∞‡•ç‡§∏ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§¨‡§æ‡§ß‡§æ ‡§π‡•ã ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à‡•§',
                    'sec10-mistake4-title': '‡§ú‡§ø‡§Æ‡•ç‡§Æ‡•á‡§¶‡§æ‡§∞ ‡§â‡§™‡§Ø‡•ã‡§ó:', 'sec10-mistake4-desc': '‡§∏‡§≠‡•Ä ‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ‡§ì‡§Ç ‡§ï‡•ã ‡§Æ‡•á‡§ü‡§æ ‡§ï‡•Ä ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§â‡§™‡§Ø‡•ã‡§ó ‡§®‡•Ä‡§§‡§ø ‡§ï‡§æ ‡§™‡§æ‡§≤‡§® ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è ‡§î‡§∞ ‡§¶‡•Å‡§∞‡•Å‡§™‡§Ø‡•ã‡§ó ‡§ï‡•ã ‡§∞‡•ã‡§ï‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§≤‡§æ‡§Æ‡§æ ‡§ó‡§æ‡§∞‡•ç‡§° 2 ‡§ú‡•à‡§∏‡•á ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§â‡§™‡§ï‡§∞‡§£‡•ã‡§Ç ‡§ï‡•ã ‡§≤‡§æ‡§ó‡•Ç ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§',
                    'sec11-title': '11. ‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§§‡§ï ‡§ï‡•à‡§∏‡•á ‡§™‡§π‡•Å‡§Ç‡§ö‡•á‡§Ç ‡§î‡§∞ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç üè´',
                    'sec11-p1': '‡§Æ‡•á‡§ü‡§æ ‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§ï‡•á ‡§≤‡§ø‡§è ‡§µ‡•ç‡§Ø‡§æ‡§™‡§ï ‡§î‡§∞ ‡§ñ‡•Å‡§≤‡•á ‡§µ‡§ø‡§§‡§∞‡§£ ‡§ï‡•Ä ‡§Ö‡§™‡§®‡•Ä ‡§∞‡§£‡§®‡•Ä‡§§‡§ø ‡§ú‡§æ‡§∞‡•Ä ‡§∞‡§ñ‡•á‡§ó‡§æ‡•§',
                    'sec11-gov-title': 'üìö ‡§°‡•á‡§µ‡§≤‡§™‡§∞‡•ç‡§∏ ‡§î‡§∞ ‡§π‡•â‡§¨‡•Ä‡§∏‡•ç‡§ü ‡§ï‡•á ‡§≤‡§ø‡§è', 'sec11-gov-list': '<li><strong>‡§π‡§ó‡§ø‡§Ç‡§ó ‡§´‡•á‡§∏:</strong> `‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡§æ‡§∞‡•ç‡§Æ‡§∞` ‡§≤‡§æ‡§á‡§¨‡•ç‡§∞‡•á‡§∞‡•Ä ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§ï‡•á ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ ‡§∏‡§¨‡§∏‡•á ‡§Ü‡§∏‡§æ‡§® ‡§§‡§∞‡•Ä‡§ï‡§æ‡•§</li><li><strong>‡§∏‡•Ä‡§ß‡§æ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§°:</strong> ‡§Æ‡•â‡§°‡§≤ ‡§≠‡§æ‡§∞ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡•á‡§ü‡§æ ‡§è‡§Ü‡§à ‡§µ‡•á‡§¨‡§∏‡§æ‡§á‡§ü ‡§™‡§∞ ‡§™‡§π‡•Å‡§Ç‡§ö ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§µ‡•á‡§¶‡§® ‡§ï‡§∞‡•á‡§Ç‡•§</li>',
                    'sec11-pvt-title': 'üíª ‡§â‡§¶‡•ç‡§Ø‡§Æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡•á ‡§≤‡§ø‡§è', 'sec11-pvt-p1': '‡§∏‡•ç‡§ï‡•á‡§≤‡•á‡§¨‡§≤ ‡§™‡§∞‡§ø‡§®‡§ø‡§Ø‡•ã‡§ú‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡•ç‡§∞‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§ï‡•ç‡§≤‡§æ‡§â‡§° ‡§∏‡•á‡§µ‡§æ‡§ì‡§Ç ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§', 'sec11-pvt-list': '<li>‡§Æ‡§æ‡§á‡§ï‡•ç‡§∞‡•ã‡§∏‡•â‡§´‡•ç‡§ü ‡§è‡§ú‡§º‡•ç‡§Ø‡•ã‡§∞, ‡§è‡§°‡§¨‡•ç‡§≤‡•ç‡§Ø‡•Ç‡§è‡§∏ ‡§î‡§∞ ‡§ó‡•Ç‡§ó‡§≤ ‡§ï‡•ç‡§≤‡§æ‡§â‡§° ‡§∏‡§π‡§ø‡§§ ‡§∏‡§≠‡•Ä ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§ï‡•ç‡§≤‡§æ‡§â‡§° ‡§™‡•ç‡§≤‡•á‡§ü‡§´‡§æ‡§∞‡•ç‡§Æ‡•ã‡§Ç ‡§™‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•à‡•§</li>',
                    'sec12-title': '12. ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø: ‡§ì‡§™‡§®-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§è‡§ú‡•Ä‡§Ü‡§à üöÄ',
                    'sec12-p1': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡•Ä ‡§∞‡§ø‡§≤‡•Ä‡§ú ‡§®‡•á ‡§ì‡§™‡§®-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§è‡§Ü‡§à ‡§Æ‡•á‡§Ç ‡§®‡§µ‡§æ‡§ö‡§æ‡§∞ ‡§ï‡•Ä ‡§è‡§ï ‡§µ‡•à‡§∂‡•ç‡§µ‡§ø‡§ï ‡§≤‡§π‡§∞ ‡§ï‡•ã ‡§â‡§§‡•ç‡§™‡•ç‡§∞‡•á‡§∞‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à‡•§ ‡§Ü‡§ó‡§æ‡§Æ‡•Ä ‡§≤‡§æ‡§Æ‡§æ 3 ‡§ï‡§æ ‡§¨‡•á‡§∏‡§¨‡•ç‡§∞‡•Ä ‡§∏‡•á ‡§á‡§Ç‡§§‡§ú‡§æ‡§∞ ‡§π‡•à ‡§î‡§∞ ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤ ‡§î‡§∞ ‡§î‡§∞ ‡§≠‡•Ä ‡§Ö‡§ß‡§ø‡§ï ‡§∂‡§ï‡•ç‡§§‡§ø‡§∂‡§æ‡§≤‡•Ä ‡§§‡§∞‡•ç‡§ï ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ‡§ì‡§Ç ‡§ï‡•Ä ‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ ‡§π‡•ã‡§®‡•á ‡§ï‡•Ä ‡§â‡§Æ‡•ç‡§Æ‡•Ä‡§¶ ‡§π‡•à, ‡§ú‡§ø‡§∏‡§∏‡•á ‡§ñ‡•Å‡§≤‡•á ‡§î‡§∞ ‡§¨‡§Ç‡§¶-‡§∏‡•ç‡§∞‡•ã‡§§ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§ï‡•Ä ‡§ñ‡§æ‡§à ‡§ï‡•ã ‡§î‡§∞ ‡§ï‡§Æ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡•á‡§ó‡§æ ‡§î‡§∞ ‡§è‡§Ü‡§à-‡§∏‡§Ç‡§ö‡§æ‡§≤‡§ø‡§§ ‡§â‡§§‡•ç‡§™‡§æ‡§¶‡•ã‡§Ç ‡§î‡§∞ ‡§∏‡•á‡§µ‡§æ‡§ì‡§Ç ‡§ï‡•Ä ‡§è‡§ï ‡§®‡§à ‡§™‡•Ä‡§¢‡§º‡•Ä ‡§ï‡•ã ‡§∏‡§∂‡§ï‡•ç‡§§ ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡•á‡§ó‡§æ‡•§',
                    'sec12-career-btn': '‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§Ö‡§Ç‡§§‡§∞ ‡§¶‡•á‡§ñ‡•á‡§Ç',
                    'sec13-title': '13. ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§è‡§Ü‡§à ‡§Ö‡§µ‡§ß‡§æ‡§∞‡§£‡§æ‡§è‡§Ç üìö',
                    'sec13-p1': '‡§≤‡§æ‡§Æ‡§æ 3.2 ‡§ï‡•á ‡§™‡•Ä‡§õ‡•á ‡§ï‡•Ä ‡§§‡§ï‡§®‡•Ä‡§ï ‡§ï‡•Ä ‡§∏‡§∞‡§æ‡§π‡§®‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§á‡§® ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§Ö‡§µ‡§ß‡§æ‡§∞‡§£‡§æ‡§ì‡§Ç ‡§ï‡•ã ‡§∏‡§Æ‡§ù‡§®‡§æ ‡§Æ‡•å‡§≤‡§ø‡§ï ‡§π‡•à‡•§',
                    'syllabus-oir-title': '‡§ì‡§™‡§® ‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§è‡§Ü‡§à',
                    'syllabus-oir-list': '<div class="subject-item flex-col sm:flex-row"><div class="flex items-center w-full"><span class="icon">üåê</span><div class="flex-grow">‡§è‡§Ü‡§à ‡§ï‡§æ ‡§≤‡•ã‡§ï‡§§‡§Ç‡§§‡•ç‡§∞‡•Ä‡§ï‡§∞‡§£</div></div><div class="w-full sm:w-auto mt-2 sm:mt-0 flex justify-end flex-shrink-0 space-x-2"><button class="notes-btn">‡§®‡•ã‡§ü‡•ç‡§∏</button><button class="lectures-btn">‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ‡§®</button></div></div>',
                    'syllabus-ppdt-title': '‡§â‡§®‡•ç‡§®‡§§ ‡§∏‡§Ç‡§∞‡•á‡§ñ‡§£',
                    'syllabus-ppdt-list': '<div class="subject-item flex-col sm:flex-row"><div class="flex items-center w-full"><span class="icon">ü§ù</span><div class="flex-grow">‡§Ü‡§∞‡§è‡§≤‡§è‡§Ü‡§à‡§è‡§´ ‡§î‡§∞ ‡§è‡§∏‡§è‡§´‡§ü‡•Ä</div></div><div class="w-full sm:w-auto mt-2 sm:mt-0 flex justify-end flex-shrink-0 space-x-2"><button class="notes-btn">‡§®‡•ã‡§ü‡•ç‡§∏</button><button class="lectures-btn">‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ‡§®</button></div></div>',
                    'footer-text': '&copy; 2025 | ‡§è‡§Ü‡§à ‡§ï‡•á ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø ‡§ï‡§æ ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£, ‡§ñ‡•Å‡§≤‡•á ‡§§‡•å‡§∞ ‡§™‡§∞‡•§',
                    'interviewChartData': {
                        labels: ['‡§è‡§Æ‡§è‡§Æ‡§è‡§≤‡§Ø‡•Ç', '‡§ú‡•Ä‡§è‡§∏‡§è‡§Æ8‡§ï‡•á', '‡§π‡•ç‡§Ø‡•Ç‡§Æ‡§®‡§á‡§µ‡§≤', '‡§Æ‡•à‡§•'],
                        data: [85.5, 96.0, 85.1, 75.3]
                    }
                }
            };
            
            function renderPostCards(lang) {
                const container = document.getElementById('postCardsContainer');
                if (!container) return;
                container.innerHTML = '';
                const posts = serviceDetailsData[lang];
                for (const id in posts) {
                    const post = posts[id];
                    const card = document.createElement('div');
                    card.className = 'group relative p-6 bg-[#2a2b3a]/50 rounded-xl text-center border-2 border-slate-700 hover:border-purple-500 transition-all duration-300 cursor-pointer shadow-lg hover:shadow-2xl transform hover:-translate-y-2';
                    card.dataset.postId = id;
                    card.innerHTML = `
                        <div class="text-6xl mb-4 transition-transform duration-300 group-hover:scale-110">${post.icon}</div>
                        <h4 class="font-bold text-xl text-white">${post.name}</h4>
                        <p class="text-sm text-indigo-200 mt-2">${post.cta}</p>
                        <div class="absolute top-3 right-3 text-purple-500 opacity-0 group-hover:opacity-100 transition-opacity duration-300">
                            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M7 17l9.2-9.2M17 17V7H7"/></svg>
                        </div>
                    `;
                    card.addEventListener('click', () => showPostDetails(id));
                    container.appendChild(card);
                }
            }

            function showPostDetails(postId) {
                const post = serviceDetailsData[currentLang][postId];
                if (post) {
                    document.getElementById('postDetailIcon').innerText = post.icon;
                    document.getElementById('postDetailTitle').innerText = post.name;
                    document.getElementById('postDetailDescription').innerText = post.description;
                    showModal('postDetailModal');
                }
            }

            function renderCareerPath(lang) {
                const container = document.getElementById('careerPathContainer');
                container.innerHTML = '';
                const path = careerPathData[lang].path;
                path.forEach((step) => {
                    const stepElement = document.createElement('div');
                    stepElement.className = 'timeline-step';
                    stepElement.innerHTML = `
                        <div class="timeline-dot"></div>
                        <h4 class="font-semibold text-xl text-white mb-1">${step.rank}</h4>
                        <p class="text-base text-indigo-200">${step.desc}</p>
                    `;
                    container.appendChild(stepElement);
                });
            }

            function initInterviewChart() {
                const ctx = document.getElementById('interviewQualitiesChart').getContext('2d');
                interviewChartInstance = new Chart(ctx, {
                    type: 'bar',
                    data: {
                        labels: translations.en.interviewChartData.labels,
                        datasets: [{
                            label: 'Llama 3.2 405B Score',
                            data: translations.en.interviewChartData.data,
                            backgroundColor: [
                                'rgba(147, 51, 234, 0.7)',
                                'rgba(14, 165, 233, 0.7)',
                                'rgba(234, 179, 8, 0.7)',
                                'rgba(52, 211, 153, 0.7)'
                            ],
                            borderColor: [
                                '#9333ea',
                                '#0ea5e9',
                                '#eab308',
                                '#22c55e'
                            ],
                            borderWidth: 2,
                            borderRadius: 8,
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        plugins: { legend: { display: false } },
                        scales: {
                            y: { 
                                beginAtZero: true, 
                                max: 100,
                                ticks: { color: '#e3e4e6' },
                                grid: { color: 'rgba(227, 228, 230, 0.1)' }
                            },
                            x: { 
                                ticks: { color: '#e3e4e6', font: { size: 14 } },
                                grid: { color: 'rgba(227, 228, 230, 0.0)' }
                            }
                        }
                    }
                });
            }

            function translateChart() {
                if (interviewChartInstance) {
                    const data = translations[currentLang].interviewChartData;
                    interviewChartInstance.data.labels = data.labels;
                    interviewChartInstance.data.datasets[0].data = data.data;
                    interviewChartInstance.update();
                }
            }
            
            function translatePage(lang) {
                currentLang = lang;
                document.documentElement.lang = lang;
                for (const id in translations[lang]) {
                    const element = document.getElementById(id);
                    if (element) {
                        element.innerHTML = translations[lang][id];
                    }
                }
                renderPostCards(lang);
                renderCareerPath(lang);
                translateChart();
            }

            document.querySelectorAll('.lang-btn').forEach(button => {
                button.addEventListener('click', (e) => {
                    document.querySelectorAll('.lang-btn').forEach(btn => btn.classList.remove('active'));
                    e.currentTarget.classList.add('active');
                    translatePage(e.currentTarget.dataset.lang);
                });
            });

            const showModal = (modalId) => document.getElementById(modalId)?.classList.add('active');
            const closeModal = (modal) => modal?.classList.remove('active');

            document.querySelectorAll('[data-modal-target]').forEach(trigger => {
                trigger.addEventListener('click', () => showModal(trigger.dataset.modalTarget));
            });


            document.querySelectorAll('.modal-overlay').forEach(modal => {
                modal.addEventListener('click', (e) => {
                    if (e.target === modal) closeModal(modal);
                });
                modal.querySelector('.modal-close-button')?.addEventListener('click', () => closeModal(modal));
            });
            
            // Initial render
            initInterviewChart();
            translatePage('en');
        });
    </script>
</body>
</html>

