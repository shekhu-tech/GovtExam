<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Meta Llama 2: The Ultimate Guide</title>
    <script src="/app-redirect.js"></script>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700;800&display=swap" rel="stylesheet">
    <meta name="description" content="A complete guide to Meta's Llama 2 models. Understand its open-source nature, capabilities, applications, and how to use it for research and commercial purposes.">
    <meta name="keywords" content="Meta, Llama 2, Open Source AI, Large Language Models, AI, Artificial Intelligence, LLM, Generative AI, Llama Guard">
    <meta name="author" content="AI Tools Guide">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #1a1a1a; /* Deep Charcoal */
            color: #f0f0f0; /* Light Gray */
        }
        .section-heading {
            border-image: linear-gradient(to right, #0062E0, #FFC700) 1;
            border-bottom: 4px solid;
            padding-bottom: 0.5rem;
            margin-bottom: 1.5rem;
            display: inline-block;
            color: #ffffff; /* White */
        }
        .card {
            background: rgba(26, 26, 26, 0.8); /* Dark Charcoal Transparent */
            backdrop-filter: blur(16px);
            -webkit-backdrop-filter: blur(16px);
            border-radius: 1.25rem;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.5);
            padding: 2rem;
            transition: all 0.4s cubic-bezier(0.175, 0.885, 0.32, 1.275);
            border: 1px solid #444444; /* Medium Gray */
        }
        .card:hover {
            box-shadow: 0 0 40px rgba(0, 98, 224, 0.2);
            transform: translateY(-12px) scale(1.02);
            border-color: #0062E0; /* Meta Blue */
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 550px;
            margin: auto;
            height: 320px;
            max-height: 450px;
        }
        @media (min-width: 768px) {
            .chart-container {
                height: 380px;
            }
        }
        .flowchart-step {
            border: 3px solid #0062E0; /* Meta Blue */
            color: #0062E0;
            background: #2a2a2a; /* Dark Gray */
            transition: all 0.3s ease-in-out;
            cursor: pointer;
            box-shadow: 0 0 15px rgba(0, 98, 224, 0.2);
            text-shadow: 0 0 5px rgba(0, 98, 224, 0.5);
        }
        .flowchart-step:hover {
            background-color: #0062E0;
            color: #ffffff;
            transform: scale(1.08);
            box-shadow: 0 0 25px rgba(0, 98, 224, 0.5);
        }
        .flowchart-arrow {
            color: #FFC700; /* Meta Yellow */
            text-shadow: 0 0 10px rgba(255, 199, 0, 0.5);
        }
        .subject-category-heading {
            color: #ffffff;
            font-weight: 700;
            margin-bottom: 1rem;
            font-size: 1.3rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid #FFC700;
        }
        .subject-item {
            display: flex;
            align-items: center;
            margin-bottom: 0.75rem;
            padding: 0.75rem;
            border-radius: 0.75rem;
            transition: background-color 0.2s;
        }
        .subject-item:hover {
            background-color: rgba(255, 199, 0, 0.1);
        }
        .subject-item .icon {
            color: #FFC700;
            margin-right: 1rem;
            font-size: 1.5rem;
        }
        .timeline-step {
            position: relative;
            padding-left: 2.5rem;
            padding-bottom: 2.5rem;
            border-left: 4px solid #0062E0;
        }
        .timeline-step:last-child {
            padding-bottom: 0;
        }
        .timeline-dot {
            position: absolute;
            left: -0.9375rem;
            top: 0;
            width: 1.75rem;
            height: 1.75rem;
            border-radius: 50%;
            background-color: #1a1a1a;
            border: 4px solid #0062E0;
            box-shadow: 0 0 15px rgba(0, 98, 224, 0.7);
        }
        .modal-overlay {
            position: fixed;
            top: 0; left: 0; width: 100%; height: 100%;
            background-color: rgba(26, 26, 26, 0.85);
            backdrop-filter: blur(8px);
            -webkit-backdrop-filter: blur(8px);
            display: flex;
            align-items: center; justify-content: center;
            z-index: 1000;
            opacity: 0; visibility: hidden;
            transition: opacity 0.3s ease, visibility 0.3s ease;
            padding: 1rem;
        }
        .modal-overlay.active {
            opacity: 1; visibility: visible;
        }
        .modal-content {
            background-color: #2a2a2a;
            color: #f0f0f0;
            border-radius: 1rem;
            padding: 2.5rem;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.5);
            max-width: 800px;
            width: 100%;
            position: relative;
            transform: translateY(20px) scale(0.98);
            opacity: 0;
            transition: transform 0.4s ease, opacity 0.4s ease;
            max-height: 90vh;
            overflow-y: auto;
            border: 1px solid #444444;
        }
        .modal-overlay.active .modal-content {
            transform: translateY(0) scale(1);
            opacity: 1;
        }
        .modal-close-button {
            position: absolute; top: 1rem; right: 1rem;
            background: none; border: none; font-size: 2.5rem;
            cursor: pointer; color: #a3a3a3; line-height: 1;
            transition: color 0.2s, transform 0.2s;
        }
        .modal-close-button:hover { color: #f87171; transform: rotate(90deg); }
        .modal-action-button {
            background: linear-gradient(45deg, #0062E0, #0052be);
            color: #ffffff;
            padding: 0.85rem 1.75rem;
            border-radius: 0.75rem;
            font-weight: 700;
            cursor: pointer;
            transition: all 0.3s ease;
            border: none;
            box-shadow: 0 5px 15px rgba(0, 98, 224, 0.3);
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        .modal-action-button:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 25px rgba(0, 98, 224, 0.4);
        }
        .lang-btn {
            padding: 0.6rem 2rem;
            font-weight: 700;
            cursor: pointer;
            transition: all 0.3s;
            border: 2px solid #0062E0;
            color: #0062E0;
            background-color: transparent;
        }
        .lang-btn:first-child { border-top-left-radius: 0.75rem; border-bottom-left-radius: 0.75rem; }
        .lang-btn:last-child { border-top-right-radius: 0.75rem; border-bottom-right-radius: 0.75rem; }
        .lang-btn.active, .lang-btn:hover {
            background-color: #0062E0;
            color: #ffffff;
            box-shadow: 0 0 15px #0062E0;
        }
        .notes-btn, .lectures-btn {
            padding: 0.35rem 1rem;
            border-radius: 0.5rem;
            font-size: 0.875rem;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.2s ease-in-out;
            border: 1px solid transparent;
            text-transform: uppercase;
        }
        .notes-btn {
            background: linear-gradient(45deg, #FFC700, #f5b700);
            color: #1a1a1a;
        }
        .notes-btn:hover {
            filter: brightness(1.1);
        }
        .lectures-btn {
            background-color: #444444; 
            color: #e0e0e0; 
            border-color: #555555; 
        }
        .lectures-btn:hover {
            background-color: #555555;
            color: #ffffff;
        }
    </style>
</head>
<body class="leading-relaxed">

    <div class="container mx-auto p-4 sm:p-6 md:p-12">

        <header class="text-center mb-10">
            <h1 id="header-title" class="text-4xl md:text-5xl font-extrabold text-[#ffffff] mb-4 tracking-tight">ü¶ô Meta's Llama 2: The Ultimate Guide ü¶ô</h1>
            <p id="header-subtitle" class="text-xl md:text-2xl font-semibold bg-clip-text text-transparent bg-gradient-to-r from-blue-400 to-yellow-400">The Open Source AI Model for Researchers and Creators</p>
        </header>

        <div class="text-center mb-12">
            <div class="inline-flex rounded-md shadow-sm" role="group">
                <button type="button" class="lang-btn active" data-lang="en">English</button>
                <button type="button" class="lang-btn" data-lang="hi">‡§π‡§ø‡§Ç‡§¶‡•Ä</button>
            </div>
        </div>

        <main class="grid grid-cols-1 gap-12">

            <!-- Section 1: What is Llama 2? -->
            <section class="card !p-0 overflow-hidden bg-gradient-to-br from-[#2a2a2a] via-[#1a1a1a] to-[#1a1a1a]">
                <div class="p-8">
                    <h2 id="sec1-title" class="text-3xl font-bold section-heading">1. What is Meta's Llama 2? üí°</h2>
                    <p id="sec1-p1" class="text-lg mb-6">Llama 2 is a family of large language models (LLMs) developed by Meta and released in partnership with Microsoft. It is a powerful, pretrained and fine-tuned model designed for a wide range of natural language tasks. What makes Llama 2 a landmark release is its **open-source availability for both research and commercial use**. This move has democratized access to state-of-the-art AI, empowering developers and businesses worldwide to build their own generative AI-powered applications.</p>
                </div>
                <div id="postCardsContainer" class="px-8 pb-8 pt-2 grid grid-cols-1 md:grid-cols-3 gap-8 bg-transparent">
                    <!-- Cards will be dynamically inserted here by JavaScript -->
                </div>
            </section>

            <!-- Section 2: How It Works -->
            <section class="card">
                <h2 id="sec2-title" class="text-3xl font-bold section-heading text-center w-full">2. The Technology Behind Llama 2 üéØ</h2>
                <p id="sec2-p1" class="text-lg text-center mb-10 max-w-3xl mx-auto">Llama 2 is based on the powerful Transformer architecture and refined with extensive fine-tuning to be a helpful and safe assistant.</p>
                <div class="flex flex-col md:flex-row items-center justify-center space-y-8 md:space-y-0 md:space-x-8 lg:space-x-16">
                    <div class="text-center flex flex-col items-center">
                        <div class="w-40 h-40 flex items-center justify-center rounded-full flowchart-step font-bold text-lg p-3 text-center leading-tight" data-modal-target="prelimsMainModal"><span id="sec2-step1-title">Stage 1:</span><br><span id="sec2-step1-name">Pre-training</span></div>
                        <p id="sec2-step1-desc" class="mt-4 font-semibold text-gray-400">Learning from vast public data</p>
                    </div>
                    <div class="text-5xl font-light flowchart-arrow transform md:rotate-0 rotate-90">‚Üí</div>
                    <div class="text-center flex flex-col items-center">
                        <div class="w-40 h-40 flex items-center justify-center rounded-full flowchart-step font-bold text-lg p-3 text-center leading-tight" data-modal-target="mainsMainModal"><span id="sec2-step2-title">Stage 2:</span><br><span id="sec2-step2-name">Fine-Tuning</span></div>
                        <p id="sec2-step2-desc" class="mt-4 font-semibold text-gray-400">Aligning with human feedback</p>
                    </div>
                    <div class="text-5xl font-light flowchart-arrow transform md:rotate-0 rotate-90">‚Üí</div>
                    <div class="text-center flex flex-col items-center">
                        <div class="w-40 h-40 flex items-center justify-center rounded-full flowchart-step font-bold text-lg p-3 text-center leading-tight" data-modal-target="interviewModal"><span id="sec2-step3-title">Output</span><br><span id="sec2-step3-name">Helpful Response</span></div>
                        <p id="sec2-step3-desc" class="mt-4 font-semibold text-gray-400">Generating a helpful and safe response</p>
                    </div>
                </div>
            </section>

            <!-- Section 3: Access & Availability -->
            <section class="card">
                <h2 id="sec3-title" class="text-3xl font-bold section-heading">3. Who Can Use It? ‚úÖ</h2>
                <div class="grid grid-cols-1 lg:grid-cols-2 gap-8 items-start">
                    <div>
                        <p id="sec3-p1" class="text-lg mb-6">Llama 2 is available for a wide range of users, from individual researchers to large commercial entities.</p>
                        <ul class="space-y-5 text-base">
                            <li class="flex items-start"><span class="text-3xl text-[#0062E0] mr-4">üî¨</span><div><strong id="sec3-age-title" class="text-white text-lg">Researchers & Academics:</strong> <span id="sec3-age-desc">Can freely download and use the models for their research and experiments.</span></div></li>
                            <li class="flex items-start"><span class="text-3xl text-[#0062E0] mr-4">üßë‚Äçüíª</span><div><strong id="sec3-edu-title" class="text-white text-lg">Developers & Startups:</strong> <span id="sec3-edu-desc">Can build and launch commercial products powered by Llama 2 without licensing fees (subject to the acceptable use policy).</span></div></li>
                        </ul>
                    </div>
                    <div>
                        <h3 id="sec3-attempts-title" class="text-2xl font-bold text-white mb-4 border-b border-[#FFC700] pb-2">Primary Platforms üìä</h3>
                         <ul id="sec3-agerelax-list" class="list-disc list-inside space-y-3 text-base">
                             <li><strong>Direct Download:</strong> Available for download from Meta's official AI website.</li>
                             <li><strong>Cloud Providers:</strong> Available on platforms like Microsoft Azure, AWS, and Google Cloud for easy deployment.</li>
                             <li><strong>Hugging Face:</strong> The models are hosted on the Hugging Face Hub, a popular platform for the AI community.</li>
                        </ul>
                    </div>
                </div>
            </section>
            
            <!-- Section 4: Core Capabilities -->
            <section class="card bg-black/20">
                <h2 id="sec4-title" class="text-3xl font-bold section-heading text-center w-full">4. Core Capabilities üìö</h2>
                <p id="sec4-p1" class="text-lg text-center mb-10 max-w-4xl mx-auto">Llama 2 is a highly capable language model proficient in a wide array of natural language understanding and generation tasks.</p>
                 <div id="syllabus-grid" class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-8">
                    <div class="bg-[#2a2a2a]/80 backdrop-blur-sm p-6 rounded-xl shadow-lg border border-neutral-700 flex flex-col transition duration-300 hover:shadow-2xl hover:border-blue-500 hover:-translate-y-1">
                        <h3 id="gs1-title" class="subject-category-heading">Text Generation</h3>
                        <p class="text-sm">Writing articles, emails, and creative content.</p>
                    </div>
                     <div class="bg-[#2a2a2a]/80 backdrop-blur-sm p-6 rounded-xl shadow-lg border border-neutral-700 flex flex-col transition duration-300 hover:shadow-2xl hover:border-blue-500 hover:-translate-y-1">
                        <h3 id="gs2-title" class="subject-category-heading">Summarization</h3>
                         <p class="text-sm">Condensing long documents into key points.</p>
                    </div>
                    <div class="bg-[#2a2a2a]/80 backdrop-blur-sm p-6 rounded-xl shadow-lg border border-neutral-700 flex flex-col transition duration-300 hover:shadow-2xl hover:border-blue-500 hover:-translate-y-1">
                        <h3 id="gs3-title" class="subject-category-heading">Coding</h3>
                         <p class="text-sm">Generating code snippets in various programming languages.</p>
                    </div>
                    <div class="bg-[#2a2a2a]/80 backdrop-blur-sm p-6 rounded-xl shadow-lg border border-neutral-700 flex flex-col transition duration-300 hover:shadow-2xl hover:border-blue-500 hover:-translate-y-1">
                        <h3 id="gs4-title" class="subject-category-heading">Conversation</h3>
                         <p class="text-sm">Powering chatbots and conversational AI assistants.</p>
                    </div>
                </div>
            </section>

            <!-- Section 5: Training & Data -->
            <section class="card">
                <h2 id="sec5-title" class="text-3xl font-bold section-heading">5. How was Llama 2 Trained? üèõÔ∏è</h2>
                <p id="sec5-p1" class="text-lg text-center mb-8 max-w-3xl mx-auto">Llama 2's impressive performance is the result of a two-stage training process focused on both knowledge acquisition and safety alignment.</p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8 text-base">
                    <div class="p-6 bg-[#2a2a2a] rounded-xl border-2 border-[#0062E0]">
                        <h3 id="sec5-hindi-title" class="font-bold text-xl text-white mb-2">1. Pre-training</h3>
                        <p id="sec5-hindi-p1" class="text-sm mb-2">Building a vast base of knowledge.</p>
                        <ul id="sec5-hindi-list" class="list-disc list-inside space-y-1 mt-2 text-sm">
                            <li>Trained on 2 trillion tokens of publicly available data from the internet.</li>
                            <li>This process allows the model to learn grammar, facts, and reasoning abilities on a massive scale.</li>
                        </ul>
                    </div>
                    <div class="p-6 bg-[#2a2a2a] rounded-xl border-2 border-[#FFC700]">
                        <h3 id="sec5-essay-title" class="font-bold text-xl text-white mb-2">2. Fine-Tuning (RLHF)</h3>
                        <p id="sec5-essay-p1" class="text-sm mb-2">Making the model helpful and safe.</p>
                        <ul id="sec5-essay-list" class="list-disc list-inside space-y-1 mt-2 text-sm">
                           <li>The pretrained model is fine-tuned using a technique called Reinforcement Learning with Human Feedback (RLHF).</li>
                           <li>Human annotators create high-quality examples and rank model responses to teach it helpfulness and safety.</li>
                           <li>Over 1 million human annotations were used to align the Llama 2-Chat models.</li>
                        </ul>
                    </div>
                </div>
            </section>
            
            <!-- Section 6: Key Applications -->
            <section class="card">
                <h2 id="sec6-title" class="text-3xl font-bold section-heading">6. Key Applications & Use Cases üåü</h2>
                <p id="sec6-p1" class="text-lg text-center mb-8 max-w-4xl mx-auto">The open and commercially viable nature of Llama 2 has unlocked a plethora of applications across various industries.</p>
                <div class="grid grid-cols-1 md:grid-cols-3 gap-8 text-base">
                    <div>
                        <h3 id="sec6-eng-title" class="subject-category-heading">Custom Chatbots</h3>
                        <ul id="sec6-eng-list" class="list-disc list-inside space-y-2 text-base">
                           <li>Building specialized customer service agents.</li>
                           <li>Creating internal knowledge base assistants for employees.</li>
                        </ul>
                    </div>
                    <div>
                        <h3 id="sec6-elec-title" class="subject-category-heading">Content Creation</h3>
                        <ul id="sec6-elec-list" class="list-disc list-inside space-y-2 text-base">
                           <li>Drafting marketing copy, blog posts, and emails.</li>
                           <li>Generating creative story plots and scripts.</li>
                           <li>Summarizing articles and research papers.</li>
                        </ul>
                    </div>
                     <div>
                        <h3 id="sec6-naic-title" class="subject-category-heading">Developer Tools</h3>
                        <ul id="sec6-naic-list" class="list-disc list-inside space-y-2 text-base">
                           <li>Code generation and debugging assistants.</li>
                           <li>Natural language interfaces for software.</li>
                           <li>Automating documentation writing.</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Section 7: Performance Benchmarks -->
            <section class="card grid grid-cols-1 lg:grid-cols-2 gap-8 items-center">
                <div>
                     <h2 id="sec7-title" class="text-3xl font-bold section-heading">7. Performance & Benchmarks üéôÔ∏è</h2>
                    <p id="sec7-p1" class="text-lg mb-6">Llama 2 has demonstrated strong performance across a wide range of academic and industry benchmarks, often outperforming other open-source models.</p>
                    <div class="chart-container h-80">
                        <canvas id="interviewQualitiesChart"></canvas>
                    </div>
                </div>
                <div>
                    <h3 id="sec7-qualities-title" class="text-2xl font-bold text-white mb-6 border-b border-[#0062E0] pb-2">Key Benchmarks:</h3>
                    <ul id="sec7-qualities-list" class="space-y-4 text-base">
                        <li class="flex items-start"><span class="text-2xl text-[#34A853] mr-3">üìö</span><div><strong class="text-white">MMLU (Massive Multitask Language Understanding):</strong> Tests general knowledge and problem-solving skills across 57 subjects.</div></li>
                        <li class="flex items-start"><span class="text-2xl text-[#34A853] mr-3">üíª</span><div><strong class="text-white">HumanEval:</strong> Measures the model's ability to correctly generate code for programming challenges.</div></li>
                        <li class="flex items-start"><span class="text-2xl text-[#34A853] mr-3">üß†</span><div><strong class="text-white">Reasoning Benchmarks:</strong> Such as GSM8K, which tests the model's ability to solve multi-step mathematical reasoning problems.</div></li>
                    </ul>
                </div>
            </section>
            
            <!-- Section 8: Prompt Engineering -->
            <section class="card">
                <h2 id="sec8-title" class="text-3xl font-bold section-heading">8. Prompting Llama 2: Best Practices üìà</h2>
                <p id="sec8-p1" class="text-lg mb-8 max-w-4xl mx-auto">Effective prompting is crucial for unlocking the full potential of Llama 2. The model responds well to clear instructions and system-level guidance.</p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                    <div>
                        <h3 id="sec8-steps-title" class="text-2xl font-bold text-white mb-4">üìö Key Principles:</h3>
                        <ol class="relative border-l-4 border-[#0062E0] pl-6 space-y-8">
                            <li class="ml-4">
                                <div class="absolute w-4 h-4 bg-[#0062E0] rounded-full -left-2.5 border-4 border-[#2a2a2a]"></div>
                                <h4 id="sec8-step1-title" class="font-semibold text-xl text-white mb-1">Use a System Prompt</h4>
                                <p id="sec8-step1-desc" class="text-base">Start with a system prompt (`<s>[INST] <<SYS>>...<</SYS>>`) to set the context and define the AI's persona and behavior for the entire conversation.</p>
                            </li>
                             <li class="ml-4">
                               <div class="absolute w-4 h-4 bg-[#0062E0] rounded-full -left-2.5 border-4 border-[#2a2a2a]"></div>
                                <h4 id="sec8-step2-title" class="font-semibold text-xl text-white mb-1">Be Clear and Specific</h4>
                                <p id="sec8-step2-desc" class="text-base">Provide unambiguous instructions. The more detail you give about the desired output, format, and style, the better the result.</p>
                            </li>
                        </ol>
                    </div>
                    <div>
                        <h3 id="sec8-revision-title" class="text-2xl font-bold text-white mb-4">üîÑ Advanced Techniques:</h3>
                         <ul id="sec8-revision-list" class="list-disc list-inside space-y-3 text-base">
                            <li><strong>Few-Shot Prompting:</strong> Provide a few examples of input-output pairs within your prompt to guide the model's response.</li>
                            <li><strong>Iterative Conversation:</strong> Refine the output through a series of conversational turns, asking the model to improve upon its previous responses.</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Section 9: Model Evolution -->
            <section class="card">
                <h2 id="sec9-title" class="text-3xl font-bold section-heading">9. Model Evolution üóìÔ∏è</h2>
                <p id="sec9-p1" class="text-lg mb-8">Llama 2 is a significant step in Meta's long-term commitment to open science and the democratization of AI technology.</p>
                <div class="career-path-timeline">
                    <div class="timeline-step">
                        <div class="timeline-dot"></div>
                        <h4 id="sec9-step1-title" class="font-semibold text-xl text-white mb-1">Llama 1</h4>
                        <p id="sec9-step1-desc" class="text-base"><strong class="text-gray-400">Timeline:</strong> February 2023. The first version was released to the research community, demonstrating the power of smaller, more efficient models.</p>
                    </div>
                    <div class="timeline-step">
                        <div class="timeline-dot"></div>
                        <h4 id="sec9-step2-title" class="font-semibold text-xl text-white mb-1">Llama 2 Release</h4>
                        <p id="sec9-step2-desc" class="text-base"><strong class="text-gray-400">Timeline:</strong> July 2023. A major release with improved performance, a permissive license for commercial use, and dedicated chat-optimized models.</p>
                    </div>
                     <div class="timeline-step">
                        <div class="timeline-dot"></div>
                        <h4 id="sec9-step3-title" class="font-semibold text-xl text-white mb-1">Code Llama</h4>
                        <p id="sec9-step3-desc" class="text-base"><strong class="text-gray-400">Timeline:</strong> August 2023. A specialized version of Llama 2 fine-tuned specifically for coding tasks.</p>
                    </div>
                     <div class="timeline-step">
                        <div class="timeline-dot"></div>
                        <h4 id="sec9-step4-title" class="font-semibold text-xl text-white mb-1">Llama 3 & Beyond</h4>
                        <p id="sec9-step4-desc" class="text-base"><strong class="text-gray-400">Timeline:</strong> Future. Meta continues to work on the next generation of Llama models, expected to have even greater capabilities, including multimodality.</p>
                    </div>
                </div>
            </section>

            <!-- Section 10: Mistakes -->
            <section class="card">
                <h2 id="sec10-title" class="text-3xl font-bold section-heading text-center w-full">10. Limitations & Responsible Use üö´</h2>
                <p id="sec10-p1" class="text-lg text-center mb-10 max-w-4xl mx-auto">While Llama 2 is a powerful tool, it's essential to be aware of its limitations and adhere to Meta's responsible use guidelines.</p>
                <div id="sec10-donts-grid" class="grid grid-cols-1 md:grid-cols-2 gap-8 text-base">
                    <div class="group relative p-6 bg-gray-900/50 rounded-lg border-2 border-red-700 shadow-md transition-all duration-300 hover:shadow-xl hover:border-red-500 hover:-translate-y-1">
                        <div class="absolute -top-4 -left-4 bg-red-600 text-white rounded-full w-12 h-12 flex items-center justify-center text-2xl font-bold shadow-lg group-hover:scale-110 transition-transform">‚ùå</div>
                        <div class="ml-10">
                            <strong id="sec10-mistake1-title" class="text-lg text-red-300">Factual Inaccuracy:</strong>
                            <p id="sec10-mistake1-desc" class="text-red-400 mt-1">Llama 2 can "hallucinate" and generate incorrect or fabricated information. Always fact-check outputs for critical use cases.</p>
                        </div>
                    </div>
                    <div class="group relative p-6 bg-gray-900/50 rounded-lg border-2 border-red-700 shadow-md transition-all duration-300 hover:shadow-xl hover:border-red-500 hover:-translate-y-1">
                         <div class="absolute -top-4 -left-4 bg-red-600 text-white rounded-full w-12 h-12 flex items-center justify-center text-2xl font-bold shadow-lg group-hover:scale-110 transition-transform">‚ùå</div>
                        <div class="ml-10">
                            <strong id="sec10-mistake2-title" class="text-lg text-red-300">Bias in Data:</strong>
                            <p id="sec10-mistake2-desc" class="text-red-400 mt-1">The model was trained on public data and may reflect societal biases. Developers must implement safeguards for their applications.</p>
                        </div>
                    </div>
                     <div class="group relative p-6 bg-gray-900/50 rounded-lg border-2 border-red-700 shadow-md transition-all duration-300 hover:shadow-xl hover:border-red-500 hover:-translate-y-1">
                         <div class="absolute -top-4 -left-4 bg-red-600 text-white rounded-full w-12 h-12 flex items-center justify-center text-2xl font-bold shadow-lg group-hover:scale-110 transition-transform">‚ùå</div>
                        <div class="ml-10">
                            <strong id="sec10-mistake3-title" class="text-lg text-red-300">Acceptable Use Policy:</strong>
                            <p id="sec10-mistake3-desc" class="text-red-400 mt-1">Commercial use is subject to Meta's Acceptable Use Policy, which prohibits use in certain sensitive or illegal activities.</p>
                        </div>
                    </div>
                     <div class="group relative p-6 bg-slate-900/50 rounded-lg border-2 border-red-700 shadow-md transition-all duration-300 hover:shadow-xl hover:border-red-500 hover:-translate-y-1">
                         <div class="absolute -top-4 -left-4 bg-red-600 text-white rounded-full w-12 h-12 flex items-center justify-center text-2xl font-bold shadow-lg group-hover:scale-110 transition-transform">‚ùå</div>
                        <div class="ml-10">
                            <strong id="sec10-mistake4-title" class="text-lg text-red-300">High Usage Clause:</strong>
                            <p id="sec10-mistake4-desc" class="text-red-400 mt-1">Companies with over 700 million monthly active users must request a special license from Meta to use Llama 2.</p>
                        </div>
                    </div>
                </div>
            </section>
            
            <!-- Section 11: How to Access -->
            <section class="card">
                <h2 id="sec11-title" class="text-3xl font-bold section-heading">11. How to Access & Use üè´</h2>
                <p id="sec11-p1" class="text-lg text-center mb-8 max-w-4xl mx-auto">Getting started with Llama 2 is straightforward, thanks to its open-source nature and strong community support.</p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                    <div>
                        <h3 id="sec11-gov-title" class="subject-category-heading">üìö For Developers & Researchers</h3>
                        <ul id="sec11-gov-list" class="list-disc list-inside space-y-2 text-base">
                           <li>**Download from Meta:** Apply for access and download the model weights directly from the Meta AI website.</li>
                           <li>**Hugging Face:** The easiest way to get started. Use the transformers library to download and run the models with just a few lines of code.</li>
                        </ul>
                    </div>
                    <div>
                        <h3 id="sec11-pvt-title" class="subject-category-heading">üíª For Enterprise Deployment</h3>
                        <p id="sec11-pvt-p1" class="text-base mb-2">Leverage managed cloud services.</p>
                        <ul id="sec11-pvt-list" class="list-disc list-inside space-y-2 text-base">
                            <li>**Microsoft Azure:** Llama 2 is available in the Azure AI model catalog.</li>
                            <li>**AWS & Google Cloud:** Can be deployed and managed through services like Amazon SageMaker and Vertex AI.</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Section 12: The Future -->
            <section class="card">
                <h2 id="sec12-title" class="text-3xl font-bold section-heading">12. The Future: Llama 3 & Open Source AI üöÄ</h2>
                <p id="sec12-p1" class="text-lg mb-6">The release of Llama 2 has catalyzed a global wave of innovation in open-source AI. The upcoming Llama 3 is highly anticipated and is expected to feature full multimodality and even more powerful reasoning capabilities, further narrowing the gap between open and closed-source models and empowering a new generation of AI-powered products and services.</p>
                <button id="sec12-career-btn" class="modal-action-button mt-4 inline-block" data-modal-target="careerGrowthModal">View Key Differentiators</button>
            </section>

            <!-- Section 13: Core Concepts -->
            <section class="card bg-black/20">
                <h2 id="sec13-title" class="text-3xl font-bold section-heading">13. Core AI Concepts üìö</h2>
                <p id="sec13-p1" class="text-lg text-center mb-10 max-w-4xl mx-auto">Understanding these core concepts is fundamental to appreciating the technology behind Llama 2.</p>
                <div id="syllabus-material-grid" class="grid grid-cols-1 md:grid-cols-2 gap-8">
                    <div class="bg-[#1a2a24] p-6 rounded-lg shadow-lg border border-stone-700">
                        <h3 id="syllabus-oir-title" class="subject-category-heading">Open Source vs. Closed Source</h3>
                        <div id="syllabus-oir-list" class="space-y-4">
                            <!-- Content via JS -->
                        </div>
                    </div>
                    <div class="bg-[#1a2a24] p-6 rounded-lg shadow-lg border border-stone-700">
                        <h3 id="syllabus-ppdt-title" class="subject-category-heading">Training & Alignment</h3>
                        <div id="syllabus-ppdt-list" class="space-y-4">
                            <!-- Content via JS -->
                        </div>
                    </div>
                </div>
            </section>

        </main>

        <footer class="text-center mt-16 border-t-2 pt-8 border-yellow-500/20">
            <p id="footer-text" class="text-gray-400 text-lg font-medium">&copy; 2025 | Democratizing Artificial Intelligence.</p>
        </footer>

    </div>

    <!-- Modals -->
    <div id="prelimsMainModal" class="modal-overlay">
        <div class="modal-content">
            <button class="modal-close-button">&times;</button>
            <h3 id="prelims-modal-title" class="text-2xl font-bold text-white mb-4">Stage 1: Pre-training</h3>
            <p id="prelims-modal-p1">In this stage, the model is trained on a massive, diverse dataset of publicly available text and code (2 trillion tokens). The model learns to predict the next word in a sentence, which helps it build a deep understanding of grammar, facts, common sense reasoning, and different language styles.</p>
        </div>
    </div>

    <div id="mainsMainModal" class="modal-overlay">
        <div class="modal-content">
            <button class="modal-close-button">&times;</button>
            <h3 id="mains-modal-title" class="text-2xl font-bold text-white mb-4">Stage 2: Fine-Tuning</h3>
            <p id="mains-modal-p1">After pre-training, the model is "fine-tuned" to become a helpful and safe assistant. This is done using Reinforcement Learning with Human Feedback (RLHF). Human annotators write high-quality examples and rank the model's responses, which teaches the model to follow instructions and align with human preferences for safety and helpfulness.</p>
        </div>
    </div>

    <div id="interviewModal" class="modal-overlay">
        <div class="modal-content">
            <button class="modal-close-button">&times;</button>
            <h3 id="interview-modal-title" class="text-2xl font-bold text-white mb-4">Output: Helpful Response</h3>
            <p id="interview-modal-p1" class="text-base mb-3">When a user provides a prompt, the fine-tuned model generates a response that is not only contextually relevant and knowledgeable but also adheres to the safety and helpfulness principles it learned during the alignment phase. This results in more reliable and safer interactions.</p>
        </div>
    </div>

    <!-- Post Detail Modal -->
    <div id="postDetailModal" class="modal-overlay">
        <div class="modal-content">
            <button class="modal-close-button">&times;</button>
            <div class="flex items-center mb-4">
                <div id="postDetailIcon" class="text-5xl mr-4"></div>
                <h3 id="postDetailTitle" class="text-3xl font-bold text-white"></h3>
            </div>
            <p id="postDetailDescription" class="text-lg text-indigo-200 leading-relaxed"></p>
        </div>
    </div>
    
    <div id="careerGrowthModal" class="modal-overlay">
        <div class="modal-content">
            <button class="modal-close-button">&times;</button>
            <h3 id="career-modal-title" class="text-2xl font-bold text-white mb-6">Key Differentiators of Llama 2</h3>
            <div id="careerPathContainer" class="career-path-timeline"></div>
        </div>
    </div>


    <script>
        document.addEventListener('DOMContentLoaded', () => {
            let interviewChartInstance;
            let currentLang = 'en';

            const serviceDetailsData = {
                'en': {
                    'opensource': { 
                        name: 'Open Source', 
                        icon: 'üåê',
                        description: 'Llama 2 is freely available for both research and commercial use, promoting widespread innovation and access to powerful AI technology.',
                        cta: 'Learn More'
                    },
                    'family': { 
                        name: 'Family of Models', 
                        icon: 'üë®‚Äçüë©‚Äçüëß‚Äçüë¶',
                        description: 'It comes in various sizes (7B, 13B, 70B parameters), allowing developers to choose the best balance of performance and computational cost.',
                        cta: 'Learn More'
                    },
                    'commercial': { 
                        name: 'Commercial Use', 
                        icon: 'üíº',
                        description: 'Unlike its predecessor, Llama 2 can be used to build and launch commercial applications, empowering startups and businesses.',
                        cta: 'Learn More'
                    }
                },
                'hi': {
                    'opensource': { 
                        name: '‡§ì‡§™‡§® ‡§∏‡•ã‡§∞‡•ç‡§∏', 
                        icon: 'üåê',
                        description: '‡§≤‡§æ‡§Æ‡§æ 2 ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§î‡§∞ ‡§µ‡§æ‡§£‡§ø‡§ú‡•ç‡§Ø‡§ø‡§ï ‡§â‡§™‡§Ø‡•ã‡§ó ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡•Å‡§´‡•ç‡§§ ‡§Æ‡•á‡§Ç ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•à, ‡§ú‡•ã ‡§∂‡§ï‡•ç‡§§‡§ø‡§∂‡§æ‡§≤‡•Ä ‡§è‡§Ü‡§à ‡§™‡•ç‡§∞‡•å‡§¶‡•ç‡§Ø‡•ã‡§ó‡§ø‡§ï‡•Ä ‡§§‡§ï ‡§µ‡•ç‡§Ø‡§æ‡§™‡§ï ‡§®‡§µ‡§æ‡§ö‡§æ‡§∞ ‡§î‡§∞ ‡§™‡§π‡•Å‡§Ç‡§ö ‡§ï‡•ã ‡§¨‡§¢‡§º‡§æ‡§µ‡§æ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§',
                        cta: '‡§î‡§∞ ‡§ú‡§æ‡§®‡•á‡§Ç'
                    },
                    'family': { 
                        name: '‡§Æ‡•â‡§°‡§≤ ‡§ï‡§æ ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞', 
                        icon: 'üë®‚Äçüë©‚Äçüëß‚Äçüë¶',
                        description: '‡§Ø‡§π ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§Ü‡§ï‡§æ‡§∞‡•ã‡§Ç (7B, 13B, 70B ‡§™‡•à‡§∞‡§æ‡§Æ‡•Ä‡§ü‡§∞) ‡§Æ‡•á‡§Ç ‡§Ü‡§§‡§æ ‡§π‡•à, ‡§ú‡§ø‡§∏‡§∏‡•á ‡§°‡•á‡§µ‡§≤‡§™‡§∞‡•ç‡§∏ ‡§ï‡•ã ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§î‡§∞ ‡§ï‡§Æ‡•ç‡§™‡•ç‡§Ø‡•Ç‡§ü‡•á‡§∂‡§®‡§≤ ‡§≤‡§æ‡§ó‡§§ ‡§ï‡§æ ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§∏‡§Ç‡§§‡•Å‡§≤‡§® ‡§ö‡•Å‡§®‡§®‡•á ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§Æ‡§ø‡§≤‡§§‡•Ä ‡§π‡•à‡•§',
                        cta: '‡§î‡§∞ ‡§ú‡§æ‡§®‡•á‡§Ç'
                    },
                    'commercial': { 
                        name: '‡§µ‡§æ‡§£‡§ø‡§ú‡•ç‡§Ø‡§ø‡§ï ‡§â‡§™‡§Ø‡•ã‡§ó', 
                        icon: 'üíº',
                        description: '‡§Ö‡§™‡§®‡•á ‡§™‡•Ç‡§∞‡•ç‡§µ‡§µ‡§∞‡•ç‡§§‡•Ä ‡§ï‡•á ‡§µ‡§ø‡§™‡§∞‡•Ä‡§§, ‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§µ‡§æ‡§£‡§ø‡§ú‡•ç‡§Ø‡§ø‡§ï ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•á ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§î‡§∞ ‡§≤‡•â‡§®‡•ç‡§ö ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§∏‡•ç‡§ü‡§æ‡§∞‡•ç‡§ü‡§Ö‡§™ ‡§î‡§∞ ‡§µ‡•ç‡§Ø‡§µ‡§∏‡§æ‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§∏‡§∂‡§ï‡•ç‡§§ ‡§¨‡§®‡§æ‡§§‡§æ ‡§π‡•à‡•§',
                        cta: '‡§î‡§∞ ‡§ú‡§æ‡§®‡•á‡§Ç'
                    }
                }
            };
            
            const careerPathData = {
                'en': {
                    path: [
                        { rank: "Open and Commercially Available", desc: "Unlike many other state-of-the-art models, Llama 2 is free for most commercial uses, which drastically lowers the barrier to entry for building AI products." },
                        { rank: "State-of-the-Art Performance", desc: "At the time of its release, Llama 2 was the best-performing open-source LLM, rivaling the performance of some closed-source models." },
                        { rank: "Emphasis on Safety", desc: "Meta invested heavily in safety fine-tuning and released a Responsible Use Guide, promoting the development of safe AI applications." },
                        { rank: "Strong Community Support", desc: "Being open-source, it benefits from a vibrant global community of developers who contribute to its improvement and create new applications." }
                    ]
                },
                'hi': {
                    path: [
                        { rank: "‡§ñ‡•Å‡§≤‡§æ ‡§î‡§∞ ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§∏‡§æ‡§Ø‡§ø‡§ï ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§â‡§™‡§≤‡§¨‡•ç‡§ß", desc: "‡§ï‡§à ‡§Ö‡§®‡•ç‡§Ø ‡§Ö‡§§‡•ç‡§Ø‡§æ‡§ß‡•Å‡§®‡§ø‡§ï ‡§Æ‡•â‡§°‡§≤‡•ã‡§Ç ‡§ï‡•á ‡§µ‡§ø‡§™‡§∞‡•Ä‡§§, ‡§≤‡§æ‡§Æ‡§æ 2 ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§Ç‡§∂ ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§∏‡§æ‡§Ø‡§ø‡§ï ‡§â‡§™‡§Ø‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡•Å‡§´‡•ç‡§§ ‡§π‡•à, ‡§ú‡•ã ‡§è‡§Ü‡§à ‡§â‡§§‡•ç‡§™‡§æ‡§¶‡•ã‡§Ç ‡§ï‡•á ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡•ç‡§∞‡§µ‡•á‡§∂ ‡§ï‡•Ä ‡§¨‡§æ‡§ß‡§æ ‡§ï‡•ã ‡§ï‡§æ‡§´‡•Ä ‡§ï‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§" },
                        { rank: "‡§Ö‡§§‡•ç‡§Ø‡§æ‡§ß‡•Å‡§®‡§ø‡§ï ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®", desc: "‡§Ö‡§™‡§®‡•Ä ‡§∞‡§ø‡§≤‡•Ä‡§ú ‡§ï‡•á ‡§∏‡§Æ‡§Ø, ‡§≤‡§æ‡§Æ‡§æ 2 ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§ì‡§™‡§®-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§è‡§≤‡§è‡§≤‡§è‡§Æ ‡§•‡§æ, ‡§ú‡•ã ‡§ï‡•Å‡§õ ‡§ï‡•ç‡§≤‡•ã‡§ú‡•ç‡§°-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡•ã ‡§ü‡§ï‡•ç‡§ï‡§∞ ‡§¶‡•á‡§§‡§æ ‡§•‡§æ‡•§" },
                        { rank: "‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§™‡§∞ ‡§ú‡•ã‡§∞", desc: "‡§Æ‡•á‡§ü‡§æ ‡§®‡•á ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§´‡§æ‡§á‡§®-‡§ü‡•ç‡§Ø‡•Ç‡§®‡§ø‡§Ç‡§ó ‡§Æ‡•á‡§Ç ‡§≠‡§æ‡§∞‡•Ä ‡§®‡§ø‡§µ‡•á‡§∂ ‡§ï‡§ø‡§Ø‡§æ ‡§î‡§∞ ‡§è‡§ï ‡§ú‡§ø‡§Æ‡•ç‡§Æ‡•á‡§¶‡§æ‡§∞ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ó‡§æ‡§á‡§° ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§Ø‡§æ, ‡§ú‡•ã ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§è‡§Ü‡§à ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•á ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§ï‡•ã ‡§¨‡§¢‡§º‡§æ‡§µ‡§æ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§" },
                        { rank: "‡§Æ‡§ú‡§¨‡•Ç‡§§ ‡§∏‡§æ‡§Æ‡•Å‡§¶‡§æ‡§Ø‡§ø‡§ï ‡§∏‡§Æ‡§∞‡•ç‡§•‡§®", desc: "‡§ì‡§™‡§®-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£, ‡§Ø‡§π ‡§°‡•á‡§µ‡§≤‡§™‡§∞‡•ç‡§∏ ‡§ï‡•á ‡§è‡§ï ‡§ú‡•Ä‡§µ‡§Ç‡§§ ‡§µ‡•à‡§∂‡•ç‡§µ‡§ø‡§ï ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø ‡§∏‡•á ‡§≤‡§æ‡§≠‡§æ‡§®‡•ç‡§µ‡§ø‡§§ ‡§π‡•ã‡§§‡§æ ‡§π‡•à ‡§ú‡•ã ‡§á‡§∏‡§ï‡•á ‡§∏‡•Å‡§ß‡§æ‡§∞ ‡§Æ‡•á‡§Ç ‡§Ø‡•ã‡§ó‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§®‡§è ‡§è‡§™‡•ç‡§≤‡§ø‡§ï‡•á‡§∂‡§® ‡§¨‡§®‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§" }
                    ]
                }
            };

            const translations = {
                'en': {
                    'header-title': 'ü¶ô Meta\'s Llama 2: The Ultimate Guide ü¶ô',
                    'header-subtitle': 'The Open Source AI Model for Researchers and Creators',
                    'sec1-title': '1. What is Meta\'s Llama 2? üí°',
                    'sec1-p1': 'Llama 2 is a family of large language models (LLMs) developed by Meta and released in partnership with Microsoft. It is a powerful, pretrained and fine-tuned model designed for a wide range of natural language tasks. What makes Llama 2 a landmark release is its **open-source availability for both research and commercial use**. This move has democratized access to state-of-the-art AI, empowering developers and businesses worldwide to build their own generative AI-powered applications.',
                    'sec2-title': '2. The Technology Behind Llama 2 üéØ',
                    'sec2-p1': 'Llama 2 is based on the powerful Transformer architecture and refined with extensive fine-tuning to be a helpful and safe assistant.',
                    'sec2-step1-title': 'Stage 1:', 'sec2-step1-name': 'Pre-training', 'sec2-step1-desc': 'Learning from vast public data',
                    'sec2-step2-title': 'Stage 2:', 'sec2-step2-name': 'Fine-Tuning', 'sec2-step2-desc': 'Aligning with human feedback',
                    'sec2-step3-title': 'Output', 'sec2-step3-name': 'Helpful Response', 'sec2-step3-desc': 'Generating a helpful and safe response',
                    'sec3-title': '3. Who Can Use It? ‚úÖ',
                    'sec3-p1': 'Llama 2 is available for a wide range of users, from individual researchers to large commercial entities.',
                    'sec3-age-title': 'Researchers & Academics:', 'sec3-age-desc': 'Can freely download and use the models for their research and experiments.',
                    'sec3-edu-title': 'Developers & Startups:', 'sec3-edu-desc': 'Can build and launch commercial products powered by Llama 2 without licensing fees (subject to the acceptable use policy).',
                    'sec3-attempts-title': 'Primary Platforms üìä',
                    'sec3-agerelax-list': '<li><strong>Direct Download:</strong> Available for download from Meta\'s official AI website.</li><li><strong>Cloud Providers:</strong> Available on platforms like Microsoft Azure, AWS, and Google Cloud for easy deployment.</li><li><strong>Hugging Face:</strong> The models are hosted on the Hugging Face Hub, a popular platform for the AI community.</li>',
                    'sec4-title': '4. Core Capabilities üìö',
                    'sec4-p1': 'Llama 2 is a highly capable language model proficient in a wide array of natural language understanding and generation tasks.',
                    'gs1-title': 'Text Generation',
                    'gs2-title': 'Summarization',
                    'gs3-title': 'Coding',
                    'gs4-title': 'Conversation',
                    'sec5-title': '5. How was Llama 2 Trained? üèõÔ∏è',
                    'sec5-p1': 'Llama 2\'s impressive performance is the result of a two-stage training process focused on both knowledge acquisition and safety alignment.',
                    'sec5-hindi-title': '1. Pre-training', 'sec5-hindi-p1': 'Building a vast base of knowledge.', 'sec5-hindi-list': '<li>Trained on 2 trillion tokens of publicly available data from the internet.</li><li>This process allows the model to learn grammar, facts, and reasoning abilities on a massive scale.</li>',
                    'sec5-essay-title': '2. Fine-Tuning (RLHF)', 'sec5-essay-p1': 'Making the model helpful and safe.', 'sec5-essay-list': '<li>The pretrained model is fine-tuned using a technique called Reinforcement Learning with Human Feedback (RLHF).</li><li>Human annotators create high-quality examples and rank model responses to teach it helpfulness and safety.</li><li>Over 1 million human annotations were used to align the Llama 2-Chat models.</li>',
                    'sec6-title': '6. Key Applications & Use Cases üåü',
                    'sec6-p1': 'The open and commercially viable nature of Llama 2 has unlocked a plethora of applications across various industries.',
                    'sec6-eng-title': 'Custom Chatbots', 'sec6-eng-list': '<li>Building specialized customer service agents.</li><li>Creating internal knowledge base assistants for employees.</li>',
                    'sec6-elec-title': 'Content Creation', 'sec6-elec-list': '<li>Drafting marketing copy, blog posts, and emails.</li><li>Generating creative story plots and scripts.</li><li>Summarizing articles and research papers.</li>',
                    'sec6-naic-title': 'Developer Tools', 'sec6-naic-list': '<li>Code generation and debugging assistants.</li><li>Natural language interfaces for software.</li><li>Automating documentation writing.</li>',
                    'sec7-title': '7. Performance & Benchmarks üéôÔ∏è',
                    'sec7-p1': 'Llama 2 has demonstrated strong performance across a wide range of academic and industry benchmarks, often outperforming other open-source models.',
                    'sec7-qualities-title': 'Key Benchmarks:',
                    'sec7-qualities-list': '<li class="flex items-start"><span class="text-2xl text-[#34A853] mr-3">üìö</span><div><strong class="text-white">MMLU (Massive Multitask Language Understanding):</strong> Tests general knowledge and problem-solving skills across 57 subjects.</div></li><li class="flex items-start"><span class="text-2xl text-[#34A853] mr-3">üíª</span><div><strong class="text-white">HumanEval:</strong> Measures the model\'s ability to correctly generate code for programming challenges.</div></li><li class="flex items-start"><span class="text-2xl text-[#34A853] mr-3">üß†</span><div><strong class="text-white">Reasoning Benchmarks:</strong> Such as GSM8K, which tests the model\'s ability to solve multi-step mathematical reasoning problems.</div></li>',
                    'sec8-title': '8. Prompting Llama 2: Best Practices üìà',
                    'sec8-p1': 'Effective prompting is crucial for unlocking the full potential of Llama 2. The model responds well to clear instructions and system-level guidance.',
                    'sec8-steps-title': 'üìö Key Principles:',
                    'sec8-step1-title': 'Use a System Prompt', 'sec8-step1-desc': 'Start with a system prompt (`<s>[INST] <<SYS>>...<</SYS>>`) to set the context and define the AI\'s persona and behavior for the entire conversation.',
                    'sec8-step2-title': 'Be Clear and Specific', 'sec8-step2-desc': 'Provide unambiguous instructions. The more detail you give about the desired output, format, and style, the better the result.',
                    'sec8-revision-title': 'üîÑ Advanced Techniques:',
                    'sec8-revision-list': '<li><strong>Few-Shot Prompting:</strong> Provide a few examples of input-output pairs within your prompt to guide the model\'s response.</li><li><strong>Iterative Conversation:</strong> Refine the output through a series of conversational turns, asking the model to improve upon its previous responses.</li>',
                    'sec9-title': '9. Model Evolution üóìÔ∏è',
                    'sec9-p1': 'Llama 2 is a significant step in Meta\'s long-term commitment to open science and the democratization of AI technology.',
                    'sec9-step1-title': 'Llama 1', 'sec9-step1-desc': '<strong>Timeline:</strong> February 2023. The first version was released to the research community, demonstrating the power of smaller, more efficient models.',
                    'sec9-step2-title': 'Llama 2 Release', 'sec9-step2-desc': '<strong>Timeline:</strong> July 2023. A major release with improved performance, a permissive license for commercial use, and dedicated chat-optimized models.',
                    'sec9-step3-title': 'Code Llama', 'sec9-step3-desc': '<strong>Timeline:</strong> August 2023. A specialized version of Llama 2 fine-tuned specifically for coding tasks.',
                    'sec9-step4-title': 'Llama 3 & Beyond', 'sec9-step4-desc': '<strong>Timeline:</strong> Future. Meta continues to work on the next generation of Llama models, expected to have even greater capabilities, including multimodality.',
                    'sec10-title': '10. Limitations & Responsible Use üö´',
                    'sec10-p1': 'While Llama 2 is a powerful tool, it\'s essential to be aware of its limitations and adhere to Meta\'s responsible use guidelines.',
                    'sec10-mistake1-title': 'Factual Inaccuracy:', 'sec10-mistake1-desc': 'Llama 2 can "hallucinate" and generate incorrect or fabricated information. Always fact-check outputs for critical use cases.',
                    'sec10-mistake2-title': 'Bias in Data:', 'sec10-mistake2-desc': 'The model was trained on public data and may reflect societal biases. Developers must implement safeguards for their applications.',
                    'sec10-mistake3-title': 'Acceptable Use Policy:', 'sec10-mistake3-desc': 'Commercial use is subject to Meta\'s Acceptable Use Policy, which prohibits use in certain sensitive or illegal activities.',
                    'sec10-mistake4-title': 'High Usage Clause:', 'sec10-mistake4-desc': 'Companies with over 700 million monthly active users must request a special license from Meta to use Llama 2.',
                    'sec11-title': '11. How to Access & Use üè´',
                    'sec11-p1': 'Getting started with Llama 2 is straightforward, thanks to its open-source nature and strong community support.',
                    'sec11-gov-title': 'üìö For Developers & Researchers', 'sec11-gov-list': '<li>**Download from Meta:** Apply for access and download the model weights directly from the Meta AI website.</li><li>**Hugging Face:** The easiest way to get started. Use the transformers library to download and run the models with just a few lines of code.</li>',
                    'sec11-pvt-title': 'üíª For Enterprise Deployment', 'sec11-pvt-p1': 'Leverage managed cloud services.', 'sec11-pvt-list': '<li>**Microsoft Azure:** Llama 2 is available in the Azure AI model catalog.</li><li>**AWS & Google Cloud:** Can be deployed and managed through services like Amazon SageMaker and Vertex AI.</li>',
                    'sec12-title': '12. The Future: Llama 3 & Open Source AI üöÄ',
                    'sec12-p1': 'The release of Llama 2 has catalyzed a global wave of innovation in open-source AI. The upcoming Llama 3 is highly anticipated and is expected to feature full multimodality and even more powerful reasoning capabilities, further narrowing the gap between open and closed-source models and empowering a new generation of AI-powered products and services.',
                    'sec12-career-btn': 'View Key Differentiators',
                    'sec13-title': '13. Core AI Concepts üìö',
                    'sec13-p1': 'Understanding these core concepts is fundamental to appreciating the technology behind Llama 2.',
                    'syllabus-oir-title': 'Open Source vs. Closed Source',
                    'syllabus-oir-list': '<div class="subject-item flex-col sm:flex-row"><div class="flex items-center w-full"><span class="icon">üåê</span><div class="flex-grow">Open Source (e.g., Llama 2)</div></div><div class="w-full sm:w-auto mt-2 sm:mt-0 flex justify-end flex-shrink-0 space-x-2"><button class="notes-btn">Notes</button><button class="lectures-btn">Lectures</button></div></div><div class="subject-item flex-col sm:flex-row"><div class="flex items-center w-full"><span class="icon">üîí</span><div class="flex-grow">Closed Source (e.g., GPT-4)</div></div><div class="w-full sm:w-auto mt-2 sm:mt-0 flex justify-end flex-shrink-0 space-x-2"><button class="notes-btn">Notes</button><button class="lectures-btn">Lectures</button></div></div>',
                    'syllabus-ppdt-title': 'Training & Alignment',
                    'syllabus-ppdt-list': '<div class="subject-item flex-col sm:flex-row"><div class="flex items-center w-full"><span class="icon">üìö</span><div class="flex-grow">Pre-training</div></div><div class="w-full sm:w-auto mt-2 sm:mt-0 flex justify-end flex-shrink-0 space-x-2"><button class="notes-btn">Notes</button><button class="lectures-btn">Lectures</button></div></div><div class="subject-item flex-col sm:flex-row"><div class="flex items-center w-full"><span class="icon">ü§ù</span><div class="flex-grow">RLHF (Reinforcement Learning with Human Feedback)</div></div><div class="w-full sm:w-auto mt-2 sm:mt-0 flex justify-end flex-shrink-0 space-x-2"><button class="notes-btn">Notes</button><button class="lectures-btn">Lectures</button></div></div>',
                    'footer-text': '&copy; 2025 | Democratizing Artificial Intelligence.',
                    'interviewChartData': {
                        labels: ['MMLU', 'GSM8K', 'HumanEval', 'TruthfulQA'],
                        data: [68.9, 56.8, 29.9, 63.3] // Llama 2 70B scores
                    }
                },
                'hi': {
                    'header-title': 'ü¶ô ‡§Æ‡•á‡§ü‡§æ ‡§ï‡§æ ‡§≤‡§æ‡§Æ‡§æ 2: ‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§ó‡§æ‡§á‡§° ü¶ô',
                    'header-subtitle': '‡§∂‡•ã‡§ß‡§ï‡§∞‡•ç‡§§‡§æ‡§ì‡§Ç ‡§î‡§∞ ‡§∞‡§ö‡§®‡§æ‡§ï‡§æ‡§∞‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ì‡§™‡§® ‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§è‡§Ü‡§à ‡§Æ‡•â‡§°‡§≤',
                    'sec1-title': '1. ‡§Æ‡•á‡§ü‡§æ ‡§ï‡§æ ‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à? üí°',
                    'sec1-p1': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§Æ‡•á‡§ü‡§æ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§µ‡§ø‡§ï‡§∏‡§ø‡§§ ‡§î‡§∞ ‡§Æ‡§æ‡§á‡§ï‡•ç‡§∞‡•ã‡§∏‡•â‡§´‡•ç‡§ü ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡§æ‡§ù‡•á‡§¶‡§æ‡§∞‡•Ä ‡§Æ‡•á‡§Ç ‡§ú‡§æ‡§∞‡•Ä ‡§¨‡§°‡§º‡•á ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•â‡§°‡§≤ (‡§è‡§≤‡§è‡§≤‡§è‡§Æ) ‡§ï‡§æ ‡§è‡§ï ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§π‡•à‡•§ ‡§Ø‡§π ‡§è‡§ï ‡§∂‡§ï‡•ç‡§§‡§ø‡§∂‡§æ‡§≤‡•Ä, ‡§™‡•Ç‡§∞‡•ç‡§µ-‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§ø‡§§ ‡§î‡§∞ ‡§†‡•Ä‡§ï-‡§†‡•Ä‡§ï ‡§Æ‡•â‡§°‡§≤ ‡§π‡•à ‡§ú‡§ø‡§∏‡•á ‡§™‡•ç‡§∞‡§æ‡§ï‡•É‡§§‡§ø‡§ï ‡§≠‡§æ‡§∑‡§æ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§è‡§ï ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§∂‡•ç‡§∞‡•É‡§Ç‡§ñ‡§≤‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§ú‡§º‡§æ‡§á‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§ ‡§ú‡•ã ‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡•ã ‡§è‡§ï ‡§ê‡§§‡§ø‡§π‡§æ‡§∏‡§ø‡§ï ‡§∞‡§ø‡§≤‡•Ä‡§ú ‡§¨‡§®‡§æ‡§§‡§æ ‡§π‡•à ‡§µ‡§π ‡§π‡•à ‡§á‡§∏‡§ï‡•Ä **‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§î‡§∞ ‡§µ‡§æ‡§£‡§ø‡§ú‡•ç‡§Ø‡§ø‡§ï ‡§â‡§™‡§Ø‡•ã‡§ó ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ì‡§™‡§®-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß‡§§‡§æ**‡•§ ‡§á‡§∏ ‡§ï‡§¶‡§Æ ‡§®‡•á ‡§Ö‡§§‡•ç‡§Ø‡§æ‡§ß‡•Å‡§®‡§ø‡§ï ‡§è‡§Ü‡§à ‡§§‡§ï ‡§™‡§π‡•Å‡§Ç‡§ö ‡§ï‡§æ ‡§≤‡•ã‡§ï‡§§‡§Ç‡§§‡•ç‡§∞‡•Ä‡§ï‡§∞‡§£ ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à, ‡§ú‡§ø‡§∏‡§∏‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§≠‡§∞ ‡§ï‡•á ‡§°‡•á‡§µ‡§≤‡§™‡§∞‡•ç‡§∏ ‡§î‡§∞ ‡§µ‡•ç‡§Ø‡§µ‡§∏‡§æ‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§Ö‡§™‡§®‡•á ‡§∏‡•ç‡§µ‡§Ø‡§Ç ‡§ï‡•á ‡§ú‡§®‡§∞‡•á‡§ü‡§ø‡§µ ‡§è‡§Ü‡§à-‡§∏‡§Ç‡§ö‡§æ‡§≤‡§ø‡§§ ‡§è‡§™‡•ç‡§≤‡§ø‡§ï‡•á‡§∂‡§® ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§∂‡§ï‡•ç‡§§ ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§',
                    'sec2-title': '2. ‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡•á ‡§™‡•Ä‡§õ‡•á ‡§ï‡•Ä ‡§§‡§ï‡§®‡•Ä‡§ï üéØ',
                    'sec2-p1': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§∂‡§ï‡•ç‡§§‡§ø‡§∂‡§æ‡§≤‡•Ä ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡§æ‡§∞‡•ç‡§Æ‡§∞ ‡§µ‡§æ‡§∏‡•ç‡§§‡•Å‡§ï‡§≤‡§æ ‡§™‡§∞ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§π‡•à ‡§î‡§∞ ‡§è‡§ï ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§î‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§µ‡•ç‡§Ø‡§æ‡§™‡§ï ‡§´‡§æ‡§á‡§®-‡§ü‡•ç‡§Ø‡•Ç‡§®‡§ø‡§Ç‡§ó ‡§ï‡•á ‡§∏‡§æ‡§• ‡§™‡§∞‡§ø‡§∑‡•ç‡§ï‡•É‡§§ ‡§π‡•à‡•§',
                    'sec2-step1-title': '‡§ö‡§∞‡§£ 1:', 'sec2-step1-name': '‡§™‡•Ç‡§∞‡•ç‡§µ-‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£', 'sec2-step1-desc': '‡§µ‡§ø‡§∂‡§æ‡§≤ ‡§∏‡§æ‡§∞‡•ç‡§µ‡§ú‡§®‡§ø‡§ï ‡§°‡•á‡§ü‡§æ ‡§∏‡•á ‡§∏‡•Ä‡§ñ‡§®‡§æ',
                    'sec2-step2-title': '‡§ö‡§∞‡§£ 2:', 'sec2-step2-name': '‡§´‡§æ‡§á‡§®-‡§ü‡•ç‡§Ø‡•Ç‡§®‡§ø‡§Ç‡§ó', 'sec2-step2-desc': '‡§Æ‡§æ‡§®‡§µ ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡§Ç‡§∞‡•á‡§ñ‡§ø‡§§ ‡§ï‡§∞‡§®‡§æ',
                    'sec2-step3-title': '‡§Ü‡§â‡§ü‡§™‡•Å‡§ü', 'sec2-step3-name': '‡§∏‡§π‡§æ‡§Ø‡§ï ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ', 'sec2-step3-desc': '‡§è‡§ï ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§î‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§â‡§§‡•ç‡§™‡§®‡•ç‡§® ‡§ï‡§∞‡§®‡§æ',
                    'sec3-title': '3. ‡§á‡§∏‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡•å‡§® ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à? ‚úÖ',
                    'sec3-p1': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§ó‡§§ ‡§∂‡•ã‡§ß‡§ï‡§∞‡•ç‡§§‡§æ‡§ì‡§Ç ‡§∏‡•á ‡§≤‡•á‡§ï‡§∞ ‡§¨‡§°‡§º‡•Ä ‡§µ‡§æ‡§£‡§ø‡§ú‡•ç‡§Ø‡§ø‡§ï ‡§∏‡§Ç‡§∏‡•ç‡§•‡§æ‡§ì‡§Ç ‡§§‡§ï, ‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ‡§ì‡§Ç ‡§ï‡•Ä ‡§è‡§ï ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§∂‡•ç‡§∞‡•É‡§Ç‡§ñ‡§≤‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•à‡•§',
                    'sec3-age-title': '‡§∂‡•ã‡§ß‡§ï‡§∞‡•ç‡§§‡§æ ‡§î‡§∞ ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ‡§µ‡§ø‡§¶:', 'sec3-age-desc': '‡§Ö‡§™‡§®‡•á ‡§∂‡•ã‡§ß ‡§î‡§∞ ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§∏‡•ç‡§µ‡§§‡§Ç‡§§‡•ç‡§∞ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§î‡§∞ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§',
                    'sec3-edu-title': '‡§°‡•á‡§µ‡§≤‡§™‡§∞‡•ç‡§∏ ‡§î‡§∞ ‡§∏‡•ç‡§ü‡§æ‡§∞‡•ç‡§ü‡§Ö‡§™:', 'sec3-edu-desc': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§∏‡§Ç‡§ö‡§æ‡§≤‡§ø‡§§ ‡§µ‡§æ‡§£‡§ø‡§ú‡•ç‡§Ø‡§ø‡§ï ‡§â‡§§‡•ç‡§™‡§æ‡§¶‡•ã‡§Ç ‡§ï‡§æ ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§î‡§∞ ‡§≤‡•â‡§®‡•ç‡§ö ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç ‡§¨‡§ø‡§®‡§æ ‡§≤‡§æ‡§á‡§∏‡•á‡§Ç‡§∏ ‡§∂‡•Å‡§≤‡•ç‡§ï ‡§ï‡•á (‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§â‡§™‡§Ø‡•ã‡§ó ‡§®‡•Ä‡§§‡§ø ‡§ï‡•á ‡§Ö‡§ß‡•Ä‡§®)‡•§',
                    'sec3-attempts-title': '‡§™‡•ç‡§∞‡§æ‡§•‡§Æ‡§ø‡§ï ‡§™‡•ç‡§≤‡•á‡§ü‡§´‡•â‡§∞‡•ç‡§Æ üìä',
                    'sec3-agerelax-list': '<li><strong>‡§∏‡•Ä‡§ß‡§æ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§°:</strong> ‡§Æ‡•á‡§ü‡§æ ‡§ï‡•Ä ‡§Ü‡§ß‡§ø‡§ï‡§æ‡§∞‡§ø‡§ï ‡§è‡§Ü‡§à ‡§µ‡•á‡§¨‡§∏‡§æ‡§á‡§ü ‡§∏‡•á ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•à‡•§</li><li><strong>‡§ï‡•ç‡§≤‡§æ‡§â‡§° ‡§™‡•ç‡§∞‡§¶‡§æ‡§§‡§æ:</strong> ‡§Ü‡§∏‡§æ‡§® ‡§™‡§∞‡§ø‡§®‡§ø‡§Ø‡•ã‡§ú‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡§æ‡§á‡§ï‡•ç‡§∞‡•ã‡§∏‡•â‡§´‡•ç‡§ü ‡§è‡§ú‡§º‡•ç‡§Ø‡•ã‡§∞, ‡§è‡§°‡§¨‡•ç‡§≤‡•ç‡§Ø‡•Ç‡§è‡§∏ ‡§î‡§∞ ‡§ó‡•Ç‡§ó‡§≤ ‡§ï‡•ç‡§≤‡§æ‡§â‡§° ‡§ú‡•à‡§∏‡•á ‡§™‡•ç‡§≤‡•á‡§ü‡§´‡§æ‡§∞‡•ç‡§Æ‡•ã‡§Ç ‡§™‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•à‡•§</li><li><strong>‡§π‡§ó‡§ø‡§Ç‡§ó ‡§´‡•á‡§∏:</strong> ‡§Æ‡•â‡§°‡§≤ ‡§π‡§ó‡§ø‡§Ç‡§ó ‡§´‡•á‡§∏ ‡§π‡§¨ ‡§™‡§∞ ‡§π‡•ã‡§∏‡•ç‡§ü ‡§ï‡§ø‡§è ‡§ó‡§è ‡§π‡•à‡§Ç, ‡§ú‡•ã ‡§è‡§Ü‡§à ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§≤‡•ã‡§ï‡§™‡•ç‡§∞‡§ø‡§Ø ‡§Æ‡§Ç‡§ö ‡§π‡•à‡•§</li>',
                    'sec4-title': '4. ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ‡§è‡§Ç üìö',
                    'sec4-p1': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§è‡§ï ‡§Ö‡§§‡•ç‡§Ø‡§ß‡§ø‡§ï ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•â‡§°‡§≤ ‡§π‡•à ‡§ú‡•ã ‡§™‡•ç‡§∞‡§æ‡§ï‡•É‡§§‡§ø‡§ï ‡§≠‡§æ‡§∑‡§æ ‡§∏‡§Æ‡§ù‡§®‡•á ‡§î‡§∞ ‡§™‡•Ä‡§¢‡§º‡•Ä ‡§ï‡•á ‡§ï‡§æ‡§∞‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§è‡§ï ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§∂‡•ç‡§∞‡•É‡§Ç‡§ñ‡§≤‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§∂‡§≤ ‡§π‡•à‡•§',
                    'gs1-title': '‡§™‡§æ‡§† ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£',
                    'gs2-title': '‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂',
                    'gs3-title': '‡§ï‡•ã‡§°‡§ø‡§Ç‡§ó',
                    'gs4-title': '‡§¨‡§æ‡§§‡§ö‡•Ä‡§§',
                    'sec5-title': '5. ‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡•ã ‡§ï‡•à‡§∏‡•á ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ? üèõÔ∏è',
                    'sec5-p1': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡§æ ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡§∂‡§æ‡§≤‡•Ä ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ú‡•ç‡§û‡§æ‡§® ‡§Ö‡§ß‡§ø‡§ó‡•ç‡§∞‡§π‡§£ ‡§î‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§∏‡§Ç‡§∞‡•á‡§ñ‡§£ ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§™‡§∞ ‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞‡§ø‡§§ ‡§¶‡•ã-‡§ö‡§∞‡§£‡•Ä‡§Ø ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§™‡•ç‡§∞‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§ï‡§æ ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ ‡§π‡•à‡•§',
                    'sec5-hindi-title': '1. ‡§™‡•Ç‡§∞‡•ç‡§µ-‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£', 'sec5-hindi-p1': '‡§ú‡•ç‡§û‡§æ‡§® ‡§ï‡§æ ‡§è‡§ï ‡§µ‡§ø‡§∂‡§æ‡§≤ ‡§Ü‡§ß‡§æ‡§∞ ‡§¨‡§®‡§æ‡§®‡§æ‡•§', 'sec5-hindi-list': '<li>‡§á‡§Ç‡§ü‡§∞‡§®‡•á‡§ü ‡§∏‡•á ‡§∏‡§æ‡§∞‡•ç‡§µ‡§ú‡§®‡§ø‡§ï ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§°‡•á‡§ü‡§æ ‡§ï‡•á 2 ‡§ü‡•ç‡§∞‡§ø‡§≤‡§ø‡§Ø‡§® ‡§ü‡•ã‡§ï‡§® ‡§™‡§∞ ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§ø‡§§‡•§</li><li>‡§Ø‡§π ‡§™‡•ç‡§∞‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§¨‡§°‡§º‡•á ‡§™‡•à‡§Æ‡§æ‡§®‡•á ‡§™‡§∞ ‡§µ‡•ç‡§Ø‡§æ‡§ï‡§∞‡§£, ‡§§‡§•‡•ç‡§Ø ‡§î‡§∞ ‡§§‡§∞‡•ç‡§ï ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ‡§ì‡§Ç ‡§ï‡•ã ‡§∏‡•Ä‡§ñ‡§®‡•á ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§¶‡•á‡§§‡•Ä ‡§π‡•à‡•§</li>',
                    'sec5-essay-title': '2. ‡§´‡§æ‡§á‡§®-‡§ü‡•ç‡§Ø‡•Ç‡§®‡§ø‡§Ç‡§ó (‡§Ü‡§∞‡§è‡§≤‡§è‡§ö‡§è‡§´)', 'sec5-essay-p1': '‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§î‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§¨‡§®‡§æ‡§®‡§æ‡•§', 'sec5-essay-list': '<li>‡§™‡•Ç‡§∞‡•ç‡§µ-‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§ø‡§§ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§Æ‡§æ‡§®‡§µ ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡•Å‡§¶‡•É‡§¢‡•Ä‡§ï‡§∞‡§£ ‡§∏‡•Ä‡§ñ‡§®‡§æ (‡§Ü‡§∞‡§è‡§≤‡§è‡§ö‡§è‡§´) ‡§®‡§æ‡§Æ‡§ï ‡§§‡§ï‡§®‡•Ä‡§ï ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§ï‡•á ‡§†‡•Ä‡§ï-‡§†‡•Ä‡§ï ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§</li><li>‡§Æ‡§æ‡§®‡§µ ‡§è‡§®‡•ã‡§ü‡•á‡§ü‡§∞ ‡§â‡§ö‡•ç‡§ö-‡§ó‡•Å‡§£‡§µ‡§§‡•ç‡§§‡§æ ‡§µ‡§æ‡§≤‡•á ‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡§¨‡§®‡§æ‡§§‡•á ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•Ä ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ‡§ì‡§Ç ‡§ï‡•ã ‡§∞‡•à‡§Ç‡§ï ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç ‡§§‡§æ‡§ï‡§ø ‡§á‡§∏‡•á ‡§∏‡§π‡§æ‡§Ø‡§ï‡§§‡§æ ‡§î‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§∏‡§ø‡§ñ‡§æ‡§à ‡§ú‡§æ ‡§∏‡§ï‡•á‡•§</li><li>‡§≤‡§æ‡§Æ‡§æ 2-‡§ö‡•à‡§ü ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§∏‡§Ç‡§∞‡•á‡§ñ‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è 1 ‡§Æ‡§ø‡§≤‡§ø‡§Ø‡§® ‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§Æ‡§æ‡§®‡§µ ‡§è‡§®‡•ã‡§ü‡•á‡§∂‡§® ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ‡•§</li>',
                    'sec6-title': '6. ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§î‡§∞ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡•á ‡§Æ‡§æ‡§Æ‡§≤‡•á üåü',
                    'sec6-p1': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡•Ä ‡§ñ‡•Å‡§≤‡•Ä ‡§î‡§∞ ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§∏‡§æ‡§Ø‡§ø‡§ï ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§µ‡•ç‡§Ø‡§µ‡§π‡§æ‡§∞‡•ç‡§Ø ‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø ‡§®‡•á ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§â‡§¶‡•ç‡§Ø‡•ã‡§ó‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•Ä ‡§Ö‡§ß‡§ø‡§ï‡§§‡§æ ‡§ï‡•ã ‡§ñ‡•ã‡§≤ ‡§¶‡§ø‡§Ø‡§æ ‡§π‡•à‡•§',
                    'sec6-eng-title': '‡§ï‡§∏‡•ç‡§ü‡§Æ ‡§ö‡•à‡§ü‡§¨‡•â‡§ü', 'sec6-eng-list': '<li>‡§µ‡§ø‡§∂‡•á‡§∑ ‡§ó‡•ç‡§∞‡§æ‡§π‡§ï ‡§∏‡•á‡§µ‡§æ ‡§è‡§ú‡•á‡§Ç‡§ü ‡§¨‡§®‡§æ‡§®‡§æ‡•§</li><li>‡§ï‡§∞‡•ç‡§Æ‡§ö‡§æ‡§∞‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§Ç‡§§‡§∞‡§ø‡§ï ‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§¨‡§®‡§æ‡§®‡§æ‡•§</li>',
                    'sec6-elec-title': '‡§∏‡§æ‡§Æ‡§ó‡•ç‡§∞‡•Ä ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£', 'sec6-elec-list': '<li>‡§µ‡§ø‡§™‡§£‡§® ‡§ï‡•â‡§™‡•Ä, ‡§¨‡•ç‡§≤‡•â‡§ó ‡§™‡•ã‡§∏‡•ç‡§ü ‡§î‡§∞ ‡§à‡§Æ‡•á‡§≤ ‡§ï‡§æ ‡§Æ‡§∏‡•å‡§¶‡§æ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§®‡§æ‡•§</li><li>‡§∞‡§ö‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§ï‡§π‡§æ‡§®‡•Ä ‡§≠‡•Ç‡§ñ‡§Ç‡§° ‡§î‡§∞ ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§®‡§æ‡•§</li><li>‡§≤‡•á‡§ñ‡•ã‡§Ç ‡§î‡§∞ ‡§∂‡•ã‡§ß ‡§™‡§§‡•ç‡§∞‡•ã‡§Ç ‡§ï‡§æ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂‡•§</li>',
                    'sec6-naic-title': '‡§°‡•á‡§µ‡§≤‡§™‡§∞ ‡§â‡§™‡§ï‡§∞‡§£', 'sec6-naic-list': '<li>‡§ï‡•ã‡§° ‡§ú‡§®‡§∞‡•á‡§∂‡§® ‡§î‡§∞ ‡§°‡•Ä‡§¨‡§ó‡§ø‡§Ç‡§ó ‡§∏‡§π‡§æ‡§Ø‡§ï‡•§</li><li>‡§∏‡•â‡§´‡•ç‡§ü‡§µ‡•á‡§Ø‡§∞ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡•ç‡§∞‡§æ‡§ï‡•É‡§§‡§ø‡§ï ‡§≠‡§æ‡§∑‡§æ ‡§á‡§Ç‡§ü‡§∞‡§´‡•á‡§∏‡•§</li><li>‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º‡•Ä‡§ï‡§∞‡§£ ‡§≤‡•á‡§ñ‡§® ‡§ï‡•ã ‡§∏‡•ç‡§µ‡§ö‡§æ‡§≤‡§ø‡§§ ‡§ï‡§∞‡§®‡§æ‡•§</li>',
                    'sec7-title': '7. ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§î‡§∞ ‡§¨‡•á‡§Ç‡§ö‡§Æ‡§æ‡§∞‡•ç‡§ï üéôÔ∏è',
                    'sec7-p1': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§®‡•á ‡§Ö‡§ï‡§æ‡§¶‡§Æ‡§ø‡§ï ‡§î‡§∞ ‡§â‡§¶‡•ç‡§Ø‡•ã‡§ó ‡§¨‡•á‡§Ç‡§ö‡§Æ‡§æ‡§∞‡•ç‡§ï ‡§ï‡•Ä ‡§è‡§ï ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§∂‡•ç‡§∞‡•É‡§Ç‡§ñ‡§≤‡§æ ‡§Æ‡•á‡§Ç ‡§Æ‡§ú‡§¨‡•Ç‡§§ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§æ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§Ö‡§ï‡•ç‡§∏‡§∞ ‡§Ö‡§®‡•ç‡§Ø ‡§ì‡§™‡§®-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§Æ‡•â‡§°‡§≤ ‡§∏‡•á ‡§¨‡•á‡§π‡§§‡§∞ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§',
                    'sec7-qualities-title': '‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§¨‡•á‡§Ç‡§ö‡§Æ‡§æ‡§∞‡•ç‡§ï:',
                    'sec7-qualities-list': '<li class="flex items-start"><span class="text-2xl text-[#34A853] mr-3">üìö</span><div><strong class="text-white">‡§è‡§Æ‡§è‡§Æ‡§è‡§≤‡§Ø‡•Ç (‡§µ‡•ç‡§Ø‡§æ‡§™‡§ï ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§ü‡§æ‡§∏‡•ç‡§ï ‡§≠‡§æ‡§∑‡§æ ‡§∏‡§Æ‡§ù‡§®‡§æ):</strong> 57 ‡§µ‡§ø‡§∑‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡•ç‡§û‡§æ‡§® ‡§î‡§∞ ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ-‡§∏‡§Æ‡§æ‡§ß‡§æ‡§® ‡§ï‡•å‡§∂‡§≤ ‡§ï‡§æ ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§</div></li><li class="flex items-start"><span class="text-2xl text-[#34A853] mr-3">üíª</span><div><strong class="text-white">‡§π‡•ç‡§Ø‡•Ç‡§Æ‡§®‡§á‡§µ‡§≤:</strong> ‡§™‡•ç‡§∞‡•ã‡§ó‡•ç‡§∞‡§æ‡§Æ‡§ø‡§Ç‡§ó ‡§ö‡•Å‡§®‡•å‡§§‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§π‡•Ä ‡§¢‡§Ç‡§ó ‡§∏‡•á ‡§ï‡•ã‡§° ‡§â‡§§‡•ç‡§™‡§®‡•ç‡§® ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•Ä ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§ï‡•ã ‡§Æ‡§æ‡§™‡§§‡§æ ‡§π‡•à‡•§</div></li><li class="flex items-start"><span class="text-2xl text-[#34A853] mr-3">üß†</span><div><strong class="text-white">‡§§‡§∞‡•ç‡§ï ‡§¨‡•á‡§Ç‡§ö‡§Æ‡§æ‡§∞‡•ç‡§ï:</strong> ‡§ú‡•à‡§∏‡•á ‡§ú‡•Ä‡§è‡§∏‡§è‡§Æ8‡§ï‡•á, ‡§ú‡•ã ‡§¨‡§π‡•Å-‡§ö‡§∞‡§£‡•Ä‡§Ø ‡§ó‡§£‡§ø‡§§‡•Ä‡§Ø ‡§§‡§∞‡•ç‡§ï ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ‡§ì‡§Ç ‡§ï‡•ã ‡§π‡§≤ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•Ä ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§ï‡§æ ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§</div></li>',
                    'sec8-title': '8. ‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡•ã ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü ‡§ï‡§∞‡§®‡§æ: ‡§∏‡§∞‡•ç‡§µ‡•ã‡§§‡•ç‡§§‡§Æ ‡§Ö‡§≠‡•ç‡§Ø‡§æ‡§∏ üìà',
                    'sec8-p1': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡•Ä ‡§™‡•Ç‡§∞‡•Ä ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§ï‡•ã ‡§Ö‡§®‡§≤‡•â‡§ï ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡•Ä ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü‡§ø‡§Ç‡§ó ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§π‡•à‡•§ ‡§Æ‡•â‡§°‡§≤ ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂‡•ã‡§Ç ‡§î‡§∞ ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ-‡§∏‡•ç‡§§‡§∞‡•Ä‡§Ø ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§',
                    'sec8-steps-title': 'üìö ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§∏‡§ø‡§¶‡•ç‡§ß‡§æ‡§Ç‡§§:',
                    'sec8-step1-title': '‡§è‡§ï ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç', 'sec8-step1-desc': '‡§™‡•Ç‡§∞‡•Ä ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§∏‡•á‡§ü ‡§ï‡§∞‡§®‡•á ‡§î‡§∞ ‡§è‡§Ü‡§à ‡§ï‡•á ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§§‡•ç‡§µ ‡§î‡§∞ ‡§µ‡•ç‡§Ø‡§µ‡§π‡§æ‡§∞ ‡§ï‡•ã ‡§™‡§∞‡§ø‡§≠‡§æ‡§∑‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü (`<s>[INST] <<SYS>>...<</SYS>>`) ‡§∏‡•á ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡•á‡§Ç‡•§',
                    'sec8-step2-title': '‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§î‡§∞ ‡§µ‡§ø‡§∂‡§ø‡§∑‡•ç‡§ü ‡§∞‡§π‡•á‡§Ç', 'sec8-step2-desc': '‡§Ö‡§∏‡§Ç‡§¶‡§ø‡§ó‡•ç‡§ß ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§Ü‡§™ ‡§µ‡§æ‡§Ç‡§õ‡§ø‡§§ ‡§Ü‡§â‡§ü‡§™‡•Å‡§ü, ‡§™‡•ç‡§∞‡§æ‡§∞‡•Ç‡§™ ‡§î‡§∞ ‡§∂‡•à‡§≤‡•Ä ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§ú‡§ø‡§§‡§®‡§æ ‡§Ö‡§ß‡§ø‡§ï ‡§µ‡§ø‡§µ‡§∞‡§£ ‡§¶‡•á‡§Ç‡§ó‡•á, ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ ‡§â‡§§‡§®‡§æ ‡§π‡•Ä ‡§¨‡•á‡§π‡§§‡§∞ ‡§π‡•ã‡§ó‡§æ‡•§',
                    'sec8-revision-title': 'üîÑ ‡§â‡§®‡•ç‡§®‡§§ ‡§§‡§ï‡§®‡•Ä‡§ï‡•á‡§Ç:',
                    'sec8-revision-list': '<li><strong>‡§´‡•ç‡§Ø‡•Ç-‡§∂‡•â‡§ü ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü‡§ø‡§Ç‡§ó:</strong> ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•Ä ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§ï‡§æ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§™‡§®‡•á ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü ‡§ï‡•á ‡§≠‡•Ä‡§§‡§∞ ‡§á‡§®‡§™‡•Å‡§ü-‡§Ü‡§â‡§ü‡§™‡•Å‡§ü ‡§ú‡•ã‡§°‡§º‡•á ‡§ï‡•á ‡§ï‡•Å‡§õ ‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡•á‡§Ç‡•§</li><li><strong>‡§™‡•Å‡§®‡§∞‡§æ‡§µ‡•É‡§§‡•ç‡§§‡§ø ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§:</strong> ‡§Ö‡§™‡§®‡•Ä ‡§™‡§ø‡§õ‡§≤‡•Ä ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ‡§ì‡§Ç ‡§Æ‡•á‡§Ç ‡§∏‡•Å‡§ß‡§æ‡§∞ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡•â‡§°‡§≤ ‡§∏‡•á ‡§™‡•Ç‡§õ‡§§‡•á ‡§π‡•Å‡§è, ‡§∏‡§Ç‡§µ‡§æ‡§¶‡§æ‡§§‡•ç‡§Æ‡§ï ‡§Æ‡•ã‡§°‡§º‡•ã‡§Ç ‡§ï‡•Ä ‡§è‡§ï ‡§∂‡•ç‡§∞‡•É‡§Ç‡§ñ‡§≤‡§æ ‡§ï‡•á ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ ‡§∏‡•á ‡§Ü‡§â‡§ü‡§™‡•Å‡§ü ‡§ï‡•ã ‡§™‡§∞‡§ø‡§∑‡•ç‡§ï‡•É‡§§ ‡§ï‡§∞‡•á‡§Ç‡•§</li>',
                    'sec9-title': '9. ‡§Æ‡•â‡§°‡§≤ ‡§µ‡§ø‡§ï‡§æ‡§∏ üóìÔ∏è',
                    'sec9-p1': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§Æ‡•á‡§ü‡§æ ‡§ï‡•Ä ‡§ñ‡•Å‡§≤‡•Ä ‡§µ‡§ø‡§ú‡•ç‡§û‡§æ‡§® ‡§î‡§∞ ‡§è‡§Ü‡§à ‡§™‡•ç‡§∞‡•å‡§¶‡•ç‡§Ø‡•ã‡§ó‡§ø‡§ï‡•Ä ‡§ï‡•á ‡§≤‡•ã‡§ï‡§§‡§Ç‡§§‡•ç‡§∞‡•Ä‡§ï‡§∞‡§£ ‡§ï‡•á ‡§™‡•ç‡§∞‡§§‡§ø ‡§¶‡•Ä‡§∞‡•ç‡§ò‡§ï‡§æ‡§≤‡§ø‡§ï ‡§™‡•ç‡§∞‡§§‡§ø‡§¨‡§¶‡•ç‡§ß‡§§‡§æ ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§ï‡§¶‡§Æ ‡§π‡•à‡•§',
                    'sec9-step1-title': '‡§≤‡§æ‡§Æ‡§æ 1', 'sec9-step1-desc': '<strong>‡§∏‡§Æ‡§Ø‡§∞‡•á‡§ñ‡§æ:</strong> ‡§´‡§∞‡§µ‡§∞‡•Ä 2023‡•§ ‡§™‡§π‡§≤‡§æ ‡§∏‡§Ç‡§∏‡•ç‡§ï‡§∞‡§£ ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ, ‡§ú‡•ã ‡§õ‡•ã‡§ü‡•á, ‡§Ö‡§ß‡§ø‡§ï ‡§ï‡•Å‡§∂‡§≤ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•Ä ‡§∂‡§ï‡•ç‡§§‡§ø ‡§ï‡§æ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§',
                    'sec9-step2-title': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§∞‡§ø‡§≤‡•Ä‡§ú', 'sec9-step2-desc': '<strong>‡§∏‡§Æ‡§Ø‡§∞‡•á‡§ñ‡§æ:</strong> ‡§ú‡•Å‡§≤‡§æ‡§à 2023‡•§ ‡§¨‡•á‡§π‡§§‡§∞ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®, ‡§µ‡§æ‡§£‡§ø‡§ú‡•ç‡§Ø‡§ø‡§ï ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§Ö‡§®‡•Å‡§ú‡•ç‡§û‡•á‡§Ø ‡§≤‡§æ‡§á‡§∏‡•á‡§Ç‡§∏ ‡§î‡§∞ ‡§∏‡§Æ‡§∞‡•ç‡§™‡§ø‡§§ ‡§ö‡•à‡§ü-‡§Ö‡§®‡•Å‡§ï‡•Ç‡§≤‡§ø‡§§ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§è‡§ï ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§∞‡§ø‡§≤‡•Ä‡§ú‡•§',
                    'sec9-step3-title': '‡§ï‡•ã‡§° ‡§≤‡§æ‡§Æ‡§æ', 'sec9-step3-desc': '<strong>‡§∏‡§Æ‡§Ø‡§∞‡•á‡§ñ‡§æ:</strong> ‡§Ö‡§ó‡§∏‡•ç‡§§ 2023‡•§ ‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡§æ ‡§è‡§ï ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§∏‡§Ç‡§∏‡•ç‡§ï‡§∞‡§£ ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§ï‡•ã‡§°‡§ø‡§Ç‡§ó ‡§ï‡§æ‡§∞‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§†‡•Ä‡§ï-‡§†‡•Ä‡§ï ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§',
                    'sec9-step4-title': '‡§≤‡§æ‡§Æ‡§æ 3 ‡§î‡§∞ ‡§™‡§∞‡•á', 'sec9-step4-desc': '<strong>‡§∏‡§Æ‡§Ø‡§∞‡•á‡§ñ‡§æ:</strong> ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø‡•§ ‡§Æ‡•á‡§ü‡§æ ‡§≤‡§æ‡§Æ‡§æ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•Ä ‡§Ö‡§ó‡§≤‡•Ä ‡§™‡•Ä‡§¢‡§º‡•Ä ‡§™‡§∞ ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§®‡§æ ‡§ú‡§æ‡§∞‡•Ä ‡§∞‡§ñ‡§§‡§æ ‡§π‡•à, ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤ ‡§∏‡§π‡§ø‡§§ ‡§î‡§∞ ‡§≠‡•Ä ‡§Ö‡§ß‡§ø‡§ï ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ‡§ì‡§Ç ‡§ï‡•Ä ‡§â‡§Æ‡•ç‡§Æ‡•Ä‡§¶ ‡§π‡•à‡•§',
                    'sec10-title': '10. ‡§∏‡•Ä‡§Æ‡§æ‡§è‡§Ç ‡§î‡§∞ ‡§ú‡§ø‡§Æ‡•ç‡§Æ‡•á‡§¶‡§æ‡§∞ ‡§â‡§™‡§Ø‡•ã‡§ó üö´',
                    'sec10-p1': '‡§ú‡§¨‡§ï‡§ø ‡§≤‡§æ‡§Æ‡§æ 2 ‡§è‡§ï ‡§∂‡§ï‡•ç‡§§‡§ø‡§∂‡§æ‡§≤‡•Ä ‡§â‡§™‡§ï‡§∞‡§£ ‡§π‡•à, ‡§á‡§∏‡§ï‡•Ä ‡§∏‡•Ä‡§Æ‡§æ‡§ì‡§Ç ‡§∏‡•á ‡§Ö‡§µ‡§ó‡§§ ‡§π‡•ã‡§®‡§æ ‡§î‡§∞ ‡§Æ‡•á‡§ü‡§æ ‡§ï‡•á ‡§ú‡§ø‡§Æ‡•ç‡§Æ‡•á‡§¶‡§æ‡§∞ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§¶‡§ø‡§∂‡§æ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂‡•ã‡§Ç ‡§ï‡§æ ‡§™‡§æ‡§≤‡§® ‡§ï‡§∞‡§®‡§æ ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•à‡•§',
                    'sec10-mistake1-title': '‡§§‡§•‡•ç‡§Ø‡§æ‡§§‡•ç‡§Æ‡§ï ‡§Ö‡§∂‡•Å‡§¶‡•ç‡§ß‡§ø:', 'sec10-mistake1-desc': '‡§≤‡§æ‡§Æ‡§æ 2 "‡§Æ‡§§‡§ø‡§≠‡•ç‡§∞‡§Æ" ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§ó‡§≤‡§§ ‡§Ø‡§æ ‡§Æ‡§®‡§ó‡§¢‡§º‡§Ç‡§§ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§â‡§§‡•ç‡§™‡§®‡•ç‡§® ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§ ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡•á ‡§Æ‡§æ‡§Æ‡§≤‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§â‡§ü‡§™‡•Å‡§ü ‡§ï‡•ã ‡§π‡§Æ‡•á‡§∂‡§æ ‡§§‡§•‡•ç‡§Ø-‡§ú‡§æ‡§Ç‡§ö ‡§ï‡§∞‡•á‡§Ç‡•§',
                    'sec10-mistake2-title': '‡§°‡•á‡§ü‡§æ ‡§Æ‡•á‡§Ç ‡§™‡•Ç‡§∞‡•ç‡§µ‡§æ‡§ó‡•ç‡§∞‡§π:', 'sec10-mistake2-desc': '‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§∏‡§æ‡§∞‡•ç‡§µ‡§ú‡§®‡§ø‡§ï ‡§°‡•á‡§ü‡§æ ‡§™‡§∞ ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ ‡§î‡§∞ ‡§Ø‡§π ‡§∏‡§æ‡§Æ‡§æ‡§ú‡§ø‡§ï ‡§™‡•Ç‡§∞‡•ç‡§µ‡§æ‡§ó‡•ç‡§∞‡§π‡•ã‡§Ç ‡§ï‡•ã ‡§™‡•ç‡§∞‡§§‡§ø‡§¨‡§ø‡§Ç‡§¨‡§ø‡§§ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§ ‡§°‡•á‡§µ‡§≤‡§™‡§∞‡•ç‡§∏ ‡§ï‡•ã ‡§Ö‡§™‡§®‡•á ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§â‡§™‡§æ‡§Ø ‡§≤‡§æ‡§ó‡•Ç ‡§ï‡§∞‡§®‡•á ‡§ö‡§æ‡§π‡§ø‡§è‡•§',
                    'sec10-mistake3-title': '‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§â‡§™‡§Ø‡•ã‡§ó ‡§®‡•Ä‡§§‡§ø:', 'sec10-mistake3-desc': '‡§µ‡§æ‡§£‡§ø‡§ú‡•ç‡§Ø‡§ø‡§ï ‡§â‡§™‡§Ø‡•ã‡§ó ‡§Æ‡•á‡§ü‡§æ ‡§ï‡•Ä ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§â‡§™‡§Ø‡•ã‡§ó ‡§®‡•Ä‡§§‡§ø ‡§ï‡•á ‡§Ö‡§ß‡•Ä‡§® ‡§π‡•à, ‡§ú‡•ã ‡§ï‡•Å‡§õ ‡§∏‡§Ç‡§µ‡•á‡§¶‡§®‡§∂‡•Ä‡§≤ ‡§Ø‡§æ ‡§Ö‡§µ‡•à‡§ß ‡§ó‡§§‡§ø‡§µ‡§ø‡§ß‡§ø‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡•ã ‡§™‡•ç‡§∞‡§§‡§ø‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§',
                    'sec10-mistake4-title': '‡§â‡§ö‡•ç‡§ö ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ñ‡§Ç‡§°:', 'sec10-mistake4-desc': '700 ‡§Æ‡§ø‡§≤‡§ø‡§Ø‡§® ‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§Æ‡§æ‡§∏‡§ø‡§ï ‡§∏‡§ï‡•ç‡§∞‡§ø‡§Ø ‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ‡§ì‡§Ç ‡§µ‡§æ‡§≤‡•Ä ‡§ï‡§Ç‡§™‡§®‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡•á‡§ü‡§æ ‡§∏‡•á ‡§è‡§ï ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§≤‡§æ‡§á‡§∏‡•á‡§Ç‡§∏ ‡§ï‡§æ ‡§Ö‡§®‡•Å‡§∞‡•ã‡§ß ‡§ï‡§∞‡§®‡§æ ‡§π‡•ã‡§ó‡§æ‡•§',
                    'sec11-title': '11. ‡§ï‡•à‡§∏‡•á ‡§™‡§π‡•Å‡§Ç‡§ö‡•á‡§Ç ‡§î‡§∞ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç üè´',
                    'sec11-p1': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§ï‡§∞‡§®‡§æ ‡§∏‡•Ä‡§ß‡§æ ‡§π‡•à, ‡§á‡§∏‡§ï‡•Ä ‡§ì‡§™‡§®-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø ‡§î‡§∞ ‡§Æ‡§ú‡§¨‡•Ç‡§§ ‡§∏‡§æ‡§Æ‡•Å‡§¶‡§æ‡§Ø‡§ø‡§ï ‡§∏‡§Æ‡§∞‡•ç‡§•‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶‡•§',
                    'sec11-gov-title': 'üìö ‡§°‡•á‡§µ‡§≤‡§™‡§∞‡•ç‡§∏ ‡§î‡§∞ ‡§∂‡•ã‡§ß‡§ï‡§∞‡•ç‡§§‡§æ‡§ì‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è', 'sec11-gov-list': '<li>**‡§Æ‡•á‡§ü‡§æ ‡§∏‡•á ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç:** ‡§™‡§π‡•Å‡§Ç‡§ö ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§µ‡•á‡§¶‡§® ‡§ï‡§∞‡•á‡§Ç ‡§î‡§∞ ‡§Æ‡•á‡§ü‡§æ ‡§è‡§Ü‡§à ‡§µ‡•á‡§¨‡§∏‡§æ‡§á‡§ü ‡§∏‡•á ‡§∏‡•Ä‡§ß‡•á ‡§Æ‡•â‡§°‡§≤ ‡§≠‡§æ‡§∞ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç‡•§</li><li>**‡§π‡§ó‡§ø‡§Ç‡§ó ‡§´‡•á‡§∏:** ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ ‡§∏‡§¨‡§∏‡•á ‡§Ü‡§∏‡§æ‡§® ‡§§‡§∞‡•Ä‡§ï‡§æ‡•§ ‡§Æ‡•â‡§°‡§≤ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ï‡§∞‡§®‡•á ‡§î‡§∞ ‡§ï‡•Å‡§õ ‡§π‡•Ä ‡§™‡§Ç‡§ï‡•ç‡§§‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§ï‡•ã‡§° ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ö‡§≤‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡§æ‡§∞‡•ç‡§Æ‡§∞ ‡§≤‡§æ‡§á‡§¨‡•ç‡§∞‡•á‡§∞‡•Ä ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§</li>',
                    'sec11-pvt-title': 'üíª ‡§è‡§Ç‡§ü‡§∞‡§™‡•ç‡§∞‡§æ‡§á‡§ú ‡§™‡§∞‡§ø‡§®‡§ø‡§Ø‡•ã‡§ú‡§® ‡§ï‡•á ‡§≤‡§ø‡§è', 'sec11-pvt-p1': '‡§™‡•ç‡§∞‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§ï‡•ç‡§≤‡§æ‡§â‡§° ‡§∏‡•á‡§µ‡§æ‡§ì‡§Ç ‡§ï‡§æ ‡§≤‡§æ‡§≠ ‡§â‡§†‡§æ‡§è‡§Ç‡•§', 'sec11-pvt-list': '<li>**‡§Æ‡§æ‡§á‡§ï‡•ç‡§∞‡•ã‡§∏‡•â‡§´‡•ç‡§ü ‡§è‡§ú‡§º‡•ç‡§Ø‡•ã‡§∞:** ‡§≤‡§æ‡§Æ‡§æ 2 ‡§è‡§ú‡§º‡•ç‡§Ø‡•ã‡§∞ ‡§è‡§Ü‡§à ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•à‡§ü‡§≤‡•â‡§ó ‡§Æ‡•á‡§Ç ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•à‡•§</li><li>**‡§è‡§°‡§¨‡•ç‡§≤‡•ç‡§Ø‡•Ç‡§è‡§∏ ‡§î‡§∞ ‡§ó‡•Ç‡§ó‡§≤ ‡§ï‡•ç‡§≤‡§æ‡§â‡§°:** ‡§Ö‡§Æ‡•á‡§ú‡§º‡•Ö‡§® ‡§∏‡•á‡§ú‡§Æ‡•á‡§ï‡§∞ ‡§î‡§∞ ‡§µ‡§∞‡•ç‡§ü‡•á‡§ï‡•ç‡§∏ ‡§è‡§Ü‡§à ‡§ú‡•à‡§∏‡•Ä ‡§∏‡•á‡§µ‡§æ‡§ì‡§Ç ‡§ï‡•á ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ ‡§∏‡•á ‡§§‡•à‡§®‡§æ‡§§ ‡§î‡§∞ ‡§™‡•ç‡§∞‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§</li>',
                    'sec12-title': '12. ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø: ‡§≤‡§æ‡§Æ‡§æ 3 ‡§î‡§∞ ‡§ì‡§™‡§® ‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§è‡§Ü‡§à üöÄ',
                    'sec12-p1': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡•Ä ‡§∞‡§ø‡§≤‡•Ä‡§ú ‡§®‡•á ‡§ì‡§™‡§®-‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§è‡§Ü‡§à ‡§Æ‡•á‡§Ç ‡§®‡§µ‡§æ‡§ö‡§æ‡§∞ ‡§ï‡•Ä ‡§è‡§ï ‡§µ‡•à‡§∂‡•ç‡§µ‡§ø‡§ï ‡§≤‡§π‡§∞ ‡§ï‡•ã ‡§â‡§§‡•ç‡§™‡•ç‡§∞‡•á‡§∞‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à‡•§ ‡§Ü‡§ó‡§æ‡§Æ‡•Ä ‡§≤‡§æ‡§Æ‡§æ 3 ‡§ï‡§æ ‡§¨‡•á‡§∏‡§¨‡•ç‡§∞‡•Ä ‡§∏‡•á ‡§á‡§Ç‡§§‡§ú‡§æ‡§∞ ‡§π‡•à ‡§î‡§∞ ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§Æ‡•â‡§°‡§≤ ‡§î‡§∞ ‡§î‡§∞ ‡§≠‡•Ä ‡§Ö‡§ß‡§ø‡§ï ‡§∂‡§ï‡•ç‡§§‡§ø‡§∂‡§æ‡§≤‡•Ä ‡§§‡§∞‡•ç‡§ï ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ‡§ì‡§Ç ‡§ï‡•Ä ‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ ‡§π‡•ã‡§®‡•á ‡§ï‡•Ä ‡§â‡§Æ‡•ç‡§Æ‡•Ä‡§¶ ‡§π‡•à, ‡§ú‡§ø‡§∏‡§∏‡•á ‡§ñ‡•Å‡§≤‡•á ‡§î‡§∞ ‡§¨‡§Ç‡§¶-‡§∏‡•ç‡§∞‡•ã‡§§ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§ï‡•Ä ‡§ñ‡§æ‡§à ‡§ï‡•ã ‡§î‡§∞ ‡§ï‡§Æ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡•á‡§ó‡§æ ‡§î‡§∞ ‡§è‡§Ü‡§à-‡§∏‡§Ç‡§ö‡§æ‡§≤‡§ø‡§§ ‡§â‡§§‡•ç‡§™‡§æ‡§¶‡•ã‡§Ç ‡§î‡§∞ ‡§∏‡•á‡§µ‡§æ‡§ì‡§Ç ‡§ï‡•Ä ‡§è‡§ï ‡§®‡§à ‡§™‡•Ä‡§¢‡§º‡•Ä ‡§ï‡•ã ‡§∏‡§∂‡§ï‡•ç‡§§ ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡•á‡§ó‡§æ‡•§',
                    'sec12-career-btn': '‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§Ö‡§Ç‡§§‡§∞ ‡§¶‡•á‡§ñ‡•á‡§Ç',
                    'sec13-title': '13. ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§è‡§Ü‡§à ‡§Ö‡§µ‡§ß‡§æ‡§∞‡§£‡§æ‡§è‡§Ç üìö',
                    'sec13-p1': '‡§≤‡§æ‡§Æ‡§æ 2 ‡§ï‡•á ‡§™‡•Ä‡§õ‡•á ‡§ï‡•Ä ‡§§‡§ï‡§®‡•Ä‡§ï ‡§ï‡•Ä ‡§∏‡§∞‡§æ‡§π‡§®‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§á‡§® ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§Ö‡§µ‡§ß‡§æ‡§∞‡§£‡§æ‡§ì‡§Ç ‡§ï‡•ã ‡§∏‡§Æ‡§ù‡§®‡§æ ‡§Æ‡•å‡§≤‡§ø‡§ï ‡§π‡•à‡•§',
                    'syllabus-oir-title': '‡§ì‡§™‡§® ‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§¨‡§®‡§æ‡§Æ ‡§ï‡•ç‡§≤‡•ã‡§ú‡•ç‡§° ‡§∏‡•ã‡§∞‡•ç‡§∏',
                    'syllabus-oir-list': '<div class="subject-item flex-col sm:flex-row"><div class="flex items-center w-full"><span class="icon">üåê</span><div class="flex-grow">‡§ì‡§™‡§® ‡§∏‡•ã‡§∞‡•ç‡§∏ (‡§ú‡•à‡§∏‡•á, ‡§≤‡§æ‡§Æ‡§æ 2)</div></div><div class="w-full sm:w-auto mt-2 sm:mt-0 flex justify-end flex-shrink-0 space-x-2"><button class="notes-btn">‡§®‡•ã‡§ü‡•ç‡§∏</button><button class="lectures-btn">‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ‡§®</button></div></div><div class="subject-item flex-col sm:flex-row"><div class="flex items-center w-full"><span class="icon">üîí</span><div class="flex-grow">‡§ï‡•ç‡§≤‡•ã‡§ú‡•ç‡§° ‡§∏‡•ã‡§∞‡•ç‡§∏ (‡§ú‡•à‡§∏‡•á, ‡§ú‡•Ä‡§™‡•Ä‡§ü‡•Ä-4)</div></div><div class="w-full sm:w-auto mt-2 sm:mt-0 flex justify-end flex-shrink-0 space-x-2"><button class="notes-btn">‡§®‡•ã‡§ü‡•ç‡§∏</button><button class="lectures-btn">‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ‡§®</button></div></div>',
                    'syllabus-ppdt-title': '‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§î‡§∞ ‡§∏‡§Ç‡§∞‡•á‡§ñ‡§£',
                    'syllabus-ppdt-list': '<div class="subject-item flex-col sm:flex-row"><div class="flex items-center w-full"><span class="icon">üìö</span><div class="flex-grow">‡§™‡•Ç‡§∞‡•ç‡§µ-‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£</div></div><div class="w-full sm:w-auto mt-2 sm:mt-0 flex justify-end flex-shrink-0 space-x-2"><button class="notes-btn">‡§®‡•ã‡§ü‡•ç‡§∏</button><button class="lectures-btn">‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ‡§®</button></div></div><div class="subject-item flex-col sm:flex-row"><div class="flex items-center w-full"><span class="icon">ü§ù</span><div class="flex-grow">‡§Ü‡§∞‡§è‡§≤‡§è‡§ö‡§è‡§´ (‡§Æ‡§æ‡§®‡§µ ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡•Å‡§¶‡•É‡§¢‡•Ä‡§ï‡§∞‡§£ ‡§∏‡•Ä‡§ñ‡§®‡§æ)</div></div><div class="w-full sm:w-auto mt-2 sm:mt-0 flex justify-end flex-shrink-0 space-x-2"><button class="notes-btn">‡§®‡•ã‡§ü‡•ç‡§∏</button><button class="lectures-btn">‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ‡§®</button></div></div>',
                    'footer-text': '&copy; 2025 | ‡§Ü‡§∞‡•ç‡§ü‡§ø‡§´‡§ø‡§∂‡§ø‡§Ø‡§≤ ‡§á‡§Ç‡§ü‡•á‡§≤‡§ø‡§ú‡•á‡§Ç‡§∏ ‡§ï‡§æ ‡§≤‡•ã‡§ï‡§§‡§Ç‡§§‡•ç‡§∞‡•Ä‡§ï‡§∞‡§£‡•§',
                    'interviewChartData': {
                        labels: ['‡§è‡§Æ‡§è‡§Æ‡§è‡§≤‡§Ø‡•Ç', '‡§ú‡•Ä‡§è‡§∏‡§è‡§Æ8‡§ï‡•á', '‡§π‡•ç‡§Ø‡•Ç‡§Æ‡§®‡§á‡§µ‡§≤', '‡§ü‡•ç‡§∞‡•Ç‡§•‡§´‡•Å‡§≤‡§ï‡•ç‡§Ø‡•Ç‡§è'],
                        data: [68.9, 56.8, 29.9, 63.3]
                    }
                }
            };
            
            function renderPostCards(lang) {
                const container = document.getElementById('postCardsContainer');
                if (!container) return;
                container.innerHTML = '';
                const posts = serviceDetailsData[lang];
                for (const id in posts) {
                    const post = posts[id];
                    const card = document.createElement('div');
                    card.className = 'group relative p-6 bg-[#2a2a2a]/50 rounded-xl text-center border-2 border-neutral-700 hover:border-yellow-400 transition-all duration-300 cursor-pointer shadow-lg hover:shadow-2xl transform hover:-translate-y-2';
                    card.dataset.postId = id;
                    card.innerHTML = `
                        <div class="text-6xl mb-4 transition-transform duration-300 group-hover:scale-110">${post.icon}</div>
                        <h4 class="font-bold text-xl text-white">${post.name}</h4>
                        <p class="text-sm text-gray-400 mt-2">${post.cta}</p>
                        <div class="absolute top-3 right-3 text-yellow-400 opacity-0 group-hover:opacity-100 transition-opacity duration-300">
                            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M7 17l9.2-9.2M17 17V7H7"/></svg>
                        </div>
                    `;
                    card.addEventListener('click', () => showPostDetails(id));
                    container.appendChild(card);
                }
            }

            function showPostDetails(postId) {
                const post = serviceDetailsData[currentLang][postId];
                if (post) {
                    document.getElementById('postDetailIcon').innerText = post.icon;
                    document.getElementById('postDetailTitle').innerText = post.name;
                    document.getElementById('postDetailDescription').innerText = post.description;
                    showModal('postDetailModal');
                }
            }

            function renderCareerPath(lang) {
                const container = document.getElementById('careerPathContainer');
                container.innerHTML = '';
                const path = careerPathData[lang].path;
                path.forEach((step) => {
                    const stepElement = document.createElement('div');
                    stepElement.className = 'timeline-step';
                    stepElement.innerHTML = `
                        <div class="timeline-dot"></div>
                        <h4 class="font-semibold text-xl text-white mb-1">${step.rank}</h4>
                        <p class="text-base text-gray-300">${step.desc}</p>
                    `;
                    container.appendChild(stepElement);
                });
            }

            function initInterviewChart() {
                const ctx = document.getElementById('interviewQualitiesChart').getContext('2d');
                interviewChartInstance = new Chart(ctx, {
                    type: 'bar',
                    data: {
                        labels: translations.en.interviewChartData.labels,
                        datasets: [{
                            label: 'Llama 2 70B Score',
                            data: translations.en.interviewChartData.data,
                            backgroundColor: [
                                'rgba(66, 133, 244, 0.7)',
                                'rgba(219, 68, 55, 0.7)',
                                'rgba(244, 180, 0, 0.7)',
                                'rgba(15, 157, 88, 0.7)'
                            ],
                            borderColor: [
                                '#4285F4',
                                '#DB4437',
                                '#F4B400',
                                '#0F9D58'
                            ],
                            borderWidth: 2,
                            borderRadius: 8,
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        plugins: { legend: { display: false } },
                        scales: {
                            y: { 
                                beginAtZero: true, 
                                max: 100,
                                ticks: { color: '#e0e0e0' },
                                grid: { color: 'rgba(224, 224, 224, 0.1)' }
                            },
                            x: { 
                                ticks: { color: '#e0e0e0', font: { size: 14 } },
                                grid: { color: 'rgba(224, 224, 224, 0.0)' }
                            }
                        }
                    }
                });
            }

            function translateChart() {
                if (interviewChartInstance) {
                    const data = translations[currentLang].interviewChartData;
                    interviewChartInstance.data.labels = data.labels;
                    interviewChartInstance.data.datasets[0].data = data.data;
                    interviewChartInstance.update();
                }
            }
            
            function translatePage(lang) {
                currentLang = lang;
                document.documentElement.lang = lang;
                for (const id in translations[lang]) {
                    const element = document.getElementById(id);
                    if (element) {
                        element.innerHTML = translations[lang][id];
                    }
                }
                renderPostCards(lang);
                renderCareerPath(lang);
                translateChart();
            }

            document.querySelectorAll('.lang-btn').forEach(button => {
                button.addEventListener('click', (e) => {
                    document.querySelectorAll('.lang-btn').forEach(btn => btn.classList.remove('active'));
                    e.currentTarget.classList.add('active');
                    translatePage(e.currentTarget.dataset.lang);
                });
            });

            const showModal = (modalId) => document.getElementById(modalId)?.classList.add('active');
            const closeModal = (modal) => modal?.classList.remove('active');

            document.querySelectorAll('[data-modal-target]').forEach(trigger => {
                trigger.addEventListener('click', () => showModal(trigger.dataset.modalTarget));
            });


            document.querySelectorAll('.modal-overlay').forEach(modal => {
                modal.addEventListener('click', (e) => {
                    if (e.target === modal) closeModal(modal);
                });
                modal.querySelector('.modal-close-button')?.addEventListener('click', () => closeModal(modal));
            });
            
            // Initial render
            initInterviewChart();
            translatePage('en');
        });
    </script>
</body>
</html>

